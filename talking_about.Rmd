---
title: "Philosophical Digression - Talking About"
author: "Dan Price"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#A Philosophical-Mathematical Digression - Speaking About 
##(the more practically-minded may simply skip this section - or maybe we should put it later, anyway)
We should give a quick nod to what it means to say one representation is "equal" to another representation, even at the most basic levels. Clearly the files at the tract level and the block group level are not simply equal to each other, since they have a different number of rows and refer to different geographies, but they are presented in such a way as to make it possible to compare arithmetically - we simply ask whether, when we set the concepts or boundaries to the same shape or conditions, do we get the same number of people counted in that category. The total number they project can be equal (or not), and that is perhaps the simplest way to think about two different representations still being "equivalent" - that is, when they have "equal" results of appropriate arithmetic operations performed on appropriate substructures.

But we need to think carefully about the several ways we talk about "appropriate," here. Even when the sums are equal to each other, we face important decisions about the structure of the representation that allows us to calculate the sums, and whether what is being counted has changed across differing structures of representation. Much of that structure, in this case, is literally a "space," because it's the difference between the geographic characteristics of block groups and tracts. The fact that both were created by large committees of government officials with a variety of opportunities for input from other stakeholders and the public, and that they were deliberately created to embed smoothly into each other for the purposes of aggregation, allows us to do the aggregations across the structures with confidence. That same structure also allows us to think more fully about other possibilities for representing the inner connections and the potential calculations that are enabled by those structures. We hope to eventually also show that these structures, even though they explicitly belong only to the choices about mathematical representation, will give us more substantial clues to otherwise hidden structures at the neighborhood level, as the accumulation of structures refines what can be said about the relations between the space and the individual, as well as what is implicit in the previous descriptions but still has to be made explicit through some determined process. 

We should also note that this touches on all sorts of political questions, with different ways in which public input is accepted into the decision-making process. For example, the Census Bureau took steps to make these boundaries coherent and complete (both the geographic boundaries and the delimitation of the conceptual categories), but that is justified by the need for a coherent and mathematically consistent representation. The necessary calculations on demographics (which are at the base of so much else in public health, after all) are often difficult or impossible in less well-structured spaces, for example, where cities and counties don't share all geographic boundaries but have large shared areas, as with the City of Houston and Harris County. Finding ways to report numbers, even the basics of population, have real world funding consequences, which become tied into the presuppositions of the models at multiple levels. However, those bigger questions remain at the horizon. For the moment, we want to take our building process slowly, for the purpose of eventually understanding what is at stake in these first and seemingly trivial decisions that in turn rely on other structures - after all, we have to be confident that there are overarching mathematical structure that are sustained across the representations in order for the larger interpretation to make sense, even when that interpretation is the almost immediate and seemingly trivial act of aggregation. 

One of our philosophical inspirations comes from the way that type theory differs from set theory. Both of them provide ways to encode other mathematics into a single way of talking and making proofs, with many outward similarities but important differences in how the process of proving mathematical propositions is expressed. In the encoding of mathematics based on set theory there are objects (called sets) which have a membership relation to other objects. In type theory, [Per Martin-Löf's intuitionist version](https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf) is especially clear in its motivation , types are the way that elements can be described without positing the existence of a set or of objects that have the property of belonging to that set. For a common example of where it makes a difference, if you encounter a group of points in a set, you cannot make sense of them in a full and meaningful way just by knowing that they all have the attribute of being a point; the Banach-Tarski paradox, for a motivating example, showed that using the rules of set theory and a few suppositions about infinity and the axioms of mathematics, a sphere of any size can be divided and then recreated into a sphere of any other size. In our terms, set theory preserved the structure of membership in the set of points that are found on the surface of a sphere, but sacrificed the relation of those points to the size of the sphere, since the structure that sustained size wasn't part of the set, although an infinite number of points, by definition, was included. In type theory, on the other hand, the objects that are manipulated, including the numbers, must be accounted for - the space that the points are found within lets you describe them as close together or far apart, as countably infinite or as generated by the relation to a fixed center, and (more broadly) Martin-Löf's type theory allows a collection of typing declarations to construct or encode any mathematical object in ways that account for the structure one wants to preserve. 

The constructive process does not imply ontological relativism, but does rely on the idea that there are many ways of talking about things and that being conscious in those choices between ways of talking, and attentive to the motivations guiding those choices, can ground the practice of mathematics as a search for structure. In the history of the theory of mathematics, constructivism is usually opposed to one or another sort of Platonism, but for our purposes, we're striving to motivate references to mathematical theory or its philosophical background only through encountering and responding to issues with the concrete understanding of creating Sam City. It is hoped that this can be done in a way that could be useful for our students, or for any of the fields, like population health, that rely on a representation of demographic attributes for a population. Here, we are underlining the fact that certain methods become available to us because of the conscious decisions of the U.S. Census Bureau to create a database that will be useful for calculations - the fact that every person is assigned at least one race, and that the reporting includes all people in its categorizations, means that you can use the law of the excluded middle and say that if someone isn't one of the other 6 races, they must be the one that's left. That is constructed by the Bureau to be useful for the purposes of the aggregation. The addition of ethnicity in the 1980 census, after a much contested attempt to add "Mexican" as a racial category in 1930, was a politically motivated attempt to account for ways in which racism against people of Latino origin was impossible to track without having some way of counting how many people fit into the category - which is to say, it's a politically meaningful choice whether to include it or not; there is no simply "non-political" side to the act of category construction or the choice to either maintain or modify the categories around race and ethnicity. (cf. [Changing Race](Rodriguez, Clara E. Changing Race: Latinos, the Census, and the History of Ethnicity in the United States. New York University Press, 2000.) E184.S75 R64 in library). In the more recent accounts of race and ethnicity, it is striking that there is simply no clear account of how people see themselves, except to say that they aren't particularly satisfied with the way that the census divided race and ethnicity in its attempts to deal with the fraught political history. (cf. [Pew Research Survey: The Many Dimensions of Hispanic Racial Identity](https://www.pewresearch.org/social-trends/2015/06/11/chapter-7-the-many-dimensions-of-hispanic-racial-identity/))

These questions are separate from the still difficult question of undercounting (cf. [NYTimes on 2020 undercount](https://www.nytimes.com/2022/03/10/us/census-undercounted-population.html)). The collection itself is riddled with difficulties that effect the final accuracy of the tables. We will try to speak, first, to what is known by virtue of the tables themselves, and then see if insight about that process, and about what it means to think in mathematically and philosophically coherent ways about these tables, can reflect back on the larger social and political context of motivating questions.

We said earlier that part of the initial impulse behind the construction of Sam City was to make a more "friendly" version of Houston for pedagogical purposes, and it's worth being explicit that there are deeply racist components to Houston's past that should not be erased in the name of that friendliness. Our hope, by contrast, was that a more honest accounting could be made possible by understanding how the construction of the basic demographic categories in the population of the simulated city could reflect the complexity at hand. Accordingly, just as the addition of ethnicity as a category was troubled by a triangulation of political issues around segregation throughout the U.S., Houston's White political leadership attempted to achieve court-ordered segregation in the schools by integrating Latino and Black schools with each other, while leaving the White schools untouched. This sleight of hand was achieved by counting the Latino students as White, but ignoring the long-standing inequities in the schools in Latino neighborhoods. There were considerable political and legal actions against Houston at the time [NYTimes, March 1970](https://www.nytimes.com/1970/03/01/archives/houston-moves-to-desegregate-schools-amid-clamor-of-protest-angry.html) and [NYTimes, September, 1970](https://www.nytimes.com/1970/09/06/archives/houston-huelga-schools-open-in-a-mexicanamerican-protest.html), but the schools remain deeply segregated throughout the United States, and especially the South, as documented by [UCLA's Civil Rights Project](https://www.civilrightsproject.ucla.edu/research/k-12-education/integration-and-diversity/brown-at-60-great-progress-a-long-retreat-and-an-uncertain-future/Brown-at-60-051814.pdf), as well as in Houston's larger metro area. These histories all have continuing ramifications on current social, educational, and health outcomes, as explained by the [City of Houston's Health Department](https://www.houstontx.gov/health/chs/documents/Health-Disparities-Data-Report-I-2019-Root-Causes.pdf).

To say that "race" is embedded in a biological determination and "ethnicity" in a cultural context, clearly, is to miss how the structuring questions are motivated and thus to obscure at least some potential pathways toward a more acceptable general approach to embedding relations between individuals and social structures than as sedimentation of layers of causes and effects. At a social level, we need a way to refer to the history of determinations that have affected individual people's life courses, as others create barriers based on those ways of talking about who does or does not belong in a group. To do so, however, risks reifying those determinations, or creating a situation where one way of talking -- for example, the idiosyncratic development of the idea of race and ethnicity in the U.S. -- is taken as the reference from which all other approaches deviate.

Here, the scientific temptation is to try for more complexity -- to determine variance in reference to an encompassing schema that provides more information and allows for more finely tuned subsetting, such as a genetic characterization which tracks ancestry. In such an analysis, one might speak of 68 distinct ethnicities in Mexico, with recently refined mathematical techniques allowing researchers to show distinct genetic characteristics, tuned to different geographic regions in Mexico [The genomic landscape of Mexican Indigenous populations brings insights into the peopling of the Americas](https://www.nature.com/articles/s41467-021-26188-w). These genetic characteristics are plausibly linked to a number of biomedical traits, with considerable scientific resources committed to finding pathways to providing genomic medicine or enabling precision medicine through incorporation of genetic analyses [The Genetics of Mexico Recapitulates Native American Substructure and Affects Biomedical Traits](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4156478/). Of course, if one is to embed these mechanisms within a more fundamental physical process of biochemistry, there are orders of magnitude more levels of complexity to account for in epigenetic terms than just the transcription of the traditional base pairs &mdash; or &#151; cf., for a recent but not final account, [Mapping the epigenetic modifications of DNA and RNA](https://europepmc.org/article/med/32440736). More fundamentally, it is clear that such a scientific account can inadvertantly end up masking the social and political dynamics, which have frequently been driven by erasing these sorts of fine-grained nuances and lumping people who are not seen as part of the accepted group(s) into an "other" category. Some sort of account that tracks what the distinctions are being used for, and in what context they make sense as being useful for answering a problem, needs to precede the questions about whether the distinctions fully exhaust the particular frame. That preceding question, for etymological reasons, we will call difficult (it is literally not easy and resists being calculated) instead of complicated (which implies a multitude of layers, which are resolved through unfolding and determining single values within a multi-dimensional space).

We would prefer to resist that temptation to speak about complexity, and seek to focus on the difficulty of the question: how do we think about the embedding so that it doesn't reify a sense of determination in isolation? And how do we bring that difficulty into focus as a pathway that addresses questions of value to social, economic, and health outcomes? Let's take a second to think about how to approach the problem of embedding, before moving back to our proposal to think about the construction of the data tables as "ways of talking."

Type theory gives us one well-articulated alternative way of thinking about the embedding, which is popular precisely because it still allows a unifying understanding of how to encode any proposition about the world in the straightforward and consistent language of types. Its modern articulation is made possible by the recognition (starting with Tarski) that truth is a property of a language that is "higher" than the language in which we describe the objects. This was meant to address foundational problems with set theory, but also has to do with how one tries to separate syntax (form) from semantics (content). Tarski says that claims made in a given language about given objects are embedded in an encompassing model that provides rules for what is true or false. Intuitionistic type theory points out that there is a process of construction that undergirds both, and thus makes it possible to understand embedding as a choice about what structures matter in the model - so that, in our example, we can ask explicitly how the relations of race to ethnicity are embodied as countable in the census, and also use that artificial construct to infer other aspects of the structure that aren't immediately given in the tables. In that sense, we are trying to understand the types as constructed categories, and then to understand what the limits of that construction show - both positively, in terms of how operations performed on the data within those bounds can create new understanding of internal relations, and negatively, in terms of understanding what can't be said in that language.

Very briefly, it's worth remembering that type theory began as an answer to Russell's paradox, although two steps removed, as Alonzo Church first developed untyped lambda calculus and then typed lambda calculus. The underlying task, stemming from Georg Cantor's foundational formulations of set theory, and also found in the continuum hypothesis, is that when you create sets, you need to be able to understand their internal structure in order to perform operations on the set. Set theory had been designed with the idea of capturing predication - one seeks to understand what it means to say that something has a predicate by saying that it belongs to the set of objects with that predicate. If one can generalize this process, then the use of numbers to ensure structure inside the sets (simply put, that I can assign a number as a predicate that "represents" each object and then, with suitable caveats, shows how the set of objects can be well-ordered) can itself be trusted. Cantor saw that you needed to be able to say that the power set of a set - that is the multiplication of all the combinations of categories - had to be larger than the elements of the set. It had to be possible to create more subsets than elements in the set. This may seem trivially true, but runs into difficult problems where the problem of representing infinite subsetting meets the problem of ordering (i.e., with Dedekind) and one has two competing ways to order what is infinite suggesting that the ways to subset the infinite do not converge to a single rule that would encapsulate the meaning of the infinite. Or more importantly, the idea is that if you are trying to understand how a type is constructed, you are asking how it is calculated, and if you calculate without attention to the construction, you fall into logical traps that prevent effective computability. (Martin-Löf, before the invention of Haskell in 1990: [Constructive Math](https://www.cs.tufts.edu/~nr/cs257/archive/per-martin-lof/constructive-math.pdf)). [Martin-Löf, Constructive mathematics and computer programming, in: Logic, Methodology and Philosophy of Science, vol. VI, 1980, pp. 153–175.] 
[[need to think about: W.A. Howard, The formulae-as-types notion of construction, in H.B. Curry: Essays on Combinatory Logic, Lambda-calculus and Formalism, 1980, pp. 479–490; there's an overview "Propositions as Types," at https://www.youtube.com/watch?v=IOiZatlZtGU by Philip Wadler that strikes me as very clear. He frames it in terms of the capacity to reduce logical propositions to a computable form and how the lambda calculus duplicates the formalism - he rather bizarrely asserts that it shows that math is discovered, and not invented, even though it's all about constructive math - including a digression on lambda as the universal programming language. It maybe be worth talking about Haskell as Curry's first name; that functional programming was looking for a way of being on a first-name basis with a dominant figure in the field, perhaps in a parallel with Sam, with some sense of the strengths and weaknesses...
Need also to think about how explicit Martin Löf is about the problem of hierarchies and how they get resolved: "156 - continuing from 155 - concisely: "'if a set is understood in Zermelo’s way as a member of the cumulative hierarchy, then a set cannot be the same kind of thing as a data type.'"; that we aren't dealing with membership, and the truth about membership, but about labeling, and the consistency of labeling - and doing it constructively]]

Our point, with this extended digression into assigning race and ethnicity, is that as we try to assign attributes to individuals in consistent ways, we must assume the legitimacy and applicability of the categories - an assumption we have strong reasons not to make! If set theory can be used, it's because it presents the possibility of infinite interior correction of categories - we can add complexity to the description, produce more factors for analysis, and refine the idea of membership - all within our overarching confidence that there are more ways to name things than there are things to be named. The problem is that this assumes that the whole problem is about coming up with the right names - the right way to say things. The idea that type theory introduced was that the higher orders of embedding could be typed in such a way as to ensure order (cf. Church's typed lambda calculus); they also introduced the idea that the order of calculation could be from the outside in, whereas set theory had created the situation where things had to be solved from the inside - which precluded some of the most useful steps in construction.

#MAYBE THE IDEA OF ORDER OF CALCULATION IS A SUBTLE WAY TO INTRODUCE THE NEED TO CONSTRUCT; THAT ONE CONSTRUCTS BOTH THE DOMAIN AND THE CODOMAIN IS KEY IN LONG RUN, AND DIFFERENT FROM TYPE THEORY, WHICH WANTS THEM TO BE THE SAME; CATEGORY THEORY ASKING HOW YOU MOVE FROM ONE WAY OF TALKING TO ANOTHER!!!
##not just that you have lazy evaluation; it's the idea that evaluation happens along the line of the construction and not along the line of a presupposed what it must have been like to have been built from first order primitives / homotopy degree zero
##for longest arc: what does it mean to stretch toward saying something? To be engaged in a question?

There's also the question of formalism as inappropriately relying on the infinite and the law of the excluded middle as purely formal.

Could talk about the "algebraic" as in Lawvere's Conceptual Mathematics in terms of multiplying objects and mappings between ways of multiplying the objects. Then the Cartesian product, below, is seen as multiplying objects and the structure is maintained.

It may be that the algebraic approach toward embedding in category theory gives us yet more powerful tools, especially in the idea that we are looking for the ways that descriptions can be transformed into or mapped onto equivalent descriptions embedded in different models as the way of specifying the meaning of an object: in other words, the proposition is not true or false depending on the embedding within a metalanguage, but rather the algebraic possibilities that govern the totality of possible ways of talking about the same thing will tell you what a mapping that conveys a true proposition would mean. That is, in Lawvere's articulation of the idea of a mapping from hyperdoctrines, as in ["Diagonal Arguments and Cartesian Closed Categories,"](https://doi.org/10.1007/BFb0080769) from 1969, (Lawvere, F. William. “Diagonal Arguments and Cartesian Closed Categories.” Category Theory, Homology Theory and Their Applications II, Springer Berlin Heidelberg, 2006, pp. 134–45.), the algebraic possibilities that govern the composition of the steps that create a representation tell one where the limits of a way of talking about things lies. Russell's paradox and Gödel's incompleteness theorem both become instances of an attempted transformation of talking about sets, where the attempt to describe the set of all sets becomes self-referential and paradoxical, into the problem of how to transform sets through mappings where the presupposed notion of truth is a mapping of points to points. 

The limitation of that approach can be seen by turning to algebraic topology and to ideas of mapping that don't reduce to point sets - or to binary representational functions. Here we hope to capture something of the humanities criticism of traditional set theory, as with all the descendants of Heidegger who decried the technological approach to language as mere predication, and say that there is also a mathematical approach that seeks to situate set theory within a broader understanding of the possibilities of speaking about. Broadly, it is the idea that the ways of talking about something are structured through the process of describing and that the attention to that structure is the core of the scientific process. Of course, it's not the only way to talk about things, and we'll try to point to the motivating questions as opposed to the dogmatic elements of any of the tools that have inspired this inquiry. How we can practically encode a claim about mathematical relation, however, is the guiding reason that Sam City was developed, as we pushed to understand what it would mean to create a reusable database of demographic, social, and economic information about residents and neighborhoods in our area that somehow also captured dynamic change and the sense of individuals that they somehow exceeded the objectifying categories assigned to them by the system. How do we make that sense of organic development and internal self intelligible to others? How do we compute the results of aggregation over large groups of individuals, each pursuing their own paths? How do we represent the effects and interactions? Those are practical and driving questions for ourselves, our students, and our community partners.

What then counts as the guiding principle for scientific research on the sort of dynamic system that constitutes a city and its inhabitants? Instead of looking for a predicate that describes the object and asking whether it is true or not - as when you ask if "Hispanic" is the right term for a Catalan on a temporary work visa - you look for the structure that enables true or false predication to be about the individuals who are being talked about. At that point, one can identify the limits of the framing questions and the limitations of the way that the domain of possible answers was constructed, and ask effectively what changes can be made in the process of talking about that would better match the motivating questions.


#here or later??
We note, by the way, that one could encode the descriptions in terms of sets, as well. Only as the construction gets yet more complicated will the distinction between types and sets really matter. [want to get, eventually, at what it means to work through syntax vs. semantics, with a brief nod to Tarski (https://en.wikipedia.org/wiki/Semantic_theory_of_truth) and Lawvere (who took classes with Tarski for a year in Berkeley and is responding to that same question of semantic vs. syntactic in 1963) - the subtlety is that the initial idea of structural embedding I'm trying to pull out is what Tarski would have assigned to the formal idea of semantics, and that Lawvere (following Grothendieck) would have given a better mathematical grounding (although we still have to talk about what gets past the "formal" idea of semantics in Tarski's sense); this could also be the place to talk about Martin Löf, and why ZF set theory (with or without C for the axiom of choice, he is clear!), doesn't work because of membership and stratification. That the strata can be cashed out - i.e., the axiom of reducibility - doesn't need to be brought in if you're doing constructive (or intuitive) type theory] That Lawvere doesn't mention anything about "membership" in his definition of the category of set, for example in the co-authored introductory textbook, is no doubt a remnant of his time thinking about Tarski's project, and about what it means to understand mathematical structure in terms of the semantics of possible transformations in "ways of talking" as opposed to building the definition internally from axioms [Awodey or McClarty or Marquis (from a Geometrical Point of View)]

#here or later??
We should do things like talk about symmetric monoidal categories, and perhaps more broadly about Abelian groups, Lebesgue measure theory and sigma algebras: https://en.wikipedia.org/wiki/%CE%A3-algebra - just not sure where

#here or later??
why the faith in complexity, and in adding more dimensions as one slowly builds more complicated representations, comes from an idea of projecting onto a surface of individuation - not that one is against the idea of number, but against the process of taking the objects as individuated as the final arbiter of what it means for something to be! Earlier work has been used to explore the idea that Difficulty is a better way of characterizing embodied contact with the world - as opposed to an original simplicity that is complicated by adding more points of contact - and the general idea for the students in our courses has been that some of the most important questions we face shouldn't be thought of as "complicated" - they should be thought of as "difficult," or as requiring sustained attention to the forms in which things are presented, and the ways in which individuation emerges from the transformative relations that comprise society. 

Somewhere above, about Type theory and computability - the only operations allowed to do the matching are things that you can place on a single row and then can match with a different table uniquely by that row. That is a complement or adjoint of the sense of complementarity itself.... You can give a whole group a name, and then count each individual, and you can do things that allow you to count multiple times as you subset, but each individual has to be somehow given a value, so that the row can be assessed. That this is also part of what drives imperative programming and nominalistic epistemology to think that the task is to test whether a name is correctly used as opposed to asking what patterns emerge. (think of difference between adding in imperative and functional languages, or in Peano vs. Church, and the order of calculation; and then how that means that our question, here, is different). Part of the difficulty is that the factors being assigned are slightly different than the cartesian product of possible assignments - could tease that out, above.