---
title: "Building Sam"
output: html_document
#output: pdf_document
#header-includes:
#  - \usepackage{titling}
#  - \pretitle{\begin{center}
#    \includegraphics[width=2in,height=2in]{CHWI_Color.png}\LARGE\\}
#  - \posttitle{\end{center}}
author: "Dan Price"
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Preliminaries
We are not saying that there's a proper way to speak - that is, in fact, what we are trying to contest. The idea that names and saying true things about names is the end of the story, and that there are true and false ways of talking, misunderstands the force of language. "I live in fear of the one true God" is not the same as "I am afraid of this God of which you speak". The idea that context makes the difference; the idea that comparing the two sentences to even understand in what sense they are saying the same thing is even troubled - yet, there are ways to begin that comparison....
Take away from Touching Difficulty - that the world recedes and language moves forward - formalism in math is made possible by the way the world recedes. In our fundamental presence in the world, there is no positive moment of intuition, which proves or disproves a hypothesis. There is a restriction of ways in which things can appear "as" something, a set of symmetries that allow things a place in our world, and then the commitments we undertake to stay true to those ways of talking. In physics, think of groups and symmetries; in ontology, think of ethics preceding representation. 

## Sam City - Houston on a first name basis as an aid to pedagogy
Sam started as an effort to think more carefully about how individuals are embedded in a context and how to model the relations between individual and aggregated levels of health outcomes. Our team had been involved in adapting and developing mapping tools for non-professional community members to help policy makers decide on priorities for different sorts of interventions. At the same time, we were starting the Community Health Worker Initiative, which used a project-based curriculum to provide Texas' certification for community health workers. The core of the course was built around advocacy through better communication, and we were disappointed to find that the current epidemiological mapping tools were not particularly well-suited for communicating strengths in a community, even when explicitly engaged in "asset mapping," and other techniques designed for eliciting a discussion of strengths. The lesson students correctly take from a map showing startling differences in life expectancy by neighborhood is that the luck of being born in one place rather than another determines how long you will live. I say "correctly," because that is the conclusion the map makers wanted to convey. The conclusion that somehow society or government could or should do better was left to the individual conscience. That map, based on impressive modeling work at the National Center for Health Statistics[https://www.cdc.gov/nchs/data/series/sr_02/sr02_181.pdf], and including an expert advisory panel in its construction, used mortality data collected from state vital statistics offices, which is reported at the county level, and geocoded to the census tract. Given the relatively small size of a census tract, however, random variation in deaths by age group means that drawing conclusions about a single census tract over a single year would misrepresent any underlying patterns that become visible at larger scales. In order to represent those larger trends, they took the 30% of all census tracts with over 5,000 people and complete death records for the year, and created a model that associated mortality to a small number of factors: geographic (whether the census tract was inside or adjacent to a reservation or not, with a control introduced for four geographic regions), demographic (age, race, hispanic origin), and socioeconomic (quartiles for median family income, population density, 4-year college degree). They were able to compare the results with other models, and found that they generally agreed within a couple of percentage points. They then assigned values to all the census tracts in the U.S. based on that model (reported in the press as zip codes, because the validity of the model depended on some aggregation, and reporting results to the general public based on postal codes made the results generally accessible to the larger public). 

This is a very sophisticated modeling process undertaken by top experts in the field, and my point is not to question their process but to point to why it structurally excludes the use case we wanted to explore for the integration of community strengths into health policy discussions. In short, no local variation of the sort that might arise in the personal experience of a community member, even one who is very attentive to health outcomes like a community health worker, was reflected in the model. That the folks living closer to the freeway or in the public housing units had worse health outcomes simply could not enter into the model. That meant that if our students and partners used a study that showed the adverse effects of living close to freeways on mortality to create a model, it would be contradicted by the more official model of life expectancy generated at the national level for all the localities. One could seek to explain the different assumptions, but then digging into the data on the effect of proximity to a freeway would lead to complicated issues about the presuppositions of that model. For example, if both models used a Poisson distribution for purposes of fitting the model, they were both making the assumption that the events occur independently of each other - i.e., that a death happening may be correlated with other deaths, but did not, by virtue of simply happening, cause other deaths. As one thinks about this assumption, however, one can imagine ways in which they aren't completely separate as occurrences; for example, it could conceivably matter in cases of gang-related violence, "deaths of despair," or perhaps a more generalized feeling of powerlessness that causes people to not feel that they can fight their situation since they see others dying at higher rates around them. This may seem like a marginal worry, at best causing the statistical conclusions about mortality to be a little less certain, but it leaves us with both the question of how we would quantify these neighborhood effects, if we want to intervene at that level, and how much interventions that just even out the more measurable inputs - like ensuring healthcare access through insurance subsidies - will really matter.

We began working with the problem in 2015, as part of an effort to have better local health maps available to policy makers, where we quickly found systematic problems in using the maps for sophisticated policy decisions, including the difficulty of representing neighborhood strengths, correlations between the various social determinants of health, longitudinal progress on metrics responsive to local intervention, and simple understanding of the comparative magnitude of effects across different potential interventions. One intuition we had was that many of the statistical approaches would be easier to understand if we enforced a common approach to mapping effects at the individual level. To take a real example, the CDC reports on its page "HIV and African American People" (https://www.cdc.gov/hiv/group/racialethnic/africanamericans/index.html) that "Black/African American people made up 42% (16,002) of the 37,968 NEW HIV DIAGNOSES in the US and dependent areas in 2018." The following infographics on that page then give the total raw numbers for type of transmission and how it compares to other demographics. There are some real difficulties in claiming that the level of precision on the infographics is justifiable, because of well-known difficulties in collecting information and ways in which, for example, potential overlapping modes of transmission had to be reduced to single answers. But overall, that sense of translating the percentages to actual numbers made it clear what sort of claims were being made as different statistical characterizations were applied. At the very least, we could demonstrate to people visually that saying 42% of Black males who sleep with males have HIV is simply false, as was the idea that black men who sleep with men had a 42% chance of getting HIV. There are, of course, many visual aids for understanding how to avoid such simplistic misunderstandings, but our purpose was both to show the ground of the misinterpretation and get students used to the exercise of working out the implications of their claims (both in statistics and in the prose that interpreted their claims). We could thus show that there are many different ways to represent a situation without abandoning the idea that some truth could still be asserted; at best, we hoped to show that transforming the equations from one mode of representation to another could, by itself, aid in understanding the underlying situation, for example, when asking questions that had embedded percentages that could break in different ways - so that one had to choose whether percent of "Black males who sleep with males" should be a subset of percent of all males, all males who sleep with males, all Black men, and further whether you were speaking of the overall population of newly infected people or of the population as a whole. We use the example, however, because the responses when actually first discussing the statistic in a public health class, the interpretations quickly brought to light deep-seated biases, including racist and homophobic generalizations that were stigmatizing instead of empathetic. The well-meaning desire to show that there was a troubling racial difference in new HIV infections led to misunderstandings and triggered biases that we had hoped could be avoided by insisting on a grounded understanding of the magnitude underlying the competing representations. Of course, such attempts to have our students asymptotically approach a correct understanding of statistical relations, their malleability and their grounds in individual cases, never solved the problem of whether the students were empathetic or indifferent in their judgments about the implications of the statistical representations. The solution, which we came to later and expand on in the narrative, below, was to stop thinking that the problem was to create the conditions for students to make correct judgments about the situation, and to get them instead to understand how the embedding of a subset of the population into a larger whole was a matter of layered choices in the representation. 

The intuition behind our approach was that the availability of easy to produce and widely available community health maps and the insistence on empowering communities through better maps would lead students to think about the process of mapping more holistically, and to think about the process of creating health interventions more rigorously in terms of outcomes. The convergence of the rational model of behavior and a naive idea of mapping as a presentation of facts that are judged and then acted upon followed a liberating narrative structure that matches the statistical framing of individual events explained by their context. Each individual, in that telling, gathers as much information as possible about what is currently the case and then judges what to do next - what many people would still think of as rational action based in human freedom. [could talk about Kant] 

We had also hoped that more complicated questions about how to represent embedded situations with reference to individuals could be elucidated with a general strategy of explicitly mapping individuals to different embedding schemas - for example, by assigning them neighborhoods with different characteristics and using visualization tools to show how different modeling parameters produced different results in terms of individuals in a neighborhood. After all, both the ecological fallacy and the individualistic fallacy are driven by unwittingly mixing levels of analysis[Krieger that I gave to Dave], and tools that made the differing levels and their effects easily visible promised immediate heuristic value. At the same time that we wanted our students to acquire a habit of explicitly thinking through the statistical embedding of subgroups in larger populations, and the implications of picking different ways of slicing up those representations, we also wanted to emphasize the dynamic potential of individuals to change. It is a commonplace in public health to emphasize a strength-based approach to community engagement, but difficult to see how statistical representations could capture the potential of a community with strengths to break with the statistically most likely outcomes. Our intuition was that emphasizing individual emergence into shared patterns would be a first step, since it at least gave the framing that individuals through creative activity acquired properties that were then aggregated into competing types of description, as opposed to saying that some abstract mechanism determined what it meant for the individuals to be in their concrete existence. Our intuition was that this would also allow a more fulsome explanation of structural mechanisms, since it would require understanding how the spread of certain ways of talking about individuals spread and came to be used in concrete situations by people with power to enforce consequences based on those representations - for example, if a city implements more aggressive policing, including arrests for minor offenses, in neighborhoods designated as high crime by a map produced through an analysis of police arrests, there is clearly going to be a reinforcing circle of more aggressive policing in some neighborhoods; and if there's a history of discretionary arrests against racial minorities and segregation patterns at the neighborhood level, then the abstract representations have clearly traceable effects in the patterns that emerge from aggregating individual outcomes in the statistics about policing. In other words, beginning from the strengths of individuals does not have to mean blaming individuals for having bad outcomes.  

##Sam City - focusing in
The Sam City project utilized several approaches, both in terms of visualization and modeling, to better convey individual and aggregate levels of strength and potential pathways for enhancing individual and community capacity to succeed in shared project development. We wished - both for our students and our community partners - to show how patterns of aggregated individual outcomes emerged over time, and began simply with visualization techniques that emphasized the process of coming to see an outcome only slowly appear. The problem was that although those representations hinted at the idea of a non-deterministic future, they remained based on numerical representations of moments in time determined by the determinations from previous moments. The representation of embedding, in other words, was that a slice of time was determined as the product of (or as embedded in) the totality of the previous moment. Our intuition about emergence, on the other hand, required the idea that individuals had competing trajectories of development, and that parts of the context were more important at differing times. To capture that sense of emergence more fundamentally, we attempted to forefront the idea of pathways - and barriers or constraints faced as one traveled - instead of determinations - so that the place one is in at a given moment is determined, and then a pathway is made up of a series of having been determined as at a particular place.

The beginning negative intuition is that the dichotomy between system and individual suffers from a fundamental and avoidable contradiction in the representation of transformation over time. Avoiding this dichotomy would perhaps sidestep sterile coldwar arguments about individualism vs collectivism by grounding the process of creating maps in a new representation of transformation over time. The approach explicitly contests the idea that the value of the map is ultimately proven through its similarity to being at the place where interactions are determined to have happened in one way or another - the representation of things as "having happened" eliminates the difficulty of trying to represent what is not yet knowable (the future), but seeing the proper representation of all things in terms of being representable as such is a bias of subjective apprehension, and the idea that to be is to be knowable. The convergence of being and knowing is tied to the idea of providing a limitation in time - I know something to be such in the moment of my apprehension - and that breeds the idea that the productivity of time, like the productivity of creating human names, is expressed as creating things on the surface of the present moment. [perhaps as footnote? don't want to lose people; could look for easier example; maybe could talk about it in the limited context of social determinants, first?]

Instead, the structuring across time should be envisioned as continuous constraint on possible outcomes, as time passes; even though individuals may experience a sense of opening possibilities as particular structural barriers are removed at the intersection of larger systems, the more original structuring movement is the constraint within which the sense of opening comes to be defined. The limiting of possible outcomes - not the instantaneous creation of determined limits at a given time standing in the way of a sui generis self-enclosed thing-in-itself that is determined as one thing at one time and against which other times are compared - should be respected as the structuring of the representational space across time. 

In order to properly capture this alternative sense of structuring, we will utilize some of the tools of applied category theory. We will try to generate an understanding of the need for that move concretely, and without assuming the reader has knowledge of any particular mathematical discipline in advance of the motivating examples. 

By way of preview, and as a sketch for much larger conceptual work that will have to be completed elsewhere, we note that there are philosophical arguments about the ways in which the speaking subject grounds category theory [Lawvere on Hegel; Badiou, Livingston], and how structuralism grounds different interpretations of category theory as emerging from either topological or set-theoretical impulses,  but broad agreement on the grand strokes of the development of the approach. Category theory began as an abstraction about mathematical theories, and more specifically the understanding that the transformations implied in different operations could be understood by concentrating on the transformation of the ways of phrasing a claim (the morphisms) even when the operations were not comparable and did not allow for a determined solution. (The idea that functions can be solved in a unified approach to representation arising from set theory continues to animate much mathematical work, but is explicitly not a feature of topology and thus only a subset of category theory). This allows category theory to treat mathematical tools and theorems as objects with structure, fundamentally connected to the idea that different ways of saying the same thing can be compared - as "equal" (as in two functions producing the same determined result), "equivalent" (as in two ways of saying something are the same), "isomorphic" (two transformations can be inverted in such a way that the "shapes" are the same) or "adjoint" (the inversion of the transformations allows for a shared delimitation of the conceptual space) ways of talking about a mathematical claim. The objects of those theories are compared and how they embody fundamental decisions about the representational regimes of the competing theories are made explicit. Applied category theory notes that this sense of being able to represent situations in different ways is also found in applied mathematics, and that a process of eliciting structures from understanding how the different possible representations relate is available in concrete use cases. 

The concluding intuition is that mathematical approaches that communicate the structures of transformative activity allow different ways of mapping activity to be compared, both in terms of how well each describes a situation, and how well each points to potential future work, both individually and as groups. To do such, we need to employ mathematical tools that capture transformative activity, that respect individual difference and embed individual activity within shared projects. Models developed with these tools allow the unifying representation of human activity to reflect and embody the strength of communicating ideas to others and working in common toward a transforming world. 

##the more strict version of the argument
The ways of talking within statistics moved from classical to quantum when the commutativity of pathways became a problem. The arguments about symmetries captures this version of pathways as ways of talking, and makes the passage through time only one of the possible places where commutativity could be problematic. The idea that each way of talking is differentiated from others by the way that it reduces the number of allowable pathways (and not by defining or creating a limit, as in perspective's on a surface, which is set-wise approach to the category), then converges with the idea that we don't want our students to sit in the place of judgment about what is true or false in a statistical claim (although they need to know how that works), but to know how the embeddings depend on the restrictions inherent in the claims.

##Plan of this argument
Introduction of Sam as a representational problem
Embedding as levels - promise and shortcomings - the set / subset as an idea of determined layers
Embedding in a determinative context - promise and shortcomings / should perspective be explicitly criticized here? Bring in Foucault at some point?
Embedding within a structuring field - promise and shortcomings
Embedding as a set of paths - promise and shortcomings
Embedding arrows that are not sets of functions - promise and future work
At each point in the argument, ways in which Marquise's explanation of category theory shows how embedding shouldn't be though of as hierarchies, or even dimensional embedding, but as a restriction to natural morphisms - may say, at some point, that the idea is to understand how freedom is different from the instantaneous present as limit of the whole, naming / engaging / subsuming the whole is different from the extended navigation of constraint.
How does that relate to the sense of adjointness / duality
Can that be made to emerge from preserving the structure of how many x do you encounter in a cell, by chance? vs. what is the distribution across cells? [as its own path, and not just as a function that produces a set of cells that can have their probabilities aggregated]

##Mapping and communicating possibility.

Story of why teaching CHWs and being involved in the HCAT with HUD was so important.

The idea is to show how a strengths based model also requires a more careful representation of individual trajectories. Teaching communication / advocacy as opposed to the idea that they already have all the capacity they need and only need obstacles removed. The process of communication is the process of finding ways to share a project with other people - to work together toward an end of some sort. Math is also a way of learning to speak to each other - of finding common ways of speaking. And at certain levels of abstraction, it's no longer that you have to learn an arcane set of rules, but that you can understand schematically how we differ in our ways of talking - and how that is different from multiple perspectives, or from complexity.

Even in Emile, the teacher has to employ artifice to bring out the natural; there's something about learning something that is not natural that is needed to make the innate strengths flourish.

The current description of the social context of health is often shortened to "the social determinants of health" and it is explicitly designed to combat the idea that disease happens to individuals in isolation. The earlier introduction of the social-ecological model of health, in the 1970s, had been directed against the ascendance of the language of personal responsibility, etc., and introduces the idea that preventive health - especially as it depends on behavior change - is embedded in a determinative context. 
[Marmot book; https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(05)71146-6/fulltext; Bronfenbrenner, U. (1979). The Ecology of Human Development: Experiments by Nature and Design. Cambridge, Massachusetts: Harvard University Press. (ISBN 0-674-22457-4)]

The longer trajectory of the development of epidemiological approaches to modeling public health includes the articulation of the "ecological fallacy" and concentration on individual as opposed to aggregate levels of analysis. [Krieger - Revisiting Robinson:... ; could expand with Spider, Summing to 100 and book]

Although there are many approaches to systems thinking, this tension between individual and community levels of analyses in one way or another depends on a theory of the structure of causation that is appropriate to the idea of health. The WHO definition of health as well-being, and not as the absence of disease, however, creates a difficulty for measurement, both at individual and aggregate levels. Healthy outcomes, if not the overcoming of a particular disease, are experienced as a feeling of robust personal, family, and community well-being in physical and psychological terms. We follow the shorthand of speaking of this starting point for measurement as "strengths," and recognize that even if it helpfully keeps us from seeing individuals and communities as passively shaped by circumstance, it still must account for the community-level effects. We propose thinking of these as "constraints," and model them as acting on strengths, both at individual and community levels, but as channeling potential outcomes along independent trajectories rather than as opposed forces within a single encompassing system of forces in opposition to each other. This puts it at odds with the representation of self-interested actors or independent particles exchanging forces within a physically deterministic system.

We should also note how this relates to thinking about language. Constraint, as opposed to determination, in what you name - as you work toward what is worth saying, you choose what you are constrained by in the saying - by an idea of eternal truth, by a sense of responding to the original question? By responding to a universal question? Probably below, but the idea that equality is about numbers being the same; equivalence is about ways of saying being the same; isomorphism is about invertibility of the equivalence (and homotopy as the idea that a continuous transformation between topologies is possible); adjunction is that they are describing paired ways of describing in the same regime. At some point, also worrying about what it means for the feeling of constraint to replace the idea of a positive intuition [in either Kant or Husserl] and how that leads to a thinking of the obligations of the continuim - i.e., to operating within the spaces that allow one to speak well, with form.

If you're trying to really work through the idea of coherence / equivalence, part of the point of the stories is that you don't have to get to a one-to-one correspondence to get at the coherence of the situation and an understanding of the way toward a better space of agreement.

More importantly, the point behind the matching on Sam is that you're trying to match along trajectories - each stage as a percentage of the choices applied at individual level, not a percentage versus the total context. Rethinking possibility.

Telling a story is a preorder - whether you're trying to tell a story with statistics or a narrative; with categories or by situating in a space.

The ethics of telling a story that is true - and figuring out how to make that truth resonant with a future that is well-formed, and not one that is simply "more" is TD

Note on Hilbert spaces and PCA sorts of things?

Thoughts about how to motivate the solving of the census question as a linear equation and the idea of the space / adjoints to that space...
https://www.youtube.com/watch?v=uQhTuRlWMxw along with Lawvere distributions

Note on channeling vs. perspective; operator spaces vs. multiplying complexity

Note on outcomes as points on the trajectory in the operator space and not as endpoints with their own orthorgonality, etc.; that this gets you to quantum vs. relativistic / Riemannian ideas of continuity. 

How it avoids/subsumes the Markov condition.

Group theory; rings. 

Hypergraphs

## Census data and individuals as determined by the intersection of factors
This is broadly true of any way we talk about factors determining an outcome, but here we are talking about what it means for a neighborhood to represent a structure that determines outcomes. Eventually, we will speak of more traditional biomedical outcomes, like heart attacks and diabetes, but begin with determinate descriptive outcomes like whether an individual is married, has children, or is employed. The census data is provided to the public in a way that masks individual identities but allows us to demonstrate the power of representing constraint as the structuring movement of the developing identities. 

For our beginning example, the American Community Survey takes decennial census data and updates it every year based on models generated from a large and detailed survey (~3 million people and ?? questions). The modeled population is available to the researchers who created it at the individual level, but the public is given tables at the census tract block group level [explain], with separate tables that contain estimated error margins. Differential privacy has been added as a feature for the 2020 data set, but they were already moving some respondents into similar census tracts in order to conceal identities. Of course, one of the more disturbing aspects of our data capacities in the last decade has been to render that bashful approach to shileding individual data essentially moot. Business interests, and political parties, now boast of incredibly detailed knowledge (or well-educated guesses) concerning almost every real resident at the household level. Our purpose was heuristic, and allows for control of the mapping process without concerns about having unethically accessed individual data. [cf. Census' history of privacy protections: https://www.census.gov/library/visualizations/2019/comm/history-privacy-protection.html]

Take, for example, the tables for race, ethnicity, age and gender, as provided for Harris County, Texas, which encompasses most of Houston. 
```{r table_demographics}
#ACS data 2017 
ACS_sar_2017 <-as.data.table(sex_by_age_race_data_from_census)
FW_ACS_SAR_2017 <- ACS_sar_2017[str_detect(label,"Female")&!str_detect(concept,"O")]
FW_ACS_SAR_2017 <- FW_ACS_SAR_2017[order(label),list(`label`,`211600`,`211300`,`211100`,`211200`,`211000`,`211700`)]
FW_ACS_SAR_2017
#make a table for the race_eth_age_sex
```
Then look at it as a plot? (have to clean up for the x and y)
Point is that the structure they're preserving is not the total population at the tract or group level, but somewhere higher. The Margin of Error for an SMSA, if you had the embedding the way you'd expect, should be a similar percentage of the total - which would put it at +- a million, not the thousands listed. 
What is being conserved, at the end of the day, is the view from the person who is describing and not the view of the one being described - this takes a while to motivate, but the sense is that we don't see ourselves as statistics because we don't see ourselves as being in an instantaneous state, without a trajectory through time.
The census has announced that it will use differential privacy methods in future releases of data (starting with 2020) and it has used a number of different ways to both store and release data that mask the underlying identities of respondents. [https://www.census.gov/content/dam/Census/library/working-papers/2018/adrm/Disclosure%20Avoidance%20Techniques%20for%20the%201970-2010%20Censuses.pdf; Zayatz, L. (2007). Disclosure avoidance practices and research at the U.S. census bureau: An update. Journal of Official Statistics, 23(2), 253.] The American Community Survey also reviews its releases for various levels, and what one finds is that even at the census tract level, the numbers of people in the cell of a crosstab will be improbably set, as a combination of data suppression and swapping means that the reported numbers are not usable at that level of analysis. If you look at just one section of ages by gender, before getting into the further specification around race and ethnicity, you see implausibly (but not impossibly) uneven distributions.
```{r specification_plot}
sex_by_age_spec <- sex_by_age_race_data_from_census[order(sex_by_age_race_data_from_census$concept,sex_by_age_race_data_from_census$label),2:19]
sex_by_age_spec[1:24,c(1,16:18)]
```

How would we decide if this distribution is by chance? Let's look at a couple of graphs? Maybe it's too much to ask people to think about the whole paradigm of statistics as subject-based point of seeing and judging whether something is likely to have been by chance or has a cause, is part of the issue. Is a Bayesian prior enough to solve the issue? Can we bring that to bear, here? There are two different implicit pictures of the underlying geometry involved. In one, the intersection of forces occurs at a surface that is determined by the current situation - an assessment of the data or evidence given - and lines of force are drawn back from it into various causal systems that might explain the event. In that geometry, each system might have a different logic of organization, but they all come together, in the form of a representation of the given fact at a given time (which can be abstractly large, but has a unity). The other geometry, stops being representable as "a" geometry, and it has lines of force that move through what is given in the evidence, and that don't necessarily converge in a single moment, or determinate being as "the case" or the "way things are, were, or will be" as determined. The example here can make that sense of topology clear, but let's take several steps.
First, what steps might you take to decide if the distribution in question is statistically likely? It's a question that depends on a number of decisions one must make about the categories at stake. For example, if one looks at the female population in 211600 (lower Fifth Ward), one can regroup to have the same size age gaps, and ask how likely it is that 65-69 has 0, and 60-64 has 107. Or, that 70-74 has 64, 80-84 has 58, but 75-79 has 0. [The point being that the way in which that collides with the need to distribute by race, and across other tracts drives the distribution differently than just throwing the dice at each square; matching by the distribution in the decennial? (which has each year)] - could try to do it as solving simultaneous equations or as a group of possible linked transformations. (like guessing how it fits) 
The underlying smoothness of the decennial census could be one of the structures they preserved, but instead they made sure each cell fell within a range...

[How the ACS gets it wrong: The story of the American Community
Survey and a small, inner city neighborhood; Applied Geography, 2013]
[NRC on Group Quarters from 2012 - Small Populations, Large Effects]
[NAS, 2007 - Using the American Community Survey: Benefits and Challenges]
[Folch, 2016 Spatial Variation in the Quality of the American Community Survey Estimates]
[Swanson, 2012, An Evaluation of Persons per Household (PPH) Estimates Generated by the American Community Survey: A Demographic Perspective]
[download 500 cities data for Census tract; https://chronicdata.cdc.gov/browse?category=500+Cities+%26+Places&sortBy=newest&utf8]
[OData endpoint for 500 cities; https://chronicdata.cdc.gov/api/odata/v4/yjkw-uj5s ; https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Census-Tract-Data-GIS-Friendly-Format-2020-/yjkw-uj5s;
https://www.cdc.gov/places/about/500-cities-2016-2019/index.html
https://www.urban.org/sites/default/files/publication/90376/500-cities-project_1.pdf]
[Kong, 2020 - The Use of Small Area Estimates in Place-Based Health Research (we have pdf)]
[Gourevitch, 2019: City-Level Measures of Health, Health
Determinants, and Equity to Foster Population
Health Improvement: The City Health Dashboard]
[Small Area Estimation:
500 Cities, SMART and BRFSS Data for New Orleans and Baton Rouge; 2016 - compares ranges for health outcome estimates]
[SAE_Drexel.pdf explains how small area estimation is done]
[cluster analysis - 2018_PCD_500cities.pdf]
[State and Local Chronic Disease Surveillance Using Electronic Health Record Systems, 2017]



[I remember an article comparing 500 cities to a small known neighborhood and saying they only used some of the variables to avoid overfitting]



## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
x <- seq(-10, 10, length= 30)
y <- x
f <- function(x, y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r }
z <- outer(x, y, f)
z[is.na(z)] <- 1
op <- par(bg = "white")
persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")
persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue",
      ltheta = 120, shade = 0.75, ticktype = "detailed",
      xlab = "X", ylab = "Y", zlab = "Sinc( r )"
) -> res
round(res, 3)
```

```{r volcano}
z <- 2 * volcano        # Exaggerate the relief
x <- 10 * (1:nrow(z))   # 10 meter spacing (S to N)
y <- 10 * (1:ncol(z))   # 10 meter spacing (E to W)
## Don't draw the grid lines :  border = NA
par(bg = "slategray")
persp(x, y, z, theta = 135, phi = 30, col = "green3", scale = FALSE,
      ltheta = -120, shade = 0.75, border = NA, box = FALSE)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
