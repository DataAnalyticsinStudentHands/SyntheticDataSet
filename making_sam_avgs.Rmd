---
title: "Making Sam Households"
author: "Dan Price"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
geometry: margin=1in
fontsize: 11pt
documentclass: article
header-includes: 
  - \usepackage{tikz}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Preliminaries -- very broad
https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html for options on the knit - results="hide"; echo=FALSE; include=FALSE, etc.

```{r prelims}
source('BaseScripts/Census_Data.R') #move out of BaseScripts?
source('tests.R')
library(tidyr)
library(dplyr)
library(stringr)
library(data.table)
#maindir = "~/University Of Houston/Price, Daniel M - Social Network Hypergraphs/"
maindir = "~/Downloads/UH_OneDrive/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at home
#maindir = "~/Downloads/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at work
housingdir = paste0(maindir,"HCAD/")
houstondatadir = paste0(maindir,"HoustonCityData/") 
censusdir = paste0(maindir,"Census/") 
vintage = "2019"
housingStockFromRDS = TRUE 
#numberOfCores = 1
state = 48 #48 Texas; 22 Louisiana
county = 201 #8 county region: 201 Harris; 157 Fort Bend; 167 Galveston; 039 Brazoria; 071 Chambers; 291 Liberty; 339 Montgomery; 473 Waller ; other place FIPS are longer
tract = "*"
Sam_seed = 135
#you don't need a censuskey if you're not pulling new files down; you can only use this one if you have correct access to the OneDrive
censuskey <- readLines(paste0(censusdir, "2017", "/key"))
```


For new file, and need a bit of a preamble

We're going to begin with the 2010 block group data table, P28 - "Household Type by Household Size" (by Race and Ethnicity). We start here not because it provides the greatest granularity, although it is by block group and is the most granular data available for these tables, but because P28 has the best _structured_ relationality, around which commutative triangles can be built. The longer idea is to see the census bureau's copy of the full data representation as being both perfectly commutable in the ways it can be represented and granular at an individual level (again, setting aside difficulties with their collection and category choice in order to concentrate on the choices made that give structure to the whole). There are then ways in which the census bureau produces tables, with different concerns at different stages and with different products, but the statisticians at the Census Bureau both try to ensure the structure of commutability is maintained within the particular representation and to ensure that the people interpreting the data understand how it relates statistically as a subset - which is to say, how the granularity of the presentation is related to the underlying granularity as the chance that when you see someone from a particular subset, the individuals in that subset fit into the demographics of the larger group. [that could be better phrased, but trying to get at difference between set theory and type theory with the categorical spin]

#Structure and representation

Let's download P28, "Household Type by Household Size" (by Race and Ethnicity) and look at it.

```{r download hhtype hhsize race eth bg}
dec_bgHH_type_size_re_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P28",county_num = "201",
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
dec_bgHH_data_10 <- as.data.table(dec_bgHH_type_size_re_data_from_census_10) 
```

Then do our basic checks:
```{r hh_type_size_re test problems}
check_summary <- census_table_check(dec_bgHH_data_10, "HOUSEHOLD TYPE BY HOUSEHOLD SIZE","households") #should get a function that fails appropriately
cat(check_summary[1])
```

Now we need to convert the census representation into a representation of individual households (move from a wide to a long table format).

```{r expand P28 into dec_bgHHr_10 and dec_bgHHe_10}
    #break into race and ethnicity files
    race_codes <- c("A","B","C","D","E","F","G")
    dec_bgHHr_10 <- dec_bgHH_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      mutate(
        race = substr(name,5,5),
        race_text = tolower(str_extract(concept, "(?<=\\().*(?=\\))")),
        race_text = str_remove_all(race_text," householder"),
        label = str_remove_all(label,"Total!!")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      separate(label, c("family","hh_size"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(hh_size_n = as.numeric(substr(hh_size,1,1))) %>%
      filter(!is.na(hh_size) & race%in%race_codes) %>% #to get rid of aggregations by family
      uncount(number_sams,.id = "hhr_size_id",.remove = TRUE) 
    dec_bgHHr_10 <- as.data.table(dec_bgHHr_10) #dyplyr had stripped it of dt
    paste0("Number of households in file: ", nrow(dec_bgHHr_10))
    print("Column names for dec_bgHHr_10, block group households by race:") 
    colnames(dec_bgHHr_10)
    
    #and same for ethnicity
    dec_bgHHe_10 <- dec_bgHH_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      mutate(
        ethnicity = substr(name,5,5),
        label = str_remove_all(label,"Total!!")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      separate(label, c("family","hh_size"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(hh_size_n = as.numeric(substr(hh_size,1,1))) %>%
      filter(!is.na(hh_size) & ethnicity%in%c("H","I")) %>% 
      uncount(number_sams,.id = "hhe_size_id",.remove = TRUE) 
    dec_bgHHe_10 <- as.data.table(dec_bgHHe_10) #dyplyr had stripped it of dt
    paste0("Number of households with ethnicity attributes in file: ", nrow(dec_bgHHe_10))
    print("Column names for dec_bgHHe_10, block group households by ethnicity:") 
    colnames(dec_bgHHe_10)
    #clean up
    rm(dec_bgHH_data_10)
    rm(dec_bgHH_type_size_re_data_from_census_10)
    
```

#Conserving Mathematical Structures in Representations
H12 is AVERAGE HOUSEHOLD SIZE OF OCCUPIED HOUSING UNITS BY TENURE and also has race/eth - there should be a triangle to do somewhere.
Now let's get P17, average household size by age (with people under 18 in household or not) and race and ethnicity by block group. It adds the question of what happens with other sorts of representations besides summation or subset, with average being easy to understand and important to the underlying idea of maintaining structure. The additional information is a hint about how households with people under 18 differ from those without, by race and ethnicity. Since there is also a total, which we can generate separately from the dec_bgHHr_10 file we can check our process as well as give further shape to the representation for the two age groups, assigning values within the known distribution and maintaining the structures reported in the various tables.  

```{r download avg hhsize race eth bg}
dec_bgHH_avg_size_re_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P17",county_num = "201",
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
dec_bgHH_avg_size_data_10 <- as.data.table(dec_bgHH_avg_size_re_data_from_census_10)
```
Then we see that our basic checks aren't exactly meaningful - the averages, as reported internally, are rounded to the hundreds and so do not always sum:
```{r hh_avg_size_re test problems}
dec_bgHH_avg_size_data_10[label=="Average household size!!Total",("label"):="Total"]
#because they're averages, the sums are often off by a decimal place so 75% are off
check_summary <- census_table_check(dec_bgHH_avg_size_data_10,"AVERAGE HOUSEHOLD SIZE BY AGE","households",1) #should get a function that fails appropriately
cat(check_summary[1])
```

We are pulling in the separate reporting table P17, "Average Household Size" (by Race and Ethnicity), in order to show how one retains the structure of the average without violating the other commutative structures. Because we can already generate some of the numbers internally to P28, we can validate at least some aspects of our work. Our strategy will be to order each household in a subtype (block group x race) as either above or below the average. For each subtype, we already know the distribution, and our first task will be to show that we can maintain the structure of the average in the new representation while still respecting the other distributions. Since they are distributed into a set number of places afterward, we don't need to approximate a normal distribution in advance. We'll do it first for the total average household size by race, which is a known amount and can be generated directly from the table in P17 and as a calculation on the data in P28. Then we'll generate the subtypes from the averages for households with members below 18 and those with only members above 18 years of age. In order to do that, we need to know the number of households in each category, which we'll be able to add from P19 (Household Size by Household Type by Presence of Own Children) and P20 (Households by Presence of People Under 18 Years by Household Type by Age of People Under 18 Years), although not by race. The challenge is whether a technique that depends on a deliberately constructed space can provide a meaningful distribution of individuals within that space, and which respects the structures that were important in the representation of the problem.  Our eventual purpose is to generalize beyond the current example by showing concretely the sense in which a structured space is a more effective way of representing the process of analysis through a guided abstraction; showing how multiple structures can be respected in the construction of a solution is an important step in that process.  

Now we'll reshape P17 (Average Household Size) to use more efficiently later the average number of households by both race and ethnicity, for households with and without people under 18 in them, for each block_group. 

```{r expand P17 into dec_bgHH_avg_size_data_10}
    dec_bgHH_avg_size_10 <- dec_bgHH_avg_size_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_avg_size_data_10),names_to = "geoid", 
                   values_to = "avg_hh_size") %>% 
      mutate(race = substr(name,5,5))   
    dec_bgHH_avg_size_10 <- as.data.table(dec_bgHH_avg_size_10) #dyplyr had stripped it of dt

    rm(dec_bgHH_avg_size_re_data_from_census_10)
```

As a quick check, the averages directly reported as "Total" in P17 should match calculations on the actual description in P28. We will check those, first, as proof that we can make the simplest case work, and then construct the pieces that allow for a sub-typing by age of the members of the household to be represented in the distribution of the data. We'll test on total, "Black or African American Alone," and "White Alone, not Hispanic or Latino." The two will not match perfectly, because P17 can have numbers greater than 7 for a household and may have some other differences in collection. P17 has averages for households with and without people under 18, by all the same categories, which is the only thing that is added beyond P28.

```{r compare P17 dec_bgHH_avg_size avg to P28 dec_bgHH avg}
  dec_bgHHr_10 <- as.data.table(dec_bgHHr_10)
  dec_bgHHr_10[,("avg_hh_size_bg"):=round(mean(hh_size_n),2),by=.(geoid)]
  
  dec_bgHHr_10[,("avg_hh_size_bg_r"):=round(mean(hh_size_n),2),by=.(geoid,race)]
  
  dec_bgHHe_10[,("avg_hh_size_bg_e"):=round(mean(hh_size_n),2),by=.(geoid,ethnicity)]
  
  paste0("All")
  summary(dec_bgHH_avg_size_10[name=="P017001",avg_hh_size])
  summary(dec_bgHHr_10[,(avg_hh_size_bg)])
  paste0("Black")
  summary(dec_bgHH_avg_size_10[name=="P017B001",avg_hh_size])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r)])
  paste0("Hispanic")
  summary(dec_bgHH_avg_size_10[name=="P017H001",avg_hh_size])
  summary(dec_bgHHe_10[ethnicity=="H",(avg_hh_size_bg_e)])
  
  rm(dec_bgHH_avg_size_data_10)
```

We remark immediately on the maximum value for Black households being 10.5, when the highest reported value in P28 is 7. The block group in question (48_201_233703_2) turns out to only have two Black households listed in P28, both in the "7-or-more-person-household"; the same block group is responsible for the max value in both. There are 293 households total in that block group and we did a deep dive into the individual household level data. We are assuming on that basis that it's not a reporting mistake, but just an unusual situation. Remember that the summary statistics are for all the averages across block groups and that only the average for that block group is reported in P17. For the total households in the block group that has the outliers (48_201_233703_2), the summary across all the households in P28 has a median a little higher than the mean, but nothing out of the ordinary and relatively close to the reported mean in P17. The fact that the actual values are whole numbers actually gives us a clue for how to think about the distribution we aim to do in the end. [could do the reverse engineering of the numbers to say what the number in the households could be?]

```{r 48_201_233703_2 summary for households}
summary(dec_bgHHr_10[geoid=="48_201_233703_2",(hh_size_n)])
paste0("Reported average in P17: ",dec_bgHH_avg_size_10[name=="P017001"&geoid=="48_201_233703_2",avg_hh_size])
```

We will implement a matching process on the households in two steps. First, we create a number for every household that is either above or below the average for that race category, by geoid and race, using sample() to create a random distribution centered on the average. Since there is no other information on the dataset, we don't have to worry about matching individual households with attributes that would influence how many people are in the household. After assigning them a number, we will order the households across the subtypes in the category that is one higher in the embedding (in this case, all households, but could also be a larger geographic area) by that new number and assign the available sizes as a vector. This preserves the structure of the differing averages - even given that we do not know anything else about the distribution. We are testing it this time, since we have a ground truth for households by race. If it works, we will then use the same technique to create sub-types by households with and without persons under 18 by race. The last by=.(geoid) keeps the statistical structure constant at the geoid (block group) level, but a by=.(geoid,race) would keep each race having the same outcomes within each of those categories. We will explore, in [later section title] below, how this idea of embedding the types within real and conceptual spaces is embodied in the structure of the data tables. 

```{r add match logic to household size test}
    #create number for each hh on both sides of the average by race
    dec_bgHHr_10[,("match_avg_r_num"):=avg_hh_size_bg_r+
          sample((-.N/2):.N/2,.N,replace=FALSE)/.N,by=.(geoid,race)]
    #order all races together by the new number and match with tract ordered by hh_size_n; 
    dec_bgHHr_10[order(match_avg_r_num),("hh_size_generated"):=.SD[order(hh_size_n),"hh_size_n"],by=.(geoid)]
    
    #let's also create something that shows how much diff it can make; 
    #any distribution could be imposed, although it will always be constrained 
    #by the choice of level where the available categories are kept constant
    dec_bgHHr_10[race!="D",("match_example_r_num"):=avg_hh_size_bg_r+
          sample((-.N/2):.N/2,.N,replace=FALSE)/.N,by=.(geoid,race)]
    #and then with Asians, we'll push it toward the high end
    dec_bgHHr_10[race!="D",("match_example_r_num"):=avg_hh_size_bg_r+
          sample(.N:.N*2,.N,replace=FALSE)/.N,by=.(geoid,race)]
    #order all races together by the new number and match with tract ordered by hh_size_n; 
    dec_bgHHr_10[order(match_example_r_num),("hh_size_example"):=.SD[order(hh_size_n),"hh_size_n"],by=.(geoid)]

```

Let's look at some statistics to see how the newly generated household size looks compared to the originals. First, let's confirm that we didn't accidentally assign the household size to the same households. We're looking for a relatively random distribution, just showing that the old hh_size_n is independent from the new hh_size_generated. [not sure this is helpful, but I had to do it because the first way I had done the script screwed up with the vectorization and assigned the same numbers to everyone in the subgroup; couldn't see it at highest level of reporting, but only when making sure that each individual had their own number assigned individually]

```{r table confirming hh_sizes are indep}
table(dec_bgHHr_10$hh_size_n,dec_bgHHr_10$hh_size_generated)
table(dec_bgHHr_10[geoid=="48_201_233702_1",hh_size_n],dec_bgHHr_10[geoid=="48_201_233702_1",hh_size_generated])
```

Then let's look at the same summary statistics we had produced before, with the newly generated household sizes included. This should give us a sense for how it worked globally. We can do this for all the distributions, and there are statistical tests for representing the difference in the distributions, but right now we're just getting a general sense. 

```{r summary for hh_size_generated}
  dec_bgHHr_10[,("avg_hh_size_bg_g"):=round(mean(hh_size_generated),2),by=.(geoid)]
  dec_bgHHr_10[,("avg_hh_size_bg_r_g"):=round(mean(hh_size_generated),2),by=.(geoid,race)]
  dec_bgHHr_10[,("avg_hh_size_bg_ex"):=round(mean(hh_size_example),2),by=.(geoid)]
  dec_bgHHr_10[,("avg_hh_size_bg_r_ex"):=round(mean(hh_size_example),2),by=.(geoid,race)]
  
  paste0("All")
  #order is published average, 
  #calculated average from P28, 
  #calculated average from newly generated household size
  #example for Asian population pushed toward higher household size
  
  summary(dec_bgHH_avg_size_10[name=="P017001",avg_hh_size])
  summary(dec_bgHHr_10[,(avg_hh_size_bg)])
  summary(dec_bgHHr_10[,(avg_hh_size_bg_g)])
  summary(dec_bgHHr_10[,(avg_hh_size_bg_ex)])
  paste0("White")
  summary(dec_bgHH_avg_size_10[name=="P017A001",avg_hh_size])
  summary(dec_bgHHr_10[race=="A",(avg_hh_size_bg_r)])
  summary(dec_bgHHr_10[race=="A",(avg_hh_size_bg_r_g)])
  summary(dec_bgHHr_10[race=="A",(avg_hh_size_bg_r_ex)])
  paste0("Black or African American")
  summary(dec_bgHH_avg_size_10[name=="P017B001",avg_hh_size])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r)])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r_g)])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r_ex)])
  paste0("Asian")
  summary(dec_bgHH_avg_size_10[name=="P017D001",avg_hh_size])
  summary(dec_bgHHr_10[race=="D",(avg_hh_size_bg_r)])
  summary(dec_bgHHr_10[race=="D",(avg_hh_size_bg_r_g)])
  summary(dec_bgHHr_10[race=="D",(avg_hh_size_bg_r_ex)])
  
```

Let's also look at the variation within a single block group for the household size, and not for the averages across all the block groups. Notice that everything but the means are whole numbers, since it's picking out actual values for households at those indexed spots.

```{r variation generated hh_size bg}
summary(dec_bgHHr_10[geoid=="48_201_233702_3",(hh_size_n)])
summary(dec_bgHHr_10[geoid=="48_201_233702_3",(hh_size_generated)])
paste("And for all of Harris County: ")
summary(dec_bgHHr_10[str_detect(geoid,"48_201"),(hh_size_generated)])
```

```{r cleanup after avg example on dec_bgHHr_10}
  dec_bgHHr_10[,c("avg_hh_size_bg","avg_hh_size_bg_r","match_avg_r_num",
                  "match_example_r_num","hh_size_generated","hh_size_example",
                  "avg_hh_size_bg_g","avg_hh_size_bg_r_g","avg_hh_size_bg_ex",
                  "avg_hh_size_bg_r_ex"):=NULL] 
```

[have to think about what other statistical tests to show]

Let's recap why that was an interesting digression. Maybe a drawing of the commutative relationships and then what it means to retain certain mathematical structures while still transforming other parts of the relation (i.e., a gentle introduction into the difference between functions and functors?). Can average household size, for example, also help to add in the Hispanic population? Can we use that trick backwards to then construct more subtypes?


Our plan is to add the information about the average household size for households with and without people under 18, using this technique to add a dependent structure within the conceptual space of the representation. If we can take advantage of the existing relational structure, however, we can ensure that the structure is respected and the final representation is more reflective of the underlying relation. Since our starting point for the households included household size and household type, as well as race and ethnicity information, let's see if any of that can be added to the averages before using them in the overall representation. 
We find that P18, for example, gives you the number of householders living alone and the ones that are living with others. P22 gives you the age of the householder by type, which would help us know if there are under 18 year olds living alone (there are a few). Then P20 has type by whether the household has people under 18 connected to household type. P21 tells us how many have their own children in the household by household type. Only P18 also has race and ethnicity, but each will allow us to get closer to specifying the complete construction of all the possible relations in a way that respects the underlying commutativity of relations, as originally conceived by the census bureau to be a complete and coherent representation of the population. Then we can return to P17 and distribute household size with reference to average and still retain the structures represented by the other tables.

[We want the preceding section to display why simply taking the average and adding it is a problem - setting up what it means to be deploying individuals into structured relations, and then saying, explicitly, that the triangles where you know the construction of both sides gives you something interesting - that we can then go a further step and talk about triangle commutativity on things that aren't immediately sets is the final key for the utility of the work for thinking about spaces and the social determinants of health, etc.]

##next up is making_sam_hh.Rmd 


