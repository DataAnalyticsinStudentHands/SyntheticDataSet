---
title: "Making Sam"
author: "Dan Price"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
geometry: margin=1in
fontsize: 11pt
documentclass: article
header-includes: 
  - \usepackage{tikz}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Preliminaries -- very broad - 


#every so often this chunk causes R to abort!!! Not sure why - perhaps something about the "source"? 
```{r prelims, include=FALSE}
library(tidyverse)
library(data.table)
library(gtsummary)
maindir = "~/Downloads/UH_OneDrive/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at home
#maindir = "~/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at work
housingdir = paste0(maindir,"HCAD/")
houstondatadir = paste0(maindir,"HoustonCityData/") 
censusdir = paste0(maindir,"Census/") 
vintage = "2010"
#numberOfCores = 1
state = "48" #48 Texas; 22 Louisiana
county = "201" #8 county region: 201 Harris; 157 Fort Bend; 167 Galveston; 039 Brazoria; 071 Chambers; 291 Liberty; 339 Montgomery; 473 Waller ; other place FIPS are longer
st_county = paste0(state,county) #"48201"
tract = "*"
Sam_seed = 135
#you don't need a censuskey if you're not pulling new files down; you can only use this one if you have correct access to the OneDrive
censuskey <- readLines(paste0(censusdir, "2017", "/key"))
source('BaseScripts/Census_Data.R') #move out of BaseScripts?
source('tests.R')
```

Include and comment on diagram: https://q.uiver.app/?q=WzAsOSxbMSwwLCJmdWxsXFw7cG9wdWxhdGlvblxcXFxhc1xcO3BlclxcXFxDZW5zdXNcXDtCdXJlYXUiXSxbNSwyLCJ0cmFjdFxcO3hcXDthZ2VcXF8xMTBcXDt4XFxcXHJhY2VcXF83XFxcXGV0aG5pY2l0eVxcXzhcXFxcUENUMTIiXSxbMTAsMCwiYmxvY2tcXDt4XFw7YWdlXFxfMjNcXDt4XFxcXHJhY2VcXF83XFxcXGV0aG5pY2l0eVxcXzJcXFxcUDEyIl0sWzEsNiwiYWdlXFw7eFxcO3NleFxcO3hcXFxccmFjZVxcXzdcXDt4XFw7ZXRobmljaXR5XFxfOFxcO3hcXFxcdHJhY3RcXDt4XFw7YmxvY2siXSxbMTAsNSwidHJhY3RcXDt4XFw7YmxvY2tcXDt4XFxcXHJhY2VcXF83XFxcXGV0aG5pY2l0eVxcXzgiXSxbMSw1XSxbNCwzLCJyYWNlXFw7YW5kXFxcXGV0aG5pY2l0eVxcXFxmdWxseVxcXFxjb21tdXRlXFxcXGJ5XFw7ZGlzanVuY3Rpb24iXSxbOSwxLCJyYWNlXFw7YW5kXFxcXGV0aG5pY2l0eVxcXFxkb24ndCBcXDtjb21tdXRlXFxcXGZvciBcXDtub25cXF93aGl0ZVxcXFxIaXNwYW5pY1xcO29yXFw7TGF0aW5vIl0sWzAsNSwiYXNzaWduXFw7YmxvY2tcXDtsZXZlbFxcXFxldGg2XFw7YnlcXDtzYW1wbGVcXFxcdHJhY3RcXDthbmRcXFxccmVzdFxcO29mXFw7YmxvY2tcXFxcZnVsbHlcXFxcY29tbXV0ZXMiXSxbMCwxLCJyZXBvcnRzXFxcXChzdWJzZXRzKSIsMSx7ImNvbG91ciI6WzEyMCw2MCw2MF19LFsxMjAsNjAsNjAsMV1dLFswLDIsInJlcG9ydHNcXFxcKHN1YnNldHMpIiwxLHsiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzEyMCw2MCw2MCwxXV0sWzEsNCwiam9pbnNcXFxcKHNvbWUgXFw7bG9zc1xcXFxibG9ja1xcO3RvXFw7dHJhY3QpIiwxLHsiY29sb3VyIjpbMjQwLDYwLDYwXX0sWzI0MCw2MCw2MCwxXV0sWzIsNCwiam9pbnNcXFxcKHNvbWUgXFw7bG9zc1xcO2V0aDYpIiwxLHsiY29sb3VyIjpbMjQwLDYwLDYwXX0sWzI0MCw2MCw2MCwxXV0sWzQsMywiY3JlYXRlc1xcXFxpbml0aWFsIiwxLHsiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzEyMCw2MCw2MCwxXV0sWzAsMywidW5rbm93blxcXFxmdW5jdG9yXFxcXHN1YnNldHMiLDEseyJjb2xvdXIiOlswLDYwLDYwXX0sWzAsNjAsNjAsMV1dLFs2LDEsImNvbW1lbnQiLDEseyJjb2xvdXIiOlszMDAsNjAsNjBdfSxbMzAwLDYwLDYwLDFdXSxbNywyLCJjb21tZW50IiwxLHsiY29sb3VyIjpbMzAwLDYwLDYwXX0sWzMwMCw2MCw2MCwxXV0sWzgsMywiY29tbWVudCIsMSx7ImNvbG91ciI6WzMwMCw2MCw2MF19LFszMDAsNjAsNjAsMV1dXQ==
or:
https://q.uiver.app/?q=WzAsMTAsWzEsMCwiUENUMTJcXFxcdHJTQVJFIl0sWzAsMiwidHJTQVJcXFxccmFjZVxcXzdcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8xMTAiXSxbMiwyLCJ0clNBRVxcXFxldGhuaWNpdHlcXF84XFw7eFxcXFxzZXhcXDt4XFw7YWdlXFxfMTEwIl0sWzEsMSwidG90YWxzXFxcXHNhbWVcXDtieVxcXFx0cmFjdFxcO2FuZFxcXFxjb21wbGV0ZVxcO2J5XFxcXGJvdGhcXDtyYWNlXFw7YW5kXFxcXGV0aG5pY2l0eSJdLFsxLDIsIlxcYnVsbGV0Il0sWzEsNywiUDEyXFxcXGJnU0FSRSJdLFswLDUsImJnU0FSXFxcXHJhY2VcXF83XFw7eFxcXFxzZXhcXDt4XFw7YWdlXFxfMjMiXSxbMiw1LCJiZ1NBRVxcXFxldGhuaWNpdHlcXF8yXFw7eFxcXFxzZXhcXDt4XFw7YWdlXFxfMjMiXSxbMiw0XSxbMiwzLCJcXGJ1bGxldCJdLFswLDEsInN1YnNldHMiLDEseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbMCwyLCJzdWJzZXRzIiwxLHsiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzEyMCw2MCw2MCwxXV0sWzEsMl0sWzIsMSwiY29tbXV0ZXMiXSxbMyw0LCJ0YWJsZXMgPSIsMSx7ImNvbG91ciI6WzMwMCw2MCw2MF19LFszMDAsNjAsNjAsMV1dLFs1LDYsInN1YnNldHMiLDAseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbNSw3LCJzdXNldHMiLDIseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbNiw3XSxbNyw2LCJkb2VzXFw7bm90XFxcXGNvbW11dGUiLDAseyJjb2xvdXIiOlswLDYwLDYwXX0sWzAsNjAsNjAsMV1dLFsyLDcsImpvaW5cXDtieVxcXFxzZXhcXDt4XFw7YWdlXFxfMjNcXDt4XFxcXHJhY2UiLDEseyJjb2xvdXIiOlsyNzAsNjAsNjBdfSxbMjcwLDYwLDYwLDFdXSxbOCw2LCJjb21tdXRlc1xcXFxzb21lXFw7bG9zc1xcXFxhc3NpZ25lZCIsMV0sWzEsOV1d

Probably the second, and "some loss assigned" means that there's a potential for non-commutativity going forward at the block level for race and ethnicity - but it's minimized by the tract level maintaining its commutativity. Nothing is lost, in terms of possible future joins, at levels / places where full commutativity is kept and the specifications within do not change other relations.
Could start a file that marks that loss, with some numbers!

In a broad sense, we're asking - at a practical level - what's it mean to use .map on the construction of epidemiology's domain and co-domain as explicitly about how we talk about vs. how relations build on each other. Not the convergence toward a single perspective, but toward a shared commitment to speaking about what matters in the relations.

For restructuring of argument part - that the construction "from the outside" of the type theory lets you resolve the arguments without positing the consistency of the inside as given - you don't have to believe the axiom of reducibility or that there is a zero-degree homotopy (should we try to show that things like that are equal?)

Plan: find smallest units of "space" (geographic and/or conceptual) from 2010 census and build a cohesive representation of Harris County (called Sam City) by piecing together the various representations output by the census. 
By "space" we mean something that helps situate a characterization within a structure. A context is an example of that sort of space, as your neighborhood provides context for your dwelling, but part of the point of our exercise with Sam City is to show other ways to think about the structuring of "space" and the ways in which an attention to space allows one to better engage in structuring data representations that are fully responsive to their originating questions. Such a project has basic implications for how we think about neighborhood effects on individuals, most immediately in terms of the granularity we build into the representations and our sense for whether that matters for the interpretation.
As we saw in the previous section on problems with the American Community Survey, there are practical reasons we need to move back to the 2010 decennial census, especially it's detailed geographic reports. There are two main technical documents for the tables created for smallest area (block group) demographic data. 
The two have similar tables, but different levels of detail provided. They provide some information on how to download the files, although we're using the R library censusapi in our script, CensusData.R, to make calls.
If you want to look at the details, go to the List of Tables (Matrices) to see what sorts of reports are produced. The first is populations summarized (with a few exceptions) at the block level:
https://www2.census.gov/programs-surveys/decennial/2010/technical-documentation/complete-tech-docs/summary-file/sf1.pdf
and the second is for the tract level summaries:
http://www2.census.gov/programs-surveys/decennial/2010/technical-documentation/complete-tech-docs/summary-file/sf2.pdf
The table names that begin with PCT never go below the tract level, but occassionally have more detail in some of the concepts. 
Let's begin with PCT12, which is tract level data but has every age by year, gender, ethnicity and race. It has 101 possible age groups, with 1-99 years and then one for "100 to 104 years" and one for "105 to 109 years". It then has categories for all the built in race and ethnicity categories, with each person counted in the total, and every person categorized by race (there are tables for how many people had to have a race assigned by the census bureau, since they refused to provide one, but they tell us how many people have an assigned race and describe their protocols); Hispanic is a problematic category for counting, as well as for historical reasons, and is dealt with differently in the decennial and ACS reporting. Let's download table PCT12, do some quick checks, and then look at it.  

```{r look at PCT12}
#this gives you by every year at the tract level - could be a good example for adding together - PCTs never go below tract level
trSAR_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "PCT12",county_num = county,
                         block="tract",api_type="dec/sf1",path_suff="est.csv")
trSAR_data_from_census_10 <- trSAR_data_from_census_10 %>%
      select(name,label,concept,starts_with(st_county))
#needs to be numeric for the sum in the test
trSAR_data_from_census_10 <- as.data.table(trSAR_data_from_census_10)
trSAR_data_from_census_10[,4:ncol(trSAR_data_from_census_10)] <- 
  trSAR_data_from_census_10[,lapply(.SD[,4:ncol(trSAR_data_from_census_10)], as.numeric)]
head(trSAR_data_from_census_10[,1:5])
```

A quick glance shows that this has reporting for every year of age and at fifteen different levels for race and ethnicity, which is considerably more specificity than the reporting under ACS or at the block group level, reported as the number of people who fall into that category for every census tract in the state. That specificity provides a very granular embedding context (or structured space) on the conceptual level but is less specific geographically. It has every tract for the entire state, but some of the larger tracts are divided into 6 block_groups, which provides considerably more geographic context. One of the goals of our current approach is to show why thinking about the reporting on basic concepts like sex and age should be thought of as a similar problem as geographic boundaries - i.e., that both should be thought of as an articulation of structure within the context where the individual is placed in the representation. By paying close attention to that problem of articulating a coherent and meaningful structure, we can make explicit which mathematical operations are justified when transforming between the various possible representations of the data. 

Just to give a sense for how testing for coherent structure arises from and resonates with the broader process of building the data representation, we include some of the very basic ways that we can test for consistency. The following are repeated for each download from the census, as we look for obvious problems with the file either as reported or as a result of an improper download (like asking for the wrong file). In this version, we have put a data.table wrapper around the census data and sum the number of NAs in the columns. Because of how the Census Bureau allows access to its APIs, public users without permission for certain levels of granularity will sometimes return a file with NAs but not consider that a problem and not produce an error. In this case, we decided to represent the number of NAs as a percentage, and not just an absolute count, because sometimes we want to use data that is reported for only some of the rows, even if NAs are reported in some of the other rows. We don't want the download to fail unless there are only NAs, but if there are any at all, we want to know about it and know where to look for the problem. In later downloads, several of these tests are gathered in a single function call in tests.R as census_table_check. One of our ongoing tasks is to better automate those tests.

```{r test for na}
    percent_na <- trSAR_data_from_census_10[,sum(is.na(.SD))] / 
      (trSAR_data_from_census_10[,sum(!is.na(.SD))]+trSAR_data_from_census_10[,sum(is.na(.SD))])
    paste("Percentage of NAs in file:",as.integer(100*percent_na))
    rm(percent_na)
```

After seeing that there are no NAs, we need to check to make sure that the reported totals at least add up - every so often the census makes mistakes that we need to clean up in some way, including very occasionally putting the wrong values in aggregation cells. As we look at the "label" column, there's a row that contains the total for each concept and for the whole. The rows that say only "Total" are given for every version of "concept" that is given. This means that the representations can be embedded in several different ways, while still counting as "total," and we need to account for that in our representation. In the rows that say more than just "total," there are also rows for total female and male, which means that adding all that side up automatically is twice as large as the rows that say "label" only. We had the whole state, so for our case we'll select down to county first. In the GEOID that contains the tract numbers, Texas is 48 and Harris county is 201. https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html

```{r check for consistency on totals}
#there's a total for each concept and for the whole, but in the ones without a total, there are also total female and male.
test <- colSums(trSAR_data_from_census_10[label=="Total",4:ncol(trSAR_data_from_census_10)])*2 ==
      colSums(trSAR_data_from_census_10[label!="Total",4:ncol(trSAR_data_from_census_10)])
#see if any of the tests don't match; if false, need to go back and check on what happened
length(test[test==F])==0
```

If we want to know only the row that gives us the official census total population per census tract, we can add in the designation for concept. We know more having looked at the calculations involving all rows, as above, because if any of the totals had not been equal, we could look for a data problem in the specific place in the tables where the error emerged. Let's get the official totals in a separate file, just so we can use it to check later. (We'll find that we need it right away). 

```{r get totals by tract}
pop_totals_tract <- trSAR_data_from_census_10[label=="Total"&concept=="SEX BY AGE",4:ncol(trSAR_data_from_census_10)]
paste0("Population total for Harris County by tract: ", sum(pop_totals_tract[,]))
```

The same convenience set lets us know some other facts about the variation in size of the tracts in Harris County. Shortly, we'll find better shortcuts for displaying summary statistics, but for now we just create a few quick measures.

```{r Harris tract stats}
paste0("Number of Harris County tracts: ", ncol(trSAR_data_from_census_10[,4:ncol(trSAR_data_from_census_10)]))
paste0("Average population size for Harris County tracts: ", as.integer(mean(as.numeric(pop_totals_tract[,]))))
paste0("Median population size for Harris County tracts: ", as.integer(median(as.numeric(pop_totals_tract[,]))))
paste0("Maximum population size for Harris County tract: ", max(as.numeric(pop_totals_tract[,])))
paste0("Minimum population size for Harris County tract: ", min(as.numeric(pop_totals_tract[,])))
```

The population total for Harris County is what you'll find on the internet as the official population of Harris County in 2010, as long as it's referring back to the Census Bureau (cf, https://www.census.gov/quickfacts/harriscountytexas, accessed on 1/14/22).

Now that we have the information at the tract level, let's also pick up the block level data.

```{r block_group sex by age}
bgSAR_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P12",county_num = county,
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
bgSAR_data_10 <- as.data.table(bgSAR_data_from_census_10)
bgSAR_data_10[,4:ncol(bgSAR_data_10)] <- 
  bgSAR_data_10[,lapply(.SD[,4:ncol(bgSAR_data_10)], as.numeric)]
rm(bgSAR_data_from_census_10)
```

It would be nice to use this immediately as our base, but we quickly see it doesn't have as many rows breaking out race and ethnicity. It has nine total categories for race and ethnicity - losing most of the categories in ethnicity - and age is reported in odd aggregations that will make comparison difficult with other data reporting. Let's run a few quick tests on the download.

```{r demography test problems}
check_summary <- census_table_check(bgSAR_data_10, "SEX BY AGE","individuals") #should get a function that fails appropriately
cat(check_summary[1])
pop_totals_bg <- bgSAR_data_10[label=="Total"&concept=="SEX BY AGE",4:ncol(bgSAR_data_10)]
sum(pop_totals_tract[,])==sum(pop_totals_bg[,])
paste0("Number of Harris County block groups: ", ncol(bgSAR_data_10[,4:ncol(bgSAR_data_10)]))
paste0("Average population size for Harris County block groups: ", as.integer(mean(as.numeric(pop_totals_bg[,]))))
paste0("Median population size for Harris County block groups: ", as.integer(median(as.numeric(pop_totals_bg[,]))))
paste0("Maximum population size for Harris County block group: ", max(as.numeric(pop_totals_bg[,])))
paste0("Minimum population size for Harris County block group: ", min(as.numeric(pop_totals_bg[,])))
rm(pop_totals_bg)
rm(pop_totals_tract)
```

We note, very quickly, that the median tract is almost three times as large as the median block group, but there is a good deal of variability in size on both. 

#A Philosophical-Mathematical Digression - Speaking About 
##(the more practically-minded may simply skip this section - or maybe we should put it later, anyway)
We should give a quick nod to what it means to say one representation is "equal" to another representation, even at the most basic levels. Clearly the files at the tract level and the block group level are not simply equal to each other, since they have a different number of rows and refer to different geographies, but they are presented in such a way as to make it possible to compare arithmetically - we simply ask whether, when we set the concepts or boundaries to the same shape or conditions, do we get the same number of people counted in that category. The total number they project can be equal (or not), and that is perhaps the simplest way to think about two different representations still being "equivalent" - that is, when they have "equal" results of appropriate arithmetic operations performed on appropriate substructures.

But we need to think carefully about the several ways we talk about "appropriate," here. Even when the sums are equal to each other, we face important decisions about the structure of the representation that allows us to calculate the sums, and whether what is being counted has changed across differing structures of representation. Much of that structure, in this case, is literally a "space," because it's the difference between the geographic characteristics of block groups and tracts. The fact that both were created by large committees of government officials with a variety of opportunities for input from other stakeholders and the public, and that they were deliberately created to embed smoothly into each other for the purposes of aggregation, allows us to do the aggregations across the structures with confidence. That same structure also allows us to think more fully about other possibilities for representing the inner connections and the potential calculations that are enabled by those structures. We hope to eventually also show that these structures, even though they explicitly belong only to the choices about mathematical representation, will give us more substantial clues to otherwise hidden structures at the neighborhood level, as the accumulation of structures refines what can be said about the relations between the space and the individual, as well as what is implicit in the previous descriptions but still has to be made explicit through some determined process. 

We should also note that this touches on all sorts of political questions, with different ways in which public input is accepted into the decision-making process. For example, the Census Bureau took steps to make these boundaries coherent and complete (both the geographic boundaries and the delimitation of the conceptual categories), but that is justified by the need for a coherent and mathematically consistent representation. The necessary calculations on demographics (which are at the base of so much else in public health, after all) are often difficult or impossible in less well-structured spaces, for example, where cities and counties don't share all geographic boundaries but have large shared areas, as with the City of Houston and Harris County. Finding ways to report numbers, even the basics of population, have real world funding consequences, which become tied into the presuppositions of the models at multiple levels. However, those bigger questions remain at the horizon. For the moment, we want to take our building process slowly, for the purpose of eventually understanding what is at stake in these first and seemingly trivial decisions that in turn rely on other structures - after all, we have to be confident that there are overarching mathematical structure that are sustained across the representations in order for the larger interpretation to make sense, even when that interpretation is the almost immediate and seemingly trivial act of aggregation. 

One of our philosophical inspirations comes from the way that type theory differs from set theory. Both of them provide ways to encode other mathematics into a single way of talking and making proofs, with many outward similarities but important differences in how the process of proving mathematical propositions is expressed. In the encoding of mathematics based on set theory there are objects (called sets) which have a membership relation to other objects. In type theory, Per Martin-Löf's intuitionist version is especially clear in its motivation (cf. https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf), types are the way that elements can be described without positing the existence of a set or of objects that have the property of belonging to that set. For a common example of where it makes a difference, if you encounter a group of points in a set, you cannot make sense of them in a full and meaningful way just by knowing that they all have the attribute of being a point; the Banach-Tarski paradox, for a motivating example, showed that using the rules of set theory and a few suppositions about infinity and the axioms of mathematics, a sphere of any size can be divided and then recreated into a sphere of any other size. In our terms, set theory preserved the structure of membership in the set of points that are found on the surface of a sphere, but sacrificed the relation of those points to the size of the sphere, since the structure that sustained size wasn't part of the set, although an infinite number of points, by definition, was included. In type theory, on the other hand, the objects that are manipulated, including the numbers, must be accounted for - the space that the points are found within lets you describe them as close together or far apart, as countably infinite or as generated by the relation to a fixed center, and (more broadly) Martin-Löf's type theory allows a collection of typing declarations to construct or encode any mathematical object in ways that account for the structure one wants to preserve. 

The constructive process does not imply ontological relativism, but does rely on the idea that there are many ways of talking about things and that being conscious in those choices between ways of talking, and attentive to the motivations guiding those choices, can ground the practice of mathematics as a search for structure. In the history of the theory of mathematics, constructivism is usually opposed to one or another sort of Platonism, but for our purposes, we're striving to motivate references to mathematical theory or its philosophical background only through encountering and responding to issues with the concrete understanding of creating Sam City. It is hoped that this can be done in a way that could be useful for our students, or for any of the fields, like population health, that rely on a representation of demographic attributes for a population. Here, we are underlining the fact that certain methods become available to us because of the conscious decisions of the U.S. Census Bureau to create a database that will be useful for calculations - the fact that every person is assigned at least one race, and that the reporting includes all people in its categorizations, means that you can use the law of the excluded middle and say that if someone isn't one of the other 6 races, they must be the one that's left. That is constructed by the Bureau to be useful for the purposes of the aggregation. The addition of ethnicity in the 1980 census, after a much contested attempt to add "Mexican" as a racial category in 1930, was a politically motivated attempt to account for ways in which racism against people of Latino origin was impossible to track without having some way of counting how many people fit into the category - which is to say, it's a politically meaningful choice whether to include it or not; there is no simply "non-political" side to the act of category construction or the choice to either maintain or modify the categories around race and ethnicity. (cf. Rodriguez, Clara E. Changing Race: Latinos, the Census, and the History of Ethnicity in the United States. New York University Press, 2000. E184.S75 R64 in library). 

We said earlier that part of the initial impulse behind the construction of Sam City was to make a more "friendly" version of Houston for pedagogical purposes, and it's worth being explicit that there are deeply racist components to Houston's past that should not be erased in the name of that friendliness. Our hope, by contrast, was that a more honest accounting could be made possible by understanding how the construction of the basic demographic categories in the population of the simulated city could reflect the complexity at hand. Accordingly, just as the addition of ethnicity as a category was troubled by a triangulation of political issues around segregation throughout the U.S., Houston's White political leadership attempted to achieve court-ordered segregation in the schools by integrating Latino and Black schools with each other, while leaving the White schools untouched. This sleight of hand was achieved by counting the Latino students as White, but ignoring the long-standing inequities in the schools in Latino neighborhoods. There were considerable political and legal actions against Houston at the time [https://www.nytimes.com/1970/03/01/archives/houston-moves-to-desegregate-schools-amid-clamor-of-protest-angry.html] [https://www.nytimes.com/1970/09/06/archives/houston-huelga-schools-open-in-a-mexicanamerican-protest.html], but the schools remain deeply segregated throughout the United States, and especially the South [https://www.civilrightsproject.ucla.edu/research/k-12-education/integration-and-diversity/brown-at-60-great-progress-a-long-retreat-and-an-uncertain-future/Brown-at-60-051814.pdf], as well as in Houston's larger metro area. These histories all have continuing ramifications on current social, educational, and health outcomes [https://www.houstontx.gov/health/chs/documents/Health-Disparities-Data-Report-I-2019-Root-Causes.pdf]

To say that "race" is embedded in a biological determination and "ethnicity" in a cultural context, clearly, is to miss how the structuring questions are motivated and thus to obscure at least some potential pathways toward a more acceptable general approach to embedding relations between individuals and social structures than as sedimentation of layers of causes and effects. At a social level, we need a way to refer to the history of determinations that have affected individual people's life courses, as others create barriers based on those ways of talking about who does or does not belong in a group. To do so, however, risks reifying those determinations, or creating a situation where one way of talking -- for example, the idiosyncratic development of the idea of race and ethnicity in the U.S. -- is taken as the reference from which all other approaches deviate.

Here, the scientific temptation is to try for more complexity -- to determine variance in reference to an encompassing schema that provides more information and allows for more finely tuned subsetting, such as a genetic characterization which tracks ancestry. In such an analysis, one might speak of 68 distinct ethnicities in Mexico, with recently refined mathematical techniques allowing researchers to show distinct genetic characteristics, tuned to different geographic regions in Mexico [The genomic landscape of Mexican Indigenous populations brings insights into the peopling of the Americas](https://www.nature.com/articles/s41467-021-26188-w). These genetic characteristics are plausibly linked to a number of biomedical traits, with considerable scientific resources committed to finding pathways to providing genomic medicine or enabling precision medicine through incorporation of genetic analyses [The Genetics of Mexico Recapitulates Native American Substructure and Affects Biomedical Traits](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4156478/). Of course, if one is to embed these mechanisms within a more fundamental physical process of biochemistry, there are orders of magnitude more levels of complexity to account for in epigenetic terms than just the transcription of the traditional base pairs &mdash; or &#151; cf., for a recent but not final account, [Mapping the epigenetic modifications of DNA and RNA](https://europepmc.org/article/med/32440736). More fundamentally, it is clear that such a scientific account can inadvertantly end up masking the social and political dynamics, which have frequently been driven by erasing these sorts of fine-grained nuances and lumping people who are not seen as part of the accepted group(s) into an "other" category. Some sort of account that tracks what the distinctions are being used for, and in what context they make sense as being useful for answering a problem, needs to precede the questions about whether the distinctions fully exhaust the particular frame. That preceding question, for etymological reasons, we will call difficult (it is literally not easy and resists being calculated) instead of complicated (which implies a multitude of layers, which are resolved through unfolding and determining single values within a multi-dimensional space).

We would prefer to resist that temptation to speak about complexity, and seek to focus on the difficulty of the question: how do we think about the embedding so that it doesn't reify a sense of determination in isolation? And how do we bring that difficulty into focus as a pathway that addresses questions of value to social, economic, and health outcomes? Let's take a second to think about how to approach the problem of embedding, before moving back to our proposal to think about the construction of the data tables as "ways of talking."

Type theory gives us one well-articulated alternative way of thinking about the embedding, which is popular precisely because it still allows a unifying understanding of how to encode any proposition about the world in the straightforward and consistent language of types. Its modern articulation is made possible by the recognition (starting with Tarski) that truth is a property of a language that is "higher" than the language in which we describe the objects. This was meant to address foundational problems with set theory, but also has to do with how one tries to separate syntax (form) from semantics (content). Tarski says that claims made in a given language about given objects are embedded in an encompassing model that provides rules for what is true or false. Intuitionistic type theory points out that there is a process of construction that undergirds both, and thus makes it possible to understand embedding as a choice about what structures matter in the model - so that, in our example, we can ask explicitly how the relations of race to ethnicity are embodied as countable in the census, and also use that artificial construct to infer other aspects of the structure that aren't immediately given in the tables. In that sense, we are trying to understand the types as constructed categories, and then to understand what the limits of that construction show - both positively, in terms of how operations performed on the data within those bounds can create new understanding of internal relations, and negatively, in terms of understanding what can't be said in that language.

Very briefly, it's worth remembering that type theory began as an answer to Russell's paradox, although two steps removed, as Alonzo Church first developed untyped lambda calculus and then typed lambda calculus. The underlying task, stemming from Georg Cantor's foundational formulations of set theory, and also found in the continuum hypothesis, is that when you create sets, you need to be able to understand their internal structure in order to perform operations on the set. Set theory had been designed with the idea of capturing predication - one seeks to understand what it means to say that something has a predicate by saying that it belongs to the set of objects with that predicate. If one can generalize this process, then the use of numbers to ensure structure inside the sets (simply put, that I can assign a number as a predicate that "represents" each object and then, with suitable caveats, shows how the set of objects can be well-ordered) can itself be trusted. Cantor saw that you needed to be able to say that the power set of a set - that is the multiplication of all the combinations of categories - had to be larger than the elements of the set. It had to be possible to create more subsets than elements in the set. This may seem trivially true, but runs into difficult problems where the problem of representing infinite subsetting meets the problem of ordering (i.e., with Dedekind) and one has two competing ways to order what is infinite suggesting that the ways to subset the infinite do not converge to a single rule that would encapsulate the meaning of the infinite. Or more importantly, the idea is that if you are trying to understand how a type is constructed, you are asking how it is calculated, and if you calculate without attention to the construction, you fall into logical traps that prevent effective computability. (Martin-Löf, before the invention of Haskell in 1990: https://www.cs.tufts.edu/~nr/cs257/archive/per-martin-lof/constructive-math.pdf) . Martin-Löf, Constructive mathematics and computer programming, in: Logic, Methodology and Philosophy of Science, vol. VI, 1980, pp. 153–175. 
[[need to think about: W.A. Howard, The formulae-as-types notion of construction, in H.B. Curry: Essays on Combinatory Logic, Lambda-calculus and Formalism, 1980, pp. 479–490; there's an overview "Propositions as Types," at https://www.youtube.com/watch?v=IOiZatlZtGU by Philip Wadler that strikes me as very clear. He frames it in terms of the capacity to reduce logical propositions to a computable form and how the lambda calculus duplicates the formalism - he rather bizarrely asserts that it shows that math is discovered, and not invented, even though it's all about constructive math - including a digression on lambda as the universal programming language. It maybe be worth talking about Haskell as Curry's first name; that functional programming was looking for a way of being on a first-name basis with a dominant figure in the field, perhaps in a parallel with Sam, with some sense of the strengths and weaknesses...
Need also to think about how explicit Martin Löf is about the problem of hierarchies and how they get resolved: "156 - continuing from 155 - concisely: "'if a set is understood in Zermelo’s way as a member of the cumulative hierarchy, then a set cannot be the same kind of thing as a data type.'"; that we aren't dealing with membership, and the truth about membership, but about labeling, and the consistency of labeling - and doing it constructively]]

Our point, with this extended digression into assigning race and ethnicity, is that as we try to assign attributes to individuals in consistent ways, we must assume the legitimacy and applicability of the categories - an assumption we have strong reasons not to make! If set theory can be used, it's because it presents the possibility of infinite interior correction of categories - we can add complexity to the description, produce more factors for analysis, and refine the idea of membership - all within our overarching confidence that there are more ways to name things than there are things to be named. The problem is that this assumes that the whole problem is about coming up with the right names - the right way to say things. The idea that type theory introduced was that the higher orders of embedding could be typed in such a way as to ensure order (cf. Church's typed lambda calculus); they also introduced the idea that the order of calculation could be from the outside in, whereas set theory had created the situation where things had to be solved from the inside - which precluded some of the most useful steps in construction.

#MAYBE THE IDEA OF ORDER OF CALCULATION IS A SUBTLE WAY TO INTRODUCE THE NEED TO CONSTRUCT; THAT ONE CONSTRUCTS BOTH THE DOMAIN AND THE CODOMAIN IS KEY IN LONG RUN, AND DIFFERENT FROM TYPE THEORY, WHICH WANTS THEM TO BE THE SAME; CATEGORY THEORY ASKING HOW YOU MOVE FROM ONE WAY OF TALKING TO ANOTHER!!!
##not just that you have lazy evaluation; it's the idea that evaluation happens along the line of the construction and not along the line of a presupposed what it must have been like to have been built from first order primitives / homotopy degree zero
##for longest arc: what does it mean to stretch toward saying something? To be engaged in a question?

There's also the question of formalism as inappropriately relying on the infinite and the law of the excluded middle as purely formal.

It may be that the algebraic approach toward embedding in category theory gives us yet more powerful tools, especially in the idea that we are looking for the ways that descriptions can be transformed into or mapped onto equivalent descriptions embedded in different models as the way of specifying the meaning of an object: in other words, the proposition is not true or false depending on the embedding within a metalanguage, but rather the algebraic possibilities that govern the totality of possible ways of talking about the same thing will tell you what a mapping that conveys a true proposition would mean. That is, in Lawvere's articulation of the idea of a mapping from hyperdoctrines, as in "Diagonal Arguments and Cartesian Closed Categories," from 1969, (Lawvere, F. William. “Diagonal Arguments and Cartesian Closed Categories.” Category Theory, Homology Theory and Their Applications II, Springer Berlin Heidelberg, 2006, pp. 134–45, https://doi.org/10.1007/BFb0080769.), the algebraic possibilities that govern the composition of the steps that create a representation tell one where the limits of a way of talking about things lies. Russell's paradox and Gödel's incompleteness theorem both become instances of an attempted transformation of talking about sets, where the attempt to describe the set of all sets becomes self-referential and paradoxical, into the problem of how to transform sets through mappings where the presupposed notion of truth is a mapping of points to points. 

The limitation of that approach can be seen by turning to algebraic topology and to ideas of mapping that don't reduce to point sets - or to binary representational functions. Here we hope to capture something of the humanities criticism of traditional set theory, as with all the descendants of Heidegger who decried the technological approach to language as mere predication, and say that there is also a mathematical approach that seeks to situate set theory within a broader understanding of the possibilities of speaking about. Broadly, it is the idea that the ways of talking about something are structured through the process of describing and that the attention to that structure is the core of the scientific process. Of course, it's not the only way to talk about things, and we'll try to point to the motivating questions as opposed to the dogmatic elements of any of the tools that have inspired this inquiry. How we can practically encode a claim about mathematical relation, however, is the guiding reason that Sam City was developed, as we pushed to understand what it would mean to create a reusable database of demographic, social, and economic information about residents and neighborhoods in our area that somehow also captured dynamic change and the sense of individuals that they somehow exceeded the objectifying categories assigned to them by the system. How do we make that sense of organic development and internal self intelligible to others? How do we compute the results of aggregation over large groups of individuals, each pursuing their own paths? How do we represent the effects and interactions? Those are practical and driving questions for ourselves, our students, and our community partners.

What then counts as the guiding principle for scientific research on the sort of dynamic system that constitutes a city and its inhabitants? Instead of looking for a predicate that describes the object and asking whether it is true or not - as when you ask if "Hispanic" is the right term for a Catalan on a temporary work visa - you look for the structure that enables true or false predication to be about the individuals who are being talked about. At that point, one can identify the limits of the framing questions and the limitations of the way that the domain of possible answers was constructed, and ask effectively what changes can be made in the process of talking about that would better match the motivating questions.

For our case, we propose to return to the example of making Sam City, and of asking explicitly about the structure and what emerges. In the somewhat simplified case before us, we are tempted to ask how to join ethnicity with race at the block level.


#Embedding Calculations in Structured Spaces. (that any and all "space" is "structured" will come out later)

In our current project, we are trying to identify and leverage the structures in place in the data tables. For the next step, we need to transform the counts we have for demography so that each person is represented as having properties, including geography and demography. From a purely technical side, this is just transposing the columns and the rows, so that each row is a census tract, and the columns correspond to the attributes that had been in the columns. Then, we will expand the number of individuals with those characteristics by just adding a row for each individual represented in the count. One can think about this equivalently as having created individuals with predicates, following the logic of the tables, or - as we propose as an alternative - we can take the predicates as structures of a shared space, where individuals expand into that structure according to rules that govern the individuals' potential pathways. Those "rules" are given either directly or indirectly from other tables, with other predicates, but in some way still referring to the same individuals within the same census tract. How to combine those predicates is the task that ran us aground the first time; how to construct a space of possible predication in terms of the individuals inhabiting that space is the question we are now pursuing. It may be helpful, though, to recount the shipwreck from the initial effort, and to place it into its explicit and practical context.

Let's take a closer look at the tract-level data, as the most specific for year and race category, and then add the block_group information because that gives us important geographic differences. We'll work with only Harris County at this point, since we assume that the structure is repeated, by explicit design of the Census Bureau, across the entire population.

First, we need to identify the rows which, if added together, give us a complete and non-repeating set of the population. There is one way to do that with the overall total row, with the two gender total rows, and with all of the individual age rows without the totals. Unfortunately, the census reports have a somewhat idiosyncratic way of reporting the data, and the way to identify these rows is not always simple. This case is relatively easy, and a good example to start with. The "label" column has three components, separated by double exclamation points. (They made changes in the way the separations work in 2019, so these small scripts will have to be modified for later years. The decennial and ACS also have small differences that have to be accounted for, as we will see. A lot of the difficult work is the tedious effort involved in cleaning up the data for processing, but cleaning and structuring are similar tasks, and they provide a good example of what it means to be clear about the structure that sustains the potential analyses).

We can use a little trick because of how the R library dyplyr performs the translation and expansion of the representation, so that we're looking at rows of people with geographic and demographic properties and not rows of categories against columns of geography. We will get a warning that NAs were introduced by coercion, but it happens that those will be the rows that we want to exclude in any case. We'll show the warnings, here, but suppress them later.

```{r expand SARE for the tracts in PC12}
trSARE_10 <- trSAR_data_from_census_10 %>%
  pivot_longer(4:ncol(trSAR_data_from_census_10),names_to = "tract", values_to = "number_sams",
               names_transform = list(tract=as.character)) %>%
  separate(label, c("total","sex","age"), sep = "!!", remove = F, convert = FALSE) %>%
  mutate(re_code = substr(name,7,7),
         race = str_replace(concept,"SEX BY AGE \\(",""),
         race = str_replace(race,"\\)",""),
         age = str_replace(age, "Under 1 year", "0"),
         age = str_replace(age,"year"," year"),
         age = as.integer(substr(age,1,3))
         ) %>%
  filter(number_sams > 0, !is.na(age))
paste0("There are ",length(unique(trSARE_10$age)), " unique categories for age (each year up to 100 and then two groups higher).")
paste0("There are ",length(unique(trSARE_10$re_code)), " unique categories for race and ethnicity.")
paste0("Along with two categories for sex, we have ",length(unique(trSARE_10$age))*length(unique(trSARE_10$re_code))*2," different categories people can fall into.")
rm(trSAR_data_from_census_10)
```

Once we have the file for the entire population, we'll want to make one complete population with ethnicity and one complete population with race. It is worthwhile to take a second to think about what it means for the two representations of the entire population to be "equal" to each other. That they have the same number of people in each tract is part of the justification for saying they are equal but might be better phrased as saying that both tables are about the "same" people. There is also the way of saying they are equal because the schemas for classification have the same number of individuals in them, regardless of what you call the person in the particular category (i.e., "Hispanic" or "Latino" are treated as the same thing). And then, yet further, there is the sense in which the distinctions, and the combinations of the distinctions (i.e., what is called the power set produced by multiplying categories by each other) are equivalent if they divide the individuals considered as objects in the schema in the same way. That is, if you slice up the sets in different ways, but still come up with the same number of people in each cell, there's a certain type of "equality" there. To speak of "equality" in "ways of talking" is an attempt to make clear how mathematical objects, their structures, and the ways that their structures are conserved (or not) when moving between these "ways of talking" are all at stake even in fairly mundane classification tasks.

The fact that both the ethnicity tables and the race tables are about the same individuals, as presented in complete form through aggregations of individuals who have shared predicates, tells us that the "ways of talking about" those individuals are grounded in that sense of the individual. Some census tables don't share that same sense of individuality - some talk about households, for example, or about the housing stock - and yet there are still ways of delimiting the "sameness" of the tables - for instance, if they each talk about and characterize in some sense individual census tracts, which can be mapped back to individuals or other sorts of objects that can be counted, and can be considered the ground of saying that we're still talking about the same thing. This fact opens the door to talking about the relations that are mapped, with the individuals as endpoints of the mapping; to speak concretely about the relations first, and allowing the individuals to emerge into systems of possible relations, is the task we set ourselves with the construction of Sam City.

This table, we admit, caught us by surprise. All the other tables by race and ethnicity had only two ethnicities reported - one for "Hispanic or Latino," and another for "White Alone, not Hispanic or Latino." This led us to considerable difficulties in sorting out who might be, for example, answering "Some Other Race" and "Hispanic or Latino," or "Black or African American" and "Hispanic or Latino." These are admittedly smaller segments of the overall population, but important sectors to capture in the representation, if we are going to claim to understand details about the ways that race and ethnicity effect outcomes within the context of neighborhood dynamics.

Here, we're doing the expansion of race first, because in all the other tables, the race categorization is more "complete" than the representation by ethnicity. Which is to say that some people in the population are not represented in terms of their ethnicity, while everyone has their race reported. The sense in which the representation by race and by ethnicity are equal at the tract level, but not at the block_group level, gives us a chance to think about what it means for the representations to be embedded in conceptual spaces on rigorous mathematical terms, and to begin to see the ways that our representations are limited by those conceptual spaces as well as pointing to techniques that help us expand those representations. The equality of the representations come from the way in which they are structured by their endpoints as represented in a certain total number of individuals; it forces us not to think about the equality of two numbers in arithmetic, but the structure of embedding claims in the same underlying space. Taking another clue from recent turns in mathematics, we can say that it is, in certain conditions, equivalent to speak of the same individuals in different ways, to the extent that they both serve to describe the same individuals. The difference in describing those individuals by ethnicity instead of race emerges in the ways that the context of that designation matters, and not as a function of individuals essentially being in one set or another.

```{r expand tract Harris SAR}
    #setup race codes https://www.census.gov/programs-surveys/acs/guidance/which-data-tool/table-ids-explained.html
    race_codes <- c("A","B","C","D","E","F","G")
    trSAR_10 <- trSARE_10 %>%
      filter(re_code %in% race_codes) %>%
      uncount(number_sams,.id = "sams_race_id") #will create different id later
    paste0("Total population in this representation is: ",nrow(trSAR_10))
```

Interestingly, because the total wasn't right the first time we processed this example, we were able to look for bugs in the code - in this case, not having noticed that there was an age called "Under 1 year" that needed to be made into a 0 to fit the pattern of the other ages (it's now fixed, above). There's a very practical side that drove us to triangulating - i.e., to checking things from multiple sides to see if they converged on the same answer - as well as the emerging understanding that moving between possible representations was itself a way of understanding how the objects of our inquiry were emerging as such.

We fixed that and then did one complete population for ethnicity: SET UP TABLE HERE FOR CODES, JUST LIKE DID FOR RACE; AND THEN SHOW THE AVERAGES ASSIGNED TO THE RACE, AND HOW THAT COMPARES TO THE GROUND TRUTH AT THE CENSUS LEVEL

```{r expand tract Harris SAE}
#one for Hispanic and then not Hispanic or Latino for each race; H and I do not match race codes in ACS
ethnicity_codes <- c("H","I","J","K","L","M","N","O") 
trSAE_10 <- trSARE_10 %>%
  filter(re_code %in% ethnicity_codes) %>%
  mutate(race = str_remove_all(race,", NOT HISPANIC OR LATINO")) %>%
  uncount(number_sams,.id = "sams_ethnicity_id") #will create different id later
paste0("Total population in this representation is: ",nrow(trSAE_10))
#which is conveniently the same as the totals above
nrow(trSAE_10)
#let's also clean up
trSAR_10 <- as.data.table(trSAR_10)
trSAE_10 <- as.data.table(trSAE_10)
rm(trSARE_10)
```

This table reports on 8 categories of ethnicity. The necessity to aggregate when reporting certainly drives much of the problem, but we do want to begin to disentangle some of the issues. Here are the further characteristics they offer:

```{r table for ethnicity types}
dt <- as.data.table(list(unique(trSAE_10$re_code),unique(trSAE_10$concept)))
setnames(dt, c("V1","V2"),c("re_code","concept"))
dt[order(re_code)]
table(trSAE_10$concept)
rm(dt)
```

This is, perhaps, still not as straightforward as one would like, because it gives us one part of a disjunctive syllogism and there are still potentially multiple categories on the other side of the disjunction (Hispanic with three races, for example). There is a table reported as part of the redistricting data (dec/pl in the API) that has separate categories for these combinations and for a large number of combinations for multi-racial individuals, but it does not give age at the same time. The ACS has tables for specific ethnicities and for multiple places of origin for ancestry, but they are not given in the decennial tables. For the moment, we want to stick with the initial tables, and talk about how to construct our approach. In Harris County, 33% of the population were categorized as "Hispanic or Latino" in 2010, and 44% in 2020. Some individual census tracts are over 95% of one race or ethnicity [[should do the calculation; it would be interesting to know]]. If we look at only race, we could get a very misleading sense of what the dynamics of a neighborhood were at the cultural level. So we wanted to somehow take the race tables, which had the virtue of a complete representation of all individuals, and add them to the ethnicity tables, which for the majority of tables are reported as only "White Alone, not Hispanic or Latino" and "Hispanic or Latino." 

Talk a little bit about how it doesn't work to just give every White person a 40% chance of also being Hispanic or Latino....
At some point, will also want to talk about using data.table vs. dyplyr / tidyr and what, in general one is looking for in rows, columns, and vectors, etc.

What we do, instead, is to order each of the sets by their shared characteristics and assign an id, with numbers assigned sequentially for rows that match on all characteristics. We create the id by ordering the collections of properties in the same way for each set, and then assigning an id. The final sampled number in the id uses all the possible numbers between 1 and the number in that cell (the base r sample function defaults to "without replace"), so that all individuals are given a unique id but in a way that takes advantage of the many ways that the two sets can be ordered internally to find equivalent representations, which in turn allow for assigning properties that were not originally assigned in the given representations. The attentive reader will note that there is no information lost here at the tract level, but that the ways in which race and ethnicity are joined at the block level could be lost.
We're doing a join, and some times the join could be done with one of the built-in merge functions in R or one of the R libraries. Here, because we want to emphasize the questions about the ordering and relations in terms of those orders, we make explicit the process of assigning numbers to each individual within a subset and then matching on those numbers to add variables across tables. As we move through the process, we'll find more precise control over the numbering within those final subtypes will matter.

```{r join trSAE and trSAR}
trSAR_10[,("sar_match_id"):=
                    paste0(tract,sex,age,race,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age,race)]
trSAE_10[,("sar_match_id"):=
                    paste0(tract,sex,age,race,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age,race)]
trSAR_10[,("ethnicity"):=
                    trSAE_10[.SD, list(re_code), on = .(sar_match_id)]]
#By the disjunctive syllogism, the ones that don't match are the ones that are Hispanic
trSAR_10[is.na(ethnicity),("ethnicity"):="H"]
length(table(trSAR_10[re_code=="A",ethnicity]))==2
length(table(trSAR_10[re_code=="B",ethnicity]))==2
length(table(trSAR_10[,ethnicity]))==8
```

Just as we did by summing up the different ways that the total populations could be represented and then comparing the outputs to determine if the two representations are equal, we can use the table function from the base R library to get the number of people in each tract in the smallest cell created by combining factors (the power set). This will tell us if we successfully represented every person from the representation in terms of ethnicity in terms of race, with the same embedding in terms of tract, sex, and age. (Could put a little bit more on why that is better captured by types designating enclosing spaces than by set membership - although, importantly, both are possible, and equivalent for practical purposes within the current problem; in long run, the question is whether more complexity solves the problem, or if the construction can point to the proper levels of complexity)

```{r trSAE trSAR join tests}
test <- table(trSAR_10[,tract],
              trSAR_10[,sex],
              trSAR_10[,age],
              trSAR_10[,ethnicity]
)==table(
  trSAE_10[,tract],
  trSAE_10[,sex],
  trSAE_10[,age],
  trSAE_10[,re_code]
)
length(test[test==F])==0
#and test that 
test <- table(trSAR_10[ethnicity!="H",race])==
  table(trSAE_10[race!="HISPANIC OR LATINO",race])
length(test[test==F])==0
```

For sake of completeness, we want to take the people who are over 100 and assign them single years of age, as well. We're going to be using SAR_10 as our base for combinations, so we only do it on that one side. We're using sample with replace, so there will be a different size for the age by year every time the sampling is done. Since we want it to be reproducible, even at this level, we set the seed for the random number generator, so the random variation will be reproduced. There are only a handful of centenarians in the Harris County 2010 census and no reason to try to chase down more specifics in their true age distribution.

```{r centenarian age}
set.seed(Sam_seed)
trSAR_10[age==100,("age"):=sample(c(100,101,102,103,104),size = .N,replace = TRUE,prob = c(.29,.24,.19,.16,.12))]
trSAR_10[age==105,("age"):=sample(c(105:109),size = .N,replace = TRUE,prob = c(.29,.24,.19,.16,.12))]
trSAR_10[age==110,("age"):=sample(c(110:113),size = .N,replace = TRUE,prob = c(.6,.27,.1,.03))]
table(trSAR_10[age>100,age])
```

We know that it's fully commutative with the SAE file at the tract level, and that any specification to the block group would maintain that tract level commutativity, simply because the blocks are fully contained within the tract.

Let's set this representation of the population aside for a moment and look at the block group level. We'll follow a very similar pattern, although there are small differences that make it impossible to run it simply through the same script. 


```{r expand bgSARE, warning = FALSE}
bgSARE_10 <- bgSAR_data_10 %>%
  pivot_longer(4:ncol(bgSAR_data_10),names_to = "geoid", values_to = "number_sams",
                   names_transform = list(geoid=as.character)) %>%
      separate(label, c("total","sex","age_range"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(re_code = substr(name,5,5),
             race = str_replace(concept,"SEX BY AGE \\(",""),
             race = str_replace(race,"\\)",""),
             age_range = str_replace(age_range,"Under 5 years","0  to  4 years"), 
             age_range = str_replace(age_range,"5 to 9 years","05 to  9 years"),
             age_range = str_replace(age_range,"18 and 19 years","18 to 19 years"),
             age_range = str_replace(age_range,"20 years","20 to 20 years"),
             age_range = str_replace(age_range,"21 years","21 to 21 years"),
             age_range = str_replace(age_range,"85 years and over","85 to 110 years"),
             age_range = str_replace(age_range,"and","to"),
             first_age = as.integer(substr(age_range,1,2)),
             last_age = as.integer(substr(age_range,7,8)),
             tract = str_remove_all(geoid,"_"),
             tract = substr(tract,1,11)
             ) %>%
      filter(number_sams > 0, !is.na(age_range))
rm(bgSAR_data_10)
```

Now we filter by race and ethnicity codes.

```{r SAR expand to bg}
race_codes <- c("A","B","C","D","E","F","G")
bgSAR_10 <- bgSARE_10 %>%
  filter(re_code %in% race_codes) %>%
  uncount(number_sams,.id = "sams_race_id")
bgSAR_10 <- as.data.table(bgSAR_10)
paste0("Total population in this representation is: ",nrow(bgSAR_10))
#which is also the same as the totals above
nrow(bgSAR_10)==nrow(trSAR_10)
```

Let's put in ethnicity codes for bgSAE and expand, so that the tables from tract and block group match as far as possible, even if we don't know all the demographic specifics (age by year and race of non-White Hispanics) that had been available at the tract level. Notice that the population totals do not match; the ethnicity description is not complete for the population as it had been for race at the block group and for both race and ethnicity at the tract levels. We will have to account for that structuring choice in how the tables are reported in order to construct our own complete representation for the simulation.

```{r SAE expand to bg}
eth_codes <- c("H","I")
bgSAE_10 <- bgSARE_10 %>%
  filter(re_code %in% eth_codes) %>%
  uncount(number_sams,.id = "sams_race_id")
bgSAE_10 <- as.data.table(bgSAE_10)
paste0("Total population in this representation is: ",nrow(bgSAE_10))
#see if age_ranges line up, because sometimes they do not between ethnicity and race reporting by the Census Bureau
test <- sort(unique(bgSAE_10$age_range))==sort(unique(bgSAR_10$age_range))
length(test[test==FALSE])==0
rm(bgSARE_10)
```

Part of what's interesting, here, is that the tract level wanted to keep straight all the potential Hispanic or Latino combinations - for example, Black Hispanic is not listed in the block_group, but is available in the tract (albeit by disjunction). At the tract level, you have both "BLACK OR AFRICAN AMERICAN ALONE" and "BLACK OR AFRICAN AMERICAN ALONE, NOT HISPANIC OR LATINO"), which is what lets you impute Black and Hispanic from the remainder. Tract level reporting did not include that extra level. The race_ethnicity codes (re_code) for ethnicity is only either H or I at the block group level.

```{r table eth_codes}
dt <- as.data.table(list(unique(bgSAE_10$re_code),unique(bgSAE_10$race)))
setnames(dt, c("V1","V2"),c("re_code","concept"))
dt[order(re_code)]
rm(dt)
```

This means that the only information we know at the block group level is the number of people who are "Hispanic or Latino" and the number who are "White Alone, Not Hispanic or Latino." By the disjunctive syllogism, we can know the number of "Not White Alone "Hispanic or Latino," which will include all the subtypes available at the tract level in table PCT12.  Because we do know their sex and age_group, we can add them to the block_group by race representation on White Alone (which, in the race representation, includes many, but not all, of the Hispanic or Latino population). Since no other information is known about the individuals, we are constructing a representation that will not lose the structures of any of the originating embeddings, as reported. As we add more contexts to construct a more complicated description of the population, that task will become more difficult (and more interesting). This example is one of the simpler versions, and is not controversial. 

#here or later??
We note, by the way, that one could encode the descriptions in terms of sets, as well. Only as the construction gets yet more complicated will the distinction between types and sets really matter. [want to get, eventually, at what it means to work through syntax vs. semantics, with a brief nod to Tarski (https://en.wikipedia.org/wiki/Semantic_theory_of_truth) and Lawvere (who took classes with Tarski for a year in Berkeley and is responding to that same question of semantic vs. syntactic in 1963) - the subtlety is that the initial idea of structural embedding I'm trying to pull out is what Tarski would have assigned to the formal idea of semantics, and that Lawvere (following Grothendieck) would have given a better mathematical grounding (although we still have to talk about what gets past the "formal" idea of semantics in Tarski's sense); this could also be the place to talk about Martin Löf, and why ZF set theory (with or without C for the axiom of choice, he is clear!), doesn't work because of membership and stratification. That the strata can be cashed out - i.e., the axiom of reducibility - doesn't need to be brought in if you're doing constructive (or intuitive) type theory] That Lawvere doesn't mention anything about "membership" in his definition of the category of set, for example in the co-authored introductory textbook, is no doubt a remnant of his time thinking about Tarski's project, and about what it means to understand mathematical structure in terms of the semantics of possible transformations in "ways of talking" as opposed to building the definition internally from axioms [Awodey or McClarty or Marquis (from a Geometrical Point of View)]

#here or later??
We should do things like talk about symmetric monoidal categories, and perhaps more broadly about Abelian groups, Lebesgue measure theory and sigma algebras: https://en.wikipedia.org/wiki/%CE%A3-algebra - just not sure where

#here or later??
why the faith in complexity, and in adding more dimensions as one slowly builds more complicated representations, comes from an idea of projecting onto a surface of individuation - not that one is against the idea of number, but against the process of taking the objects as individuated as the final arbiter of what it means for something to be! Earlier work has been used to explore the idea that Difficulty is a better way of characterizing embodied contact with the world - as opposed to an original simplicity that is complicated by adding more points of contact - and the general idea for the students in our courses has been that some of the most important questions we face shouldn't be thought of as "complicated" - they should be thought of as "difficult," or as requiring sustained attention to the forms in which things are presented, and the ways in which individuation emerges from the transformative relations that comprise society. 

As an aside that we will take up more at length, later, the race is allocated by the census taker (or sometimes an automated process) when the surveyed person refuses to give a race (or if the answer given doesn't agree with the algorithms). For the 2010 census, that activity was reported under the table, "Allocation of Race" (P46). There's not much we can do to dive into the detail about why it happened, except to note that a fairly large percentage had to be allocated, and that it was quite variable by tract. Deciding whether there is a source of error also depends on whether the ground truth would be the individual's self-designation, assuming they trusted the census-taker enough to tell them the truth, or whether there is an objective (or objectively consistent) designation that should be followed for the classification. For example, a person who prefers to be called "Latinx" would be grouped with "Hispanic or Latino" in this approach, and the political differences that were already inherent in "Hispanic" vs. "Latino" were collapsed into an awkward category that just had both names, just as "Black or African American" collapses many important distinctions about how people self-identify.[https://news.gallup.com/poll/353000/no-preferred-racial-term-among-black-hispanic-adults.aspx] The technical details for the Census Bureau's own framework can be found here: https://www.socialexplorer.com/data/C2010/metadata/?ds=SF1&table=P0460, but we will also have a more detailed conversation, below. The important thing to note is that the Census Bureau tried to conserve structures of consistency and completeness by carefully articulating rules for the data collection and ensuring that everyone fit into some category. This concern for consistency and completeness is completely justified, given the goals of the census, but we should be attentive to the other structures of representation that are not conserved but could have been, including people's right to self-identification, genetic profiles, either patrilineal or matrilineal descent, adoptive or biological descent, or self-identification with particular cultural tropes or characters from popular culture. Then, at the limit, what things are not amenable to this sort of "naming" should also be articulated explicitly, so that we can construct better ways of talking about the world than just more and more complicated names attached to individuals. 

For instance, in the tract tables, we see that there is a category for "SOME OTHER RACE ALONE" (re_code=="F") that has 583,566 people in it. Of those people, fewer than 8,000 were listed as "NOT HISPANIC OR LATINO" - just over 1%. Similarly, "TWO OR MORE RACES" (with 131,332 total and re_code=="G") has more than 60% (82494) listed as "HISPANIC OR LATINO."  Do we really imagine that the designations make the same sense to the people who are talking about themselves as it does to the Census Bureau? This table shows the cross-tabs for race and ethnicity, where not "H" indicates that they are the race listed, but "NOT HISPANIC OR LATINO."

```{r table for race and ethnicity}
table(trSAR_10$re_code,trSAR_10$ethnicity)
```

The final articulation we are struggling toward is understanding the goal and purpose of the distribution of names, so that we can assess whether the names assigned answer the questions that originally framed the collection and analyses of the data - or whether some particular extension retains the sense of being "about" the same data in enough ways to justify that secondary use. For the Census Bureau, there is some attempt to retain the sense of capturing how people want to describe themselves, but a greater technological imperative to ensure that the names are complete (i.e., no one is simply not assigned one of the race categories used for their analysis). 

```{r race allocated 2010}
race_allocation_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                  groupname = "P46",county_num = county,
                                                                  block="block_group",api_type="dec/sf1",path_suff="est.csv")
race_allocated_data <- race_allocation_data_from_census_10 %>%
  pivot_longer(4:ncol(race_allocation_data_from_census_10),names_to = "geoid", values_to = "totals")
race_allocated_data <- as.data.table(race_allocated_data)
race_allocated_total <- race_allocated_data[label=="Total"]
race_allocated <- race_allocated_data[label=="Total!!Allocated"]
race_allocated[,("total"):=race_allocated_total[.SD,list(as.numeric(totals)),on=.(geoid)]]
race_allocated[,("percent"):=round(as.numeric(totals)/total*100,2)]
summary(race_allocated[,percent])
paste0("number of block groups with higher than 10% attributed race is ",
nrow(race_allocated[percent>10])," and the number with 1% or less is ",
nrow(race_allocated[percent<=1]),
" of ",nrow(race_allocated))
```

There is one block group that is an outlier at 72%, with the next highest at 26%, and only 86 with higher than 10%. It would be interesting to know how those block_groups correlate with other factors about the neighborhood demographics. 

```{r join allocated to other demographics, eval = FALSE}
bgSAR_10[,("percent_allocated_race"):=race_allocated[.SD,list(percent),on=.(geoid)]]
bgSAR_10[, ("sum") := .N, by = geoid]
bgSAR_10[, ("proportion") := .N, by = c("race","geoid")]
bgSAR_10[, ("proportion") := proportion/sum]
```

```{r draw plots of percent_allocate and proportion_race, eval = FALSE}
#setDT(bgSAR_10)[, sum := NULL]
ggplot(bgSAR_10, aes(proportion,percent_allocated_race,color=re_code,group=re_code)) + #[re_code%in%c("A","B")]
  geom_smooth() +
  labs(title = "Block groups by proportion of race and number who had race allocated",color = "Race (by re_code)")
dt <- as.data.table(list(unique(bgSAR_10$re_code),unique(bgSAR_10$race)))
setnames(dt, c("V1","V2"),c("re_code","race"))
dt[order(re_code)]
ggplot(bgSAR_10, aes(proportion,sum,color=re_code,group=re_code)) +
  geom_smooth()
rm(list = ls(pattern="race_allocat"))
```

##could break out the proportion by population, too. Not sure the graphs help, at all. 

The second chart shows the proportion of a given race inside the block group, and although there may be reasons to think they correlate - perhaps because of patterns in urbanization that have also effected the choice of boundaries for block groups - it is generally meant to show what the general curves for distributions look like. The expected bell curves are not exact, but you can't claim that the first chart shows that as a block group has a higher percentage of blacks, it increases the percentage who have allocated race, until it gets to about 30%, when it starts going down again. After all, the percent allocated by race isn't reported by race, and there will be some tendency for the block groups that are grouped around the mean to have some sort of bell curve shape. Perhaps there's something to look at in depth about F - "some other race alone" - which is saying that as that percentage grows in the block group reports, the allocation by race grows. That number, after all, could easily be an artifact of how the "some other race" is collected, for example around people who consider themselves Arab, as opposed to people who say "put me down as 'some other race'". 

The graphs, frankly, bring up more questions than they answer, but they are only meant to show their limitations in this case. In the language that we are using to describe our overall process, the graphs have set rigid naming structures in place as the outer frame. Then we have both mathematical (i.e. percentages) and visual (i.e., lines drawn) to help us think about how that conceptual frame captures variation as it is applied to smaller subsets. A large portion of the work of statistics, judging from typical publications in the admittedly subjective position of an outsider to the field, tends to be about using those techniques to make a judgment about whether the population fits the descriptive frame. If something about the population doesn't fit, then that tells us about the population. Only occasionally will one see in the publications that the fit is so obviously bad that a different conceptual frame altogether had to be used. That could be a version of publication bias - one only submits the final story, after all the frames that don't work have been tried and discarded, or one is already working inside a subfield where the decision about the conceptual frame has been well adjudicated in the founding publications for that subfield. For educational purposes, we were of the opinion that we should make these moments of early exploration and decisions about the framing be made explicit - and techniques associated with resolving the issues be explicitly taught. We felt compelled, over the course of almost a decade, to expand the scope and refine the methods, however, as the general paradigm of "testing within" the given frame obscured fundamental questions about how the details of the framing were being constructed. For example, teasing out the contribution of race to the percent of the population for whom race was allocated by the professional staff at the Census Bureau, requires constructing the conceptual space of the question with considerable care to variations - like the size of the block group and the reasons for the need to allocate - that don't rise to the level of questioning the overall frame. How, we asked ourselves, do we understand the tasks associated with constructing the conceptual frame, and where did issues like granularity and question design overlap with the more obviously dubious (but highly important) categories like race and ethnicity? 

We are consciously putting these questions to the side, with the expectation that a careful analysis of how we are constructing the mathematical objects will point to an eventual better way to construct the measures associated with demographics, and that the internal analysis we associate with classical forms of statistical analysis will find their appropriate place in the overall process. (For more philosophically-minded audiences, Van Fraassen, Bas C. The Scientific Image. Clarendon Press, 1980., provides a coherent picture of how the scientific process could mirror broadly "constructive empiricism"; there remains the question of whether that is the best path for science, but it places us within a tradition, for this first step through constructing our explicit model.) We point, by way of adumbration, to the interesting proposition from Tai-Danae Bradley [https://arxiv.org/abs/2004.05631] that the algebraic construction of probability (resulting in the reduced density operator) carries more information than the classical statistical construction, where she helpfully uses quite mundane characteristics (marginal probabilities on the colors of fruit) to show the point behind drawing in the arcane-sounding mathematics. For our purposes, she shows convincingly that a construction that conserves the algebraic commutativity retains more information than one that conserves the statistical representations in isolation.

For now, we want to focus on the prosaic process of construction. To read the data.table script, below, you'd say that for the rows in bgSAR_10 that match the White only group (re_code=="A"), assign a match_id with the components of tract, sex, age, and an appropriate random number chosen between 1 and the total number of individuals in that subgroup (the default is replace=FALSE, which is to say that all those numbers are assigned). Then you do the same for bgSAE_10. Then the folks left in each tract in the category of Hispanic or Latino should be more than the number of White Alone that are left (presumably that portion of the Hispanic population was listed as some other racial category).

With these caveats in mind, let's look at how to best move the detailed tract data onto the block_group, with some discussion of other approaches and inherent dangers to the representations. We begin with the joins on ethnicity and race that are available at the block_group level:

```{r join bgSAE and bgSAR}
bgSAR_10[re_code=="A",("sar_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[re_code=="I",("sar_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAR_10[re_code=="A",("ethnicity"):=
                    bgSAE_10[.SD, list(re_code), on = .(sar_match_id)]]
bgSAE_10[re_code=="I",("r_code"):=
                    bgSAR_10[.SD, list(re_code), on = .(sar_match_id)]]

nrow(bgSAR_10[!is.na(ethnicity)&re_code=="A"])==nrow(bgSAE_10[re_code=="I"])

test <- table(bgSAR_10[re_code=="A"&!is.na(ethnicity),geoid],
              bgSAR_10[re_code=="A"&!is.na(ethnicity),sex],
              bgSAR_10[re_code=="A"&!is.na(ethnicity),age_range]
) == table(
  bgSAE_10[re_code=="I",geoid],
  bgSAE_10[re_code=="I",sex],
  bgSAE_10[re_code=="I",age_range]
)
length(test[test==FALSE])==0
#should mean that all the individuals who identify as both White and Hispanic or Latino are accounted for
#We can put the Hispanic label on them, and then make sure that we put the marker back on SAE, so that age and sex for the others stays same
bgSAR_10[is.na(ethnicity)&re_code=="A",("ethnicity"):="H"]
nrow(bgSAR_10[re_code=="A"&ethnicity=="H"])
nrow(bgSAE_10[re_code=="H"&is.na(r_code)])==nrow(bgSAE_10[re_code=="H"])
bgSAR_10[re_code=="A"&ethnicity=="H",("sar2_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[re_code=="H",("sar2_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[re_code=="H",("r_code"):=
                    bgSAR_10[.SD, list(re_code), on = .(sar2_match_id)]]
#So bgSAE10 now knows where the individuals who are Hispanic or Latino and White are.

#Now let's do some tests first to ensure we've got what we think we have
nrow(bgSAR_10[ethnicity!="I"&re_code=="A"])==nrow(trSAR_10[ethnicity!="I"&re_code=="A"])
nrow(bgSAR_10[is.na(ethnicity)&re_code=="A"])==0

paste0("number of Hispanic or Latino who are not White alone: ",
       nrow(bgSAE_10)-nrow(bgSAR_10[!is.na(ethnicity)]))
#check to make sure totals line up with tracts
#SAE_Harris (tract) for just H and I has the same total size as bgSAE for blocks.
nrow(trSAE_10[re_code%in%c("H","I")])==nrow(bgSAE_10)
#the ones who are neither H nor I are all the non-White categories minus the Hispanics
H_A <- nrow(bgSAR_10[ethnicity=="H"&re_code=="A"])
paste0("The number of people listed as White by race and Hispanic or Latino by ",
       "ethnicity at the block level, ",
       H_A,", should equal the total from tracts")
H_A==nrow(trSAR_10[re_code=="A"])-nrow(trSAR_10[ethnicity=="I"])
#which means the complete White population has ethnicity assigned
paste0("This leaves ",nrow(bgSAE_10[re_code=="H"]) - H_A," non-White, Hispanic or Latino,",
       " out of ",nrow(bgSAR_10[re_code!="A"])," non-White individuals.")
```

We can add in the tract data at this point, matching on ethnicity, with race assigned within the subgroups where we don't know ethnicity by random sample, but there's a potential that some of the folks (1,774,203) who don't have ethnicity assigned at the block group will be misassigned within the tract to the wrong block group. That is, we can create a commutative representation, but it might break with other commutative relations at other levels that depend on how race is combined with ethnicity at the block group level. 

The temptation is to add the tract and group level data together. We have a bit more information to account for, though, and some other sources for triangulation. We know how many people are Hispanic in each block group, which means that we also know how many are non-Hispanic, but we don't know which races should also be listed as Hispanic. We do know where races are by block group, however. We could estimate by calculating the statistical likelihood of race x ethnicity x age for each race at the tract level and assigning it to each race x age at the block level. Surely better than a simple random distribution - it takes into account some strong priors - it still favors the individual calculation of likelihood to encounter an individual with certain characteristics instead of maintaining the structure of relations between race and ethnicity at the block group level. If, instead, we think of every relation at the tract level as given, so there is a total number of people with each combination of race x ethnicity x age at the tract level, and we know the White populations distribution at the block level, what you have is not the percentage that are in each block group, but the actual number that the block group provides, and our issue is to distribute them. 

We have each race x age total and each race x age, non-Hispanic at tract - so know Hispanic by disjunction at tract, but not what individuals to attach them to at the block group. This is a recurring issue for later, of course. 

We have each race x age total and White x age x ethnicity at the block group - so know non-White x ethnicity by disjunction. 
Could create a small data.table with the white x age x ethnicity and then the Hispanics and non-Hispanics counted out at the block level; then join that back to the tract level to pick up the race x age x ethnicity distribution at that level; then back to the bg. Are there any other places to test for lost relationality? Should we run a quick test on the subsection of race and re_code to demonstrate no loss before setting to "temp"? 

Move block by race over to trSAR? WE WANT ETHNICITY TO BE ALLOWED TO VARY, BUT NOT RACE, BY BLOCK 
DRAW!!!!
The idea is that you want the values to have been determined by the structure provided from both sides, with the values assigned from the one that constrains toward the individual values. [[it's a weird question about what the precise language should be]] Perhaps it's more like the idea that you have channels, like sieves, that come from different structures, but attach to the row as it passes through the alluvial. 

What we mean by structure is embodied in how the names can circulate in relation to the named - not the Kantian (as diagnosed by Sellars) moment of simple and single contact with the noumenal, but a channel within which the names circulate according to a structure. That is, we are not trying to find a place where the name is correct, but where the names have a logic of application and the structure that makes that application make sense is captured. This allows for the movement between different ways of naming to make sense. Here, we do it one at a time, so that the determination from the ethnicity as given in the table for tracts has the relation to race moved over from the block group tables. When we match block and ethnicity from the block group level for the group quarters, below, we are combining the two structured ways of naming by capturing how one moves between them. As we find more difficult situations, that sense of movement will allow us to speak to the sense of a dynamic dialogue between different ways of naming that demonstrates more capacity for eliciting and understanding structures. We are not trying to become relativists by refusing the idea of naming as the place where a truth is instantiated, but showing that ways of naming allow us to see structures that encompass multiple strategies for structuring how the names are attached, and to see the comparison in algebraic or topological terms (as ways of combining structures) and not as a single action of determination in an individual's act of naming what is as the result of an act of naming from a determined perspective on the whole. 

```{r move block by race over to trSAR to save structure when moving back later}
#first, get age_range onto trSAR in a way that matches
bgSAR_10[order(tract,first_age),("age_match_id"):=
                    paste0(tract,as.character(100000+seq.int(1:.N))),
                  by=.(tract)]
trSAR_10[order(tract,age),("age_match_id"):=
                    paste0(tract,as.character(100000+seq.int(1:.N))),
                  by=.(tract)]
trSAR_10[,("age_range"):=
                    bgSAR_10[.SD, list(age_range), on = .(age_match_id)]]
test <- table(bgSAR_10[,tract],
              bgSAR_10[,sex],
              bgSAR_10[,age_range]
              )==
        table(trSAR_10[,tract],
              trSAR_10[,sex],
              trSAR_10[,age_range]
              )
length(test[test==FALSE])==0
#then move over bg on race, sex, and age_range - White, then rest
bgSAR_10[re_code=="A",("race_match_id"):=
                    paste0(tract,sex,age_range,re_code,as.character(100000+seq.int(1:.N))),
                  by=.(tract,sex,age_range,re_code)]
trSAR_10[re_code=="A",("race_match_id"):=
                    paste0(tract,sex,age_range,re_code,as.character(100000+seq.int(1:.N))),
                  by=.(tract,sex,age_range,re_code)]
trSAR_10[re_code=="A",("geoid"):=
                    bgSAR_10[.SD, list(geoid), on = .(race_match_id)]]
test <- table(bgSAR_10[re_code=="A",tract],
              bgSAR_10[re_code=="A",geoid],
              bgSAR_10[re_code=="A",sex],
              bgSAR_10[re_code=="A",re_code],
              bgSAR_10[re_code=="A",age_range]
              )==
        table(trSAR_10[re_code=="A",tract],
              trSAR_10[re_code=="A",geoid],
              trSAR_10[re_code=="A",sex],
              trSAR_10[re_code=="A",re_code],
              trSAR_10[re_code=="A",age_range]
              )
length(test[test==FALSE])==0
#rest
bgSAR_10[re_code!="A",("race_match2_id"):=
                    paste0(tract,sex,age_range,re_code,as.character(100000+seq.int(1:.N))),
                  by=.(tract,sex,age_range,re_code)]
trSAR_10[re_code!="A",("race_match2_id"):=
                    paste0(tract,sex,age_range,re_code,as.character(100000+seq.int(1:.N))),
                  by=.(tract,sex,age_range,re_code)]
trSAR_10[re_code!="A",("geoid"):=
                    bgSAR_10[.SD, list(geoid), on = .(race_match2_id)]]
test <- table(bgSAR_10[,tract],
              bgSAR_10[,geoid],
              bgSAR_10[,sex],
              bgSAR_10[,re_code],
              bgSAR_10[,age_range]
              )==
        table(trSAR_10[,tract],
              trSAR_10[,geoid],
              trSAR_10[,sex],
              trSAR_10[,re_code],
              trSAR_10[,age_range]
              )
length(test[test==FALSE])==0

```

Then we can empty the bgSAR_10 re_code and race, in order to allow the ethnicity to be attached.
HAVE TO RETHINK!!!! DO THE DRAWINGS!!!

```{r join rest of bgSAR by emptying some cells and structuring possible distributions}
#take bgSAR and create a eth_match column
bgSAR_10[is.na(ethnicity),c("race","re_code"):="temp"]
#put the rest of the re_code=="H" from bgSAE onto bgSAR
bgSAR_10[is.na(ethnicity),("sar3_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+seq.int(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[re_code=="H"&is.na(r_code),("sar3_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+seq.int(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAR_10[is.na(ethnicity),("ethnicity"):=
                    bgSAE_10[.SD, list(re_code), on = .(sar3_match_id)]]
nrow(bgSAR_10[is.na(ethnicity)])==nrow(trSAR_10[!ethnicity%in%c("H","I")])
table(bgSAR_10[re_code=="A",ethnicity])
table(trSAR_10[re_code=="A",ethnicity])
table(bgSAR_10[re_code!="A",ethnicity])
table(trSAR_10[re_code!="A",ethnicity])
nrow(bgSAR_10[!is.na(ethnicity)])==nrow(bgSAE_10)
#then match on trSAR on ethnicity


nrow(trSAR_10[re_code!="A"&ethnicity=="H"])==nrow(bgSAR_10[re_code!="A"&ethnicity=="H"])
#now matching on ethnicity for all trSAR not re_code=="A" but still ethnicity=="H"
bgSAR_10[re_code!="A"&ethnicity=="H",("eth2_match_id"):=
                    paste0(tract,sex,re_code,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,re_code,age_range)]
trSAR_10[re_code!="A"&ethnicity=="H",("eth2_match_id"):=
                    paste0(tract,sex,re_code,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,re_code,age_range)]
bgSAR_10[re_code!="A"&ethnicity=="H",c("race","ethnicity"):=
                    trSAR_10[.SD, c(list(race),list(ethnicity)), on = .(eth2_match_id)]]
test <- table(bgSAR_10[ethnicity%in%c("H","I"),tract],
              bgSAR_10[ethnicity%in%c("H","I"),sex],
              bgSAR_10[ethnicity%in%c("H","I"),age_range],
              bgSAR_10[ethnicity%in%c("H","I"),ethnicity]
              )==
        table(trSAR_10[ethnicity%in%c("H","I"),tract],
              trSAR_10[ethnicity%in%c("H","I"),sex],
              trSAR_10[ethnicity%in%c("H","I"),age_range],
              trSAR_10[ethnicity%in%c("H","I"),ethnicity]
              )
length(test[test==FALSE])==0
#and then the rest
bgSAR_10[is.na(ethnicity),("eth3_match_id"):=
                    paste0(tract,sex,re_code,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,re_code,age_range)]
trSAR_10[re_code!="A"&ethnicity!="H",("eth3_match_id"):=
                    paste0(tract,sex,re_code,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,re_code,age_range)]
bgSAR_10[is.na(ethnicity),c("race","ethnicity"):=
                    trSAR_10[.SD, c(list(race),list(ethnicity)), on = .(eth3_match_id)]]
#then the tests 
nrow(bgSAR_10[race=="temp"])==0
nrow(bgSAR_10[is.na(ethnicity)])==0
#test that tract level is complete on bgSAR
test <- table(bgSAR_10[,tract],
              bgSAR_10[,sex],
              bgSAR_10[,re_code],
              bgSAR_10[,ethnicity],
              bgSAR_10[,age_range]
) == table(
  trSAR_10[,tract],
  trSAR_10[,sex],
  trSAR_10[,re_code],
  trSAR_10[,ethnicity],
  trSAR_10[,age_range]
)
length(test[test==FALSE])==0
#test that block level hasn't changed on ethnicity with bgSAE
test <- table(bgSAR_10[ethnicity%in%c("H","I"),geoid],
              bgSAR_10[ethnicity%in%c("H","I"),sex],
              bgSAR_10[ethnicity%in%c("H","I"),ethnicity],
              bgSAR_10[ethnicity%in%c("H","I"),age_range]
) == table(
  bgSAE_10[,geoid],
  bgSAE_10[,sex],
  bgSAE_10[,re_code],
  bgSAE_10[,age_range]
)
length(test[test==FALSE])==0
```

#ADD HOUSEHOLDS AND GROUP QUARTERS!!

[[longer conversation about what it means to understand this as a problem with finding the right unit for analysis - either the individual or the relations; can this use that distinction between building from the inside (set theory) and from the outside (type theory)?; that we can look to the 'empty' representation that only has the bear counts of SARE at the block group, and then have it carry some information about that geography that trumps our expectations but still has to mesh with the counts that come from other subsetting moves - in fact, and maybe from the beginning, this is where you're not really subsetting, but sub-typing; types are propositions... in any set of claims, not just abstract logical principles]]

SARE tract level for households does not have all the ethnicities that SARE for individuals had - just more breakdowns (23 groups) in age and H,I in ethnicity at the tract level; since we've already done the match on bgSAE, though, we can be sure that we can get the right matches. We'll want to add it to as much information as we can get from the Group Quarters, first, so that the extra information on that side isn't obscured. 

We'll also want to make sure to not move to matching at the tract level too quickly, because we want to conserve that tract level matching when assigning block level matches and there are possible ways to introduce non-commutative relations - for example, by having non-White Hispanics in the wrong block for households and then not having that category given for group quarters and so having a mis-assignment.

What we'll do, therefore, is move the tract levl information from bgSAR to the tract level on HH_SARE, since no information is lost that way, and then we'll do refinements from that side, before moving back to the bgSAR and making assignments - to the best of our ability! some information was not given - at the block level.

```{r download household family SARE tract}
trHH_SARE_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "PCT13",county_num = county,
                         block="tract",api_type="dec/sf1",path_suff="est.csv")
trHH_SARE_data_10 <- as.data.table(trHH_SARE_data_from_census_10)
trHH_SARE_data_10[,4:ncol(trHH_SARE_data_10)] <- 
  trHH_SARE_data_10[,lapply(.SD[,4:ncol(trHH_SARE_data_10)], as.numeric)]
rm(trHH_SARE_data_from_census_10)
```

Then do our basic checks: 
```{r household own kids race / ethnicity age test problems}
check_summary <- census_table_check(trHH_SARE_data_10[!str_detect(concept,"HISPANIC")], 
      "SEX BY AGE FOR THE POPULATION IN HOUSEHOLDS","individuals in state") 
cat(check_summary[1])
```

```{r expand family SARE tract, warning = FALSE}
race_codes <- c("A","B","C","D","E","F","G")
trHHr_10 <- trHH_SARE_data_10 %>%
  pivot_longer(4:ncol(trHH_SARE_data_10),names_to = "tract", values_to = "number_sams") %>% 
  filter(substr(tract,3,5)==county) %>% #only Harris County
  mutate(
    race = substr(name,7,7),
    label = str_remove_all(label,"Total!!")) %>%
  filter(label != "Total") %>% #keep only the ones that aren't aggregated
  separate(label, c("sex","age_range"), sep = "!!", remove = F, convert = FALSE) %>%
  #using the beg/end_age_P22, since they match.
  mutate(age_range = str_replace_all(age_range,"and","to"),
         age_range = str_replace_all(age_range,"85","85 to 110"),
         age_range = str_replace_all(age_range,"Under 5","0  to 05"),
         age_range = str_replace_all(age_range,"5 to 9","05 to 09"),
         age_range = str_replace_all(age_range,"20 years","20 to 20"),
         age_range = str_replace_all(age_range,"21 years","21 to 21"),
         first_age = as.numeric(substr(age_range,1,2)),
         last_age = as.numeric(substr(age_range,7,9))) %>%
  filter(!is.na(age_range) & race%in%race_codes) %>% #to get rid of aggregations by family
  uncount(number_sams,.id = "hhr_tr_SARE_id",.remove = TRUE) 
trHHr_10 <- as.data.table(trHHr_10) #dyplyr had stripped it of dt
paste0("Number of individuals in households by race: ", nrow(trHHr_10))

#and same for ethnicity
trHHe_10 <- trHH_SARE_data_10 %>%
  pivot_longer(4:ncol(trHH_SARE_data_10),names_to = "tract", values_to = "number_sams") %>% 
  filter(substr(tract,3,5)==county) %>% #only Harris County
  mutate(
    ethnicity = substr(name,7,7),
    label = str_remove_all(label,"Total!!")) %>%
  filter(label != "Total") %>% #keep only the ones that aren't aggregated
  separate(label, c("sex","age_range"), sep = "!!", remove = F, convert = FALSE) %>%
  #using the beg/end_age_P22, since they match.
  mutate(age_range = str_replace_all(age_range,"and","to"),
         age_range = str_replace_all(age_range,"85","85 to 110"),
         age_range = str_replace_all(age_range,"Under 5","0  to 05"),
         age_range = str_replace_all(age_range,"5 to 9","05 to 09"),
         age_range = str_replace_all(age_range,"20 years","20 to 20"),
         age_range = str_replace_all(age_range,"21 years","21 to 21"),
         first_age = as.numeric(substr(age_range,1,2)),
         last_age = as.numeric(substr(age_range,7,9))) %>%
  filter(!is.na(age_range) & ethnicity%in%c("H","I")) %>% #to get rid of aggregations by family
  uncount(number_sams,.id = "hhe_tr_SARE_id",.remove = TRUE) 
trHHe_10 <- as.data.table(trHHe_10) 
paste0("Number of individuals in households by ethnicity: ", nrow(trHHe_10))
#ensure age_range matches
test <- unique(trHHe_10[order(age_range),age_range])==
  unique(trHHr_10[order(age_range),age_range])
length(test[test==FALSE])==0
rm(trHH_SARE_data_10)
```

Let's do a couple of quick tests to make sure that the overall approach will work. We're assuming that there are a determined set of individual rows, of which some portion is the population living in households, and that the tables representing characteristics of those individuals all refer back to the same overall representation in terms of those rows. Let's just make sure that the counts per cell are less for all our relevant combinations in the household tables than in the overall.

Later, we'll match with the block group; the remainder between those in households and the whole population (by the rules of the construction and the disjunctive syllogism) should be those in group quarters, but we'll also take some steps to ensure that we haven't accidentally misattributed cases. In particular, we don't want to lose any of the fine-grained information on race and ethnicity combined at the tract level, or the greater geographic specificity available for all demographics at the block level for the total population table (bgSAR). The only race category that is completely exhausted by the ethnicity categories given in the household population tables is White, and so we go ahead and match them, with those in the Hispanic non-white remaining to be counted and distributed more carefully, below.

```{r join trHHr_10 and trHHe_10 for White non-Hispanic}
trHHr_10[race=="A",("trHHsae_match_id"):=
                    paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHe_10[ethnicity=="I",("trHHsae_match_id"):=
                    paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHr_10[race=="A",("ethnicity"):=
                    trHHe_10[.SD, list(ethnicity), on = .(trHHsae_match_id)]]
nrow(trHHr_10[!is.na(ethnicity)])==nrow(trHHe_10[ethnicity=="I"])
test <- table(
  trHHr_10[!is.na(ethnicity),tract],
  trHHr_10[!is.na(ethnicity),sex],
  trHHr_10[!is.na(ethnicity),age_range],
  trHHr_10[!is.na(ethnicity),ethnicity]
) == table(
  trHHe_10[ethnicity=="I",tract],
  trHHe_10[ethnicity=="I",sex],
  trHHe_10[ethnicity=="I",age_range],
  trHHe_10[ethnicity=="I",ethnicity]
)
length(test[test==FALSE])==0
```

By the rules as we understand them, all individuals with race of "White Alone" are either Hispanic or not, but let's check that and use it to also give us some sense of a new method we will use to assign a specific number to missing categories, and then use that ordering within our types to make more detailed assignations. We will try to create an expanded table that includes as much information as possible about the individuals in households and group quarters, including the number of individuals in categories that we don't yet know how to assign, and then join with the block group information for all individuals. Talk a bit about the conservation of structure in terms of relation as a problem of well-ordered sets and the construction of the domains within which things are determined - building from outside in and not from inside out.

Although we know enough to fill in for the rest of the race=="A" (White) population, we want to create matching ids with an if clause that changes if the number is larger than the available designations in that category. We're testing the approach, and will later use it in places where we can't test it on individual steps (but can confirm whether it worked as a composite of steps). There are some complexities, here, that will pay off later as we think more carefully about what it means to have order conserved across table representations.

```{r count of ethnicity on trHHe_10 and trHHr_10 to determine non-White remainder}
trHHr_10[race=="A"&is.na(ethnicity),("ethnicity_check"):="H"]
#but we want to mark the ones in trHHe_10, so we know which ones to eliminate (although an anti-join should also work)
trHHr_10[race=="A"&is.na(ethnicity),("trHHsae2_match_id"):=paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHe_10[ethnicity=="H",("trHHsae2_match_id"):=
                    paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHr_10[race=="A"&is.na(ethnicity),("ethnicity"):=
                    trHHe_10[.SD, list(ethnicity), on = .(trHHsae2_match_id)]]
trHHe_10[ethnicity=="H",("marked_r"):=
                    trHHr_10[.SD, list(race), on = .(trHHsae2_match_id)]]
#test to see whether race=="A"&ethnicity!="I" tables are equal on tract,sex,age_range with ethnicity=="H"
nrow(trHHr_10[race=="A"])-nrow(trHHr_10[ethnicity=="I"])==nrow(trHHr_10[ethnicity=="H"])
nrow(trHHe_10[!is.na(marked_r)])==nrow(trHHr_10[ethnicity=="H"])
#also put a temporary variable in place to see if it's the same thing as just having placed the designation directly
trHHr_10[race=="A"&is.na(ethnicity),("ethnicity_check"):="H"]
nrow(trHHr_10[!is.na(ethnicity_check)])
test <- table(trHHr_10[race=="A"&ethnicity!="I",tract],
              trHHr_10[race=="A"&ethnicity!="I",sex],
              trHHr_10[race=="A"&ethnicity!="I",age_range],
              trHHr_10[race=="A"&ethnicity!="I",ethnicity_check]
) == table(
  trHHr_10[ethnicity=="H",tract],
  trHHr_10[ethnicity=="H",sex],
  trHHr_10[ethnicity=="H",age_range],
  trHHr_10[ethnicity=="H",ethnicity]
)
length(test[test==FALSE])==0
test <- table(trHHr_10[ethnicity=="I",tract],
              trHHr_10[ethnicity=="I",sex],
              trHHr_10[ethnicity=="I",age_range]
) == table(
  trHHe_10[ethnicity=="I",tract],
  trHHe_10[ethnicity=="I",sex],
  trHHe_10[ethnicity=="I",age_range]
)
length(test[test==FALSE])==0
trHHr_10$ethnicity_check <- NULL
```

The difference between the number of individuals in households and the number overall is only about 1% of the population - it's the people who are in so-called "group quarters," and includes different types of institutionalized settings, from college dorms to nursing homes to prisons. They are a small portion of the whole, and even within that small subset, there is enormous diversity, but they include some of the most vulnerable and important populations from the perspective of public health. 

For the household data, we don't have anything that tells us which block they're in yet, although we do have a great deal of block group level data that we'll incorporate in making_sam_hh.Rmd, but it is mostly for the head of household and not for the entire household. Here we want to show how to use the known structure from the block group level sex, age, race, and ethnicity to create a better representation of the households. Since we don't want to create structures that aren't justified by a known relation, we'll create counts for subgroups to use for the matching. So that those counts will be correct at the tract level, we'll need to add the group quarters to the households. To do that, first, we'll add rows to the household population data using only those facts we're sure of, with the other tables helping us add determination. Remember that we know the combination of race and ethnicity for every individual without any loss at the tract level, and only have worries about some of the combinations of non-White and Hispanic or Latino at the block level. We'll use that fact to grow the household table appropriately. Mostly, "appropriately" here means only that we aren't accidentally adding extra structure without warrant; we want to begin, at least, asking strictly mathematical questions about what we can do, given that the political questions about naming are given, at least for the present context. We can approach the mathematics by thinking of both tables as ordered sets, and then adding to the household table the number of rows needed to finish out the rows by sex, age, race, and ethnicity in each tract by group quarters. The group quarters tables includes some information about the block group structure that may also help, so we'll want to include it before distributing the household information to the total population. But let's create a workspace where we allow the characteristics we don't know to vary while securing the traits we do know into place in the table. We know that the information on the tract, race, and ethnicity is complete for the White population, and that the ordering we have for the block level by those same variables is complete - so no information will be lost by matching to the block level for that subgroup, with White population in group quarters and non-White populations overall to be distributed after taking into account the block group data.

Should be able to do all of them at once, after doing group quarters - the .N on the cell size from bgSAR is key; do them here, as something to test against on the full idea, too.

```{r add White population in households to block group population}
trHHr_10[,("household"):=list("In household")] #more convenient for below
bgSAR_10[re_code=="A",("bg_saW_hh_match_id"):=
                    paste0(tract,ethnicity,sex,first_age,as.character(100000+sample(1:.N))),
                  by=.(tract,ethnicity,sex,age_range)]
trHHr_10[race=="A",("bg_saW_hh_match_id"):=
                    paste0(tract,ethnicity,sex,first_age,as.character(100000+sample(1:.N))),
                  by=.(tract,ethnicity,sex,age_range)]
bgSAR_10[re_code=="A",("household"):=
                    trHHr_10[.SD, list(household), on = .(bg_saW_hh_match_id)]]
#if race=="A", and not "In household", then "In group quarters"
bgSAR_10[re_code=="A"&is.na(household),("household"):="In group quarters"]
#this adds totals, and can be done later in ways that make it only the remaining H...
bgSAR_10[,("bg_eth_total"):=.N,by=.(geoid,ethnicity)]#have to match with ethnicity in that row
bgSAR_10[ethnicity=="H",("bg_H_eth_total"):=.N,by=.(geoid,ethnicity)]
bgSAR_10[,("bg_H_total"):=.SD[1,list(as.integer(bg_H_eth_total))],
         by=.(geoid)]
#tests
nrow(trHHr_10[race=="A"])==nrow(bgSAR_10[household=="In household"])
test <- table(
  bgSAR_10[re_code=="A"&household=="In household",tract],
  bgSAR_10[re_code=="A"&household=="In household",sex],
  bgSAR_10[re_code=="A"&household=="In household",ethnicity],
  bgSAR_10[re_code=="A"&household=="In household",first_age]
)==table(
  trHHr_10[race=="A",tract],
  trHHr_10[race=="A",sex],
  trHHr_10[race=="A",ethnicity],
  trHHr_10[race=="A",first_age]
)
length(test[test==FALSE])==0
```

One of our difficulties, above, was that we weren't sure how to distribute some of the race and ethnicity combinations that we knew to be valid at the tract level once we moved to the block level - although we knew distributions at the block level for both race and ethnicity, which gives us a ceiling for the possible misattributions. We have some information on group quarters that may help, if we're careful not to overwrite, but in general the very detailed information about age and race means that the household structure should be added first, with the specifics for quarters added within that structure (since no other designations besides sex, age, race, and ethnicity are given at this point, and so no information is lost). 

We're hoping that some of the structure from additional tables will help us determine how the non-White ethnicity (re_code=="H" or not) should be distributed; accordingly, we have on each row a marker for how many should be in that category by tract, sex, and the detailed age_range. The households need to account for the distribution of people in group quarters by sex, age, race, and ethnicity in order to know how many in each group are left to be distributed.

Group quarters are treated differently in some of the tables and often not included in others. They're characteristics are reported very differently, especially under the ACS rules for small populations, but they are an interesting, too little understood, and often vulnerable population. Let's download the tables from the publicly accessible parts of the 2010 census and see what we can do.

https://www.socialexplorer.com/data/C2010RC/metadata/?ds=SF1&table=P0430; they changed how they were categorized between 2000 and 2010
group quarters are needed b/c hh don't include them - PC03 offers more detail, on population in correctional facilities by age and sex, but it comes out as all NAs.

```{r group quarters to add block}
group_quarters_block_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P43",county_num = county,
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
bg_group_quarters_data_10 <- as.data.table(group_quarters_block_data_from_census_10)
bg_group_quarters_data_10[,4:ncol(bg_group_quarters_data_10)] <- 
  bg_group_quarters_data_10[,lapply(.SD[,4:ncol(bg_group_quarters_data_10)], as.numeric)]
rm(group_quarters_block_data_from_census_10)
```

And the basic check on integrity of file.
```{r group quarters test downloaded file}
check_summary <- census_table_check(bg_group_quarters_data_10, 
      "GROUP QUARTERS POPULATION BY SEX BY AGE BY GROUP QUARTERS TYPE","individuals","Total",2) 
cat(check_summary[1])

```

The fact that 18% sum incorrectly is very odd. Let's see if there's an obvious place where it's not working. 
Let's look at a different way of calculating the totals against each other. Here, we test the rows for the subtotals by sex and age against the total of the whole, and everything lines up.

```{r double check on group quarters subtotals}
test <- colSums(bg_group_quarters_data_10[label=="Total",
                                          4:ncol(bg_group_quarters_data_10)])*3 ==
    colSums(bg_group_quarters_data_10[!str_detect(label,"population"),
                                      4:ncol(bg_group_quarters_data_10)])
length(test[test==FALSE])==0
```
The first test looked at the subgroup sums, and this one will sum at the lowest granularity provided in the tables, ensuring that all people counted as part of the aggregate are also given one and only one label at that level.

```{r triple check on group quarters totals}
test <- colSums(bg_group_quarters_data_10[label=="Total",
                                          4:ncol(bg_group_quarters_data_10)])*2 ==
    colSums(bg_group_quarters_data_10[str_detect(label,"population"),
                                      4:ncol(bg_group_quarters_data_10)])
length(test[test==FALSE])==0
```

It seems to have been an oddity in how the rows were tallied so that the subtotals across all second level groups were three times the total, but for the lowest level, only twice. Let's go ahead and expand, before making decisions about the tract level data.

```{r expand group quarters block}
bg_group_quarters_10 <- bg_group_quarters_data_10 %>%
  pivot_longer(4:ncol(bg_group_quarters_data_10),names_to = "geoid", values_to = "number_sams") %>% 
  mutate(tract = str_remove_all(geoid,"_"),
         tract = substr(tract,1,11),
         label = str_remove_all(label,"Total!!")) %>%
  filter(label != "Total") %>% #keep only the ones that aren't aggregated
  separate(label, c("sex","age_range","institutionalized","gq_type"), sep = "!!", remove = F, convert = FALSE) %>%
  mutate(beg_age_gq = case_when(age_range=="Under 18 years" ~ as.numeric(0),
                                age_range=="18 to 64 years" ~ as.numeric(18),
                                age_range=="65 years and over" ~ as.numeric(65)),
         gq_type = case_when(str_detect(gq_type,"College") ~ #catching some idiosyncratic capitalization
                               "College/University student housing",
                             str_detect(gq_type,"Nursing") ~
                               "Nursing facilities",
                             gq_type=="Correctional facilities for adults (101-106)" ~
                               "Correctional facilities for adults",
                             gq_type=="Juvenile facilities (201-203)" ~
                               "Juvenile facilities",
                             gq_type=="Other institutional facilities (401-405)" ~
                               "Other institutional facilities",
                             gq_type=="Other noninstitutional facilities (701-702, 704, 706, 801-802, 900-901, 903-904)" ~
                               "Other noninstitutional facilities")) %>%
  filter(!is.na(gq_type)) %>% #to get rid of aggregations by institutional type
  uncount(number_sams,.id = "bggq_id",.remove = TRUE) 
bggq_10 <- as.data.table(bg_group_quarters_10) #dyplyr had stripped it of dt
paste0("Number of households in file: ", nrow(bggq_10))
#check that the number of households plus number of group quarters = total population
nrow(bggq_10)+nrow(trHHr_10)==nrow(bgSAR_10)
rm(list = ls(pattern="bg_group_quarters")) #think through all the clean up in the right places!!
```

The age ranges are quite broad, and the absence of race and ethnicity data would make it hard to make assignments with any confidence. However, PCT20 has tract level group quarters by type with race and ethnicity, while PCT21 has tract level group quarters by sex by age by type and PCT22 has the more age ranges (but only over 18) for group quarters by sex and type. We want to join 20, 21, and 22 to get the best match at the tract level, then we'll add the information we have at block.

```{r group quarters to add tract race ethnicity}
group_quarters_tract_type_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "PCT20",county_num = county,
                         block="tract",api_type="dec/sf1",path_suff="est.csv")
tr_re_group_quarters_type_data_10 <- as.data.table(group_quarters_tract_type_data_from_census_10) %>%
      select(name,label,concept,starts_with(st_county))
tr_re_group_quarters_type_data_10[,4:ncol(tr_re_group_quarters_type_data_10)] <- 
  tr_re_group_quarters_type_data_10[,lapply(.SD[,4:ncol(tr_re_group_quarters_type_data_10)], as.numeric)]
rm(group_quarters_tract_type_data_from_census_10)
```


And the basic check on integrity of file.
```{r group quarters test downloaded file}
check_summary <- census_table_check(tr_re_group_quarters_type_data_10, 
      "GROUP QUARTERS POPULATION BY GROUP QUARTERS TYPE","individuals","Total",3) 
cat(check_summary[1])

```
We should check, just like we did on the block group for group quarters, why the totals don't sum. Here, we do a very ad hoc process on just the overall totals, and not on each race and ethnicity.

```{r double check on tr_re_group quarters subtotals}
test <- colSums(tr_re_group_quarters_type_data_10[label=="Total"&length(name)==9,
                                          4:ncol(tr_re_group_quarters_type_data_10)])*2 ==
    colSums(tr_re_group_quarters_type_data_10[!str_detect(label,"population")&length(name)==9,
                                      4:ncol(tr_re_group_quarters_type_data_10)])
length(test[test==FALSE])==0
test <- colSums(tr_re_group_quarters_type_data_10[label=="Total"&length(name)==9,
                                          4:ncol(tr_re_group_quarters_type_data_10)])*2 ==
    colSums(tr_re_group_quarters_type_data_10[str_detect(label,"population")&length(name)==9,
                                      4:ncol(tr_re_group_quarters_type_data_10)])
length(test[test==FALSE])==0
```

For right now, let's expand this group into both a race and ethnicity table.

```{r expand tract group quarters population by type race and ethnicity}
tr_re_group_quarters_10 <- tr_re_group_quarters_type_data_10 %>%
  pivot_longer(4:ncol(tr_re_group_quarters_type_data_10),names_to = "tract", values_to = "number_sams") %>% 
  mutate(
    re_code = substr(name,7,7),
    label = str_remove_all(label,"Total!!")) %>%
  filter(label != "Total") %>% #keep only the ones that aren't aggregated
  separate(label, c("institutionalized","gq_type","gq_specs"), sep = "!!", remove = F, convert = FALSE) %>%
  mutate(
    gq_type = case_when(str_detect(gq_type,"College") ~
                               "College/University student housing",
                             str_detect(gq_type,"Nursing") ~
                               "Nursing facilities",
                             gq_type=="Correctional facilities for adults (101-106)" ~
                               "Correctional facilities for adults",
                             gq_type=="Juvenile facilities (201-203)" ~
                               "Juvenile facilities",
                             gq_type=="Other institutional facilities (401-405)" ~
                               "Other institutional facilities",
                             gq_type=="Other noninstitutional facilities (701-702, 704, 706, 801-802, 900-901, 903-904)" ~
                               "Other noninstitutional facilities"),
    gq_specs = case_when(str_detect(gq_type,"College") ~ "College/University student housing",
                         str_detect(gq_type,"Nursing") ~ "Nursing facilities/Skilled-nursing facilities",
                        TRUE ~ gq_specs)
  ) %>%
  filter(!is.na(gq_specs)) %>% #to get rid of aggregations by institutional type
  uncount(number_sams,.id = "trregq_id",.remove = TRUE) 
tr_re_group_quarters_10 <- as.data.table(tr_re_group_quarters_10) 
#Break them into race and ethnicity tables.
tr_r_gq_10 <- tr_re_group_quarters_10[re_code%in%race_codes]
tr_e_gq_10 <- tr_re_group_quarters_10[re_code%in%c("H","I")]
paste0("Number of individuals in group quarters table: ", nrow(tr_r_gq_10))
#check that the number of households plus number of group quarters = total population
nrow(tr_r_gq_10)+nrow(trHHr_10)==nrow(bgSAR_10)
#clean up
rm(tr_re_group_quarters_10)
rm(tr_re_group_quarters_type_data_10)
```

PCT21 is tract level group quarters, with sex by age. It would be nice for our construction to have the relations by sex, age, and race, ethnicity in the same table, but the Census Bureau decided to mask that information. The protection of privacy for the individuals should be balanced with the need to understand potential patterns across categories, but there are a number of places that such analysis could happen. There are, additionally, many ways in which the population characteristics are available, including many data tables not controlled by the Census Bureau that have very detailed information. The problem of privacy is quickly transforming, and there is no clear long-term solution that addresses all goals. 

```{r group quarters to add tract sex age}
group_quarters_tract_sex_age_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "PCT21",county_num = county,
                         block="tract",api_type="dec/sf1",path_suff="est.csv")
tr_sex_age_group_quarters_data_10 <- as.data.table(group_quarters_tract_sex_age_data_from_census_10)%>%
      select(name,label,concept,starts_with(st_county))
tr_sex_age_group_quarters_data_10[,4:ncol(tr_sex_age_group_quarters_data_10)] <- 
  tr_sex_age_group_quarters_data_10[,lapply(.SD[,4:ncol(tr_sex_age_group_quarters_data_10)], as.numeric)]
rm(group_quarters_tract_sex_age_data_from_census_10)
```

And the basic check on integrity of file.
```{r group quarters test downloaded file}
check_summary <- census_table_check(tr_sex_age_group_quarters_data_10, 
      "GROUP QUARTERS POPULATION BY SEX BY AGE BY GROUP QUARTERS TYPE","individuals","Total",5) 
cat(check_summary[1])

```

Here, we check again on the lower level sums, in case something doesn't match at a deeper level.

```{r double check on tr_sa_group quarters subtotals}
test <- colSums(tr_sex_age_group_quarters_data_10[label=="Total",
                                          4:ncol(tr_sex_age_group_quarters_data_10)])*3 ==
    colSums(tr_sex_age_group_quarters_data_10[!str_detect(label,"population"),
                                      4:ncol(tr_sex_age_group_quarters_data_10)])
length(test[test==FALSE])==0
```

For right now, let's expand this group.

```{r expand tract group quarters population by type sex and age}
tr_sex_age_group_quarters_10 <- tr_sex_age_group_quarters_data_10 %>%
  pivot_longer(4:ncol(tr_sex_age_group_quarters_data_10),names_to = "tract", values_to = "number_sams") %>% 
  mutate(label = str_remove_all(label,"Total!!")) %>%
  filter(label != "Total") %>% #keep only the ones that aren't aggregated
  separate(label, c("sex","age_range","institutionalized","gq_type","gq_specs"), sep = "!!", remove = F, convert = FALSE) %>%
  mutate(
    beg_age_gq = case_when(age_range=="Under 18 years" ~ as.numeric(0),
                                age_range=="18 to 64 years" ~ as.numeric(18),
                                age_range=="65 years and over" ~ as.numeric(65)),
    gq_type = case_when(str_detect(gq_type,"College") ~
                               "College/University student housing",
                             str_detect(gq_type,"Nursing") ~
                               "Nursing facilities",
                             gq_type=="Correctional facilities for adults (101-106)" ~
                               "Correctional facilities for adults",
                             gq_type=="Juvenile facilities (201-203)" ~
                               "Juvenile facilities",
                             gq_type=="Other institutional facilities (401-405)" ~
                               "Other institutional facilities",
                             gq_type=="Other noninstitutional facilities (701-702, 704, 706, 801-802, 900-901, 903-904)" ~
                               "Other noninstitutional facilities"),
    gq_specs = case_when(str_detect(gq_type,"College") ~ "College/University student housing",
                         str_detect(gq_type,"Nursing") ~ "Nursing facilities/Skilled-nursing facilities",
                        TRUE ~ gq_specs)
  ) %>%
  filter(!is.na(gq_specs)) %>% #to get rid of aggregations by institutional type
  uncount(number_sams,.id = "trsagq_id",.remove = TRUE) 
tr_sa_gq_10 <- as.data.table(tr_sex_age_group_quarters_10) 
paste0("Number of individuals in group quarters table: ", nrow(tr_sex_age_group_quarters_10))
#check that the number of households plus number of group quarters = total population
nrow(tr_sa_gq_10)+nrow(trHHr_10)==nrow(bgSAR_10)
#clean up
rm(tr_sex_age_group_quarters_data_10)
```

And PCT22 is tract level group quarters, 18 years old and over, but with sex by race/ethnicity

```{r group quarters add tract sex type race and ethnicity over 17}
group_quarters_tract_sex_type_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "PCT22",county_num = county,
                         block="tract",api_type="dec/sf1",path_suff="est.csv")
tr_re_sex_type_group_quarters_data_10 <- as.data.table(group_quarters_tract_sex_type_data_from_census_10) %>%
      select(name,label,concept,starts_with(st_county))
tr_re_sex_type_group_quarters_data_10[,4:ncol(tr_re_sex_type_group_quarters_data_10)] <- 
  tr_re_sex_type_group_quarters_data_10[,lapply(.SD[,4:ncol(tr_re_sex_type_group_quarters_data_10)], as.numeric)]
rm(group_quarters_tract_sex_type_data_from_census_10)
```

```{r group quarters test PCT22 file}
check_summary <- census_table_check(tr_re_sex_type_group_quarters_data_10, 
      "GROUP QUARTERS POPULATION BY SEX  BY GROUP QUARTERS TYPE FOR THE POPULATION 18 YEARS AND OVER","individuals","Total",3) 
cat(check_summary[1])

```

Expand the tract level race, ethnicity, sex, and age data for over 17 group quarters.

```{r expand tract group quarters population by type sex, age, race and ethnicity}
tr_re_over17_type_group_quarters_10 <- tr_re_sex_type_group_quarters_data_10 %>%
  pivot_longer(4:ncol(tr_re_sex_type_group_quarters_data_10),names_to = "tract", values_to = "number_sams") %>% 
  mutate(
    re_code = substr(name,7,7),
    label = str_remove_all(label,"Total!!")) %>%
  filter(label != "Total") %>% #keep only the ones that aren't aggregated
  separate(label, c("sex","institutionalized","gq_type"), sep = "!!", remove = F, convert = FALSE) %>%
  mutate(gq_type = case_when(str_detect(gq_type,"College") ~
                               "College/University student housing",
                             str_detect(gq_type,"Nursing") ~
                               "Nursing facilities",
                             gq_type=="Correctional facilities for adults (101-106)" ~
                               "Correctional facilities for adults",
                             gq_type=="Juvenile facilities (201-203)" ~
                               "Juvenile facilities",
                             gq_type=="Other institutional facilities (401-405)" ~
                               "Other institutional facilities",
                             gq_type=="Other noninstitutional facilities (701-702, 704, 706, 801-802, 900-901, 903-904)" ~
                               "Other noninstitutional facilities")) %>%
  filter(!is.na(gq_type)) %>% #to get rid of aggregations by institutional type
  uncount(number_sams,.id = "trov18gq_id",.remove = TRUE) 
tr_re_over17_gq_10 <- as.data.table(tr_re_over17_type_group_quarters_10) 
#Break them into race and ethnicity tables.
tr_r_gq_over17_10 <- tr_re_over17_gq_10[re_code%in%race_codes]
tr_e_gq_over17_10 <- tr_re_over17_gq_10[re_code%in%c("H","I")]
paste0("Number of individuals in group quarters table: ", nrow(tr_r_gq_over17_10))
#check that the number of households plus number of group quarters = total population
nrow(tr_r_gq_over17_10)+nrow(tr_sa_gq_10[age_range=="Under 18 years"])+nrow(trHHr_10)==nrow(bgSAR_10)
#clean up
```

Joining PCT21 (tr_sa_gq_10) with P43 (bggq_10), since PCT21 just adds group quarter specifics. This follows the rules for triangle commutativity, but has a potential of mismatching at the block group level so we move it over with a caveat by putting a tr_ prefix on the gq_specs variable (tr_gq_specs).

```{r add gq_spec_count_sa to bggq from tr_sa_gq}
#tr_sa_gq_10[,("gq_spec_count_sa"):=.N,by=c("tract","sex","age_range","institutionalized","gq_type","gq_specs")]
bggq_10[,("bg_sa_gq_match_id"):=
                    paste0(tract,sex,age_range,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range,gq_type)]
tr_sa_gq_10[,("bg_sa_gq_match_id"):=
                    paste0(tract,sex,age_range,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range,gq_type)]
bggq_10[,("tr_gq_specs"):=
                    tr_sa_gq_10[.SD, list(gq_specs), on = .(bg_sa_gq_match_id)]]
#tests
nrow(bggq_10[is.na(tr_gq_specs)])==0
test <- table(
  bggq_10[,tract],
  bggq_10[,sex],
  bggq_10[,age_range],
  bggq_10[,institutionalized],
  bggq_10[,gq_type],
  bggq_10[,tr_gq_specs]
)==table(
  tr_sa_gq_10[,tract],
  tr_sa_gq_10[,sex],
  tr_sa_gq_10[,age_range],
  tr_sa_gq_10[,institutionalized],
  tr_sa_gq_10[,gq_type],
  tr_sa_gq_10[,gq_specs]
)
length(test[test==FALSE])==0
```

But now we need to do some better matching, and see if that could also help us determine which group quarter specifics belong in each block group. We know how many we're supposed to have by race and how many by ethnicity of "H" and "I" in households and group quarters, but not in the same place, and we know some age_range information on group quarters, but not at as granular of a level as for the households.  
We know more about the White population, because "White, not Hispanic or Latino" is given, so we match that group first, before moving to the rest.

```{r attempt matching with span for group quarters on race and ethnicity}
tr_r_gq_10[,("tr_r_count"):=.N,by=c("tract","re_code","gq_type","gq_specs")]

tr_r_gq_10[re_code=="A",("tr_r_gq_match_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
tr_e_gq_10[re_code=="I",("tr_r_gq_match_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
tr_r_gq_10[re_code=="A",("ethnicity"):=
                    tr_e_gq_10[.SD, list(re_code), on = .(tr_r_gq_match_id)]]
tr_e_gq_10[re_code=="I",c("matched"):=
                    tr_r_gq_10[.SD, list(re_code), on = .(tr_r_gq_match_id)]]
#now do ones who are re_code=="A" for race and "H" for ethnicity.
tr_r_gq_10[re_code=="A"&is.na(ethnicity),("tr_r_gq_match1_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
tr_e_gq_10[re_code=="H",("tr_r_gq_match1_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
tr_r_gq_10[re_code=="A"&is.na(ethnicity),("ethnicity"):=
                    tr_e_gq_10[.SD, list(re_code), on = .(tr_r_gq_match1_id)]]
tr_e_gq_10[re_code=="H",c("matched"):=
                    tr_r_gq_10[.SD, list(re_code), on = .(tr_r_gq_match1_id)]]
nrow(tr_r_gq_10[re_code=="A"])==nrow(tr_e_gq_10[!is.na(matched)])
nrow(bgSAR_10[re_code=="A"&household=="In group quarters"])==nrow(tr_e_gq_10[!is.na(matched)])
test <- table(
  tr_r_gq_10[re_code=="A",tract],
  tr_r_gq_10[re_code=="A",re_code],
  tr_r_gq_10[re_code=="A",ethnicity]
)==table(
  bgSAR_10[re_code=="A"&household=="In group quarters",tract],
  bgSAR_10[re_code=="A"&household=="In group quarters",re_code],
  bgSAR_10[re_code=="A"&household=="In group quarters",ethnicity]
)
length(test[test==FALSE])==0
#those count as ethnicity known
#now do re_code!="A"; the remaining hispanic are tr_eh_remain_count as an integer for everyone in that subgroup
tr_r_gq_10[is.na(ethnicity),("tr_r_gq_match2_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
tr_e_gq_10[is.na(matched),("tr_r_gq_match2_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
tr_e_gq_10[is.na(matched),("tr_eh_remain_count"):=.N,
           by=.(tract,gq_type,gq_specs)]
tr_r_gq_10[is.na(ethnicity),("tr_eh_remain_count"):=
                    tr_e_gq_10[.SD, list(as.integer(tr_eh_remain_count)), 
                               on = .(tr_r_gq_match2_id)]]
#and for matching with over 17, we want to ensure we know how many remain in each tract with ethnicity of H.
tr_e_gq_10[is.na(matched),("tr_eh_remain_type_count"):=.N,
           by=.(tract,gq_type)]
tr_r_gq_10[is.na(ethnicity),("tr_eh_remain_type_count"):=
                    tr_e_gq_10[.SD, list(as.integer(tr_eh_remain_type_count)), 
                               on = .(tr_r_gq_match2_id)]]
#this assigns to every row in a category that is not fully determined, the remaining number of H.
tr_r_gq_10[is.na(ethnicity)&order(tr_eh_remain_count),
           ("tr_eh_remain_count"):=
             .SD[1,list(as.integer(tr_eh_remain_count))],
           by = .(tract,gq_type,gq_specs)]
tr_r_gq_10[is.na(ethnicity)&order(tr_eh_remain_type_count),
           ("tr_eh_remain_type_count"):=
             .SD[1,list(as.integer(tr_eh_remain_type_count))],
           by = .(tract,gq_type)]
#look at the following carefully!!! should they be race/eth or eth/race? and what happens div/0???
tr_r_gq_10[is.na(ethnicity),
           ("tr_r_eh_spec_prob"):=list(tr_r_count/tr_eh_remain_count),
           by = .(tract,gq_type,gq_specs)]
tr_r_gq_10[is.na(ethnicity),
           ("tr_r_eh_type_prob"):=list(tr_r_count/tr_eh_remain_type_count),
           by = .(tract,gq_type)]
#prob only need the ones by spec!
paste0("The summary for probability by specifics is: ")
summary(tr_r_gq_10$tr_r_eh_spec_prob)
paste0("The summary for probability by type is: ")
summary(tr_r_gq_10$tr_r_eh_type_prob)
paste0("There remain ",nrow(tr_e_gq_10[re_code=="H"])-(nrow(tr_r_gq_10[re_code=="A"])-nrow(tr_e_gq_10[re_code=="I"]))," non-White Hispanic or Latino individuals that have not been assigned a matching race")  
nrow(tr_e_gq_10[is.na(matched)])==nrow(tr_e_gq_10[re_code=="H"])-
  (nrow(tr_r_gq_10[re_code=="A"])-nrow(tr_e_gq_10[re_code=="I"]))
```

Once we have expanded each matching category of guest quarter type and specifics to have the count for that category of possible Hispanic or Latino, we know by adjunction that the ones without any matching categories across guest quarter specifics for that tract must not be Hispanic or Latino. 

```{r add not Hispanic or Latino to the non-matched categories by }
paste0("There were ",nrow(tr_e_gq_10[is.na(matched)])," who were not matched in the ethnicity tables (i.e., non-White Hispanic or Latino)")
paste0("There were ",nrow(tr_r_gq_10[!is.na(tr_eh_remain_count)])," who could have matched by gq_specs and ",
       nrow(tr_r_gq_10[!is.na(tr_eh_remain_type_count)])," who could have matched by gq_type.")
paste0("Assigning Not Hispanic or Latino to ",nrow(tr_r_gq_10)-nrow(tr_r_gq_10[re_code=="A"|!is.na(tr_eh_remain_type_count)])," rows.")
tr_r_gq_10[is.na(tr_eh_remain_type_count)&is.na(ethnicity),
                  ("ethnicity"):=fcase(re_code=="B","J",
                                       re_code=="C","K",
                                       re_code=="D","L",
                                       re_code=="E","M",
                                       re_code=="F","N",
                                       re_code=="G","O",
                                       default = "def")]
nrow(tr_r_gq_10[ethnicity=="def"])==0
nrow(tr_r_gq_10[!is.na(ethnicity)])+
  nrow(tr_r_gq_10[!is.na(tr_eh_remain_type_count)])==
  nrow(tr_r_gq_10)
```

We can't fully match with the ethnicity tables because "H" is only listed for the Whites who identify as Hispanic or Latino, and the others are spread out by race in a way that we don't yet know. We were able to identify quite a few that could not match, however, and to assign them their ethnicity as not Hispanic or Latino definitively in the last step.
We have an interesting table which has the group quarters by the same variables, but only for people over 17 (over 95% are) and without the final specifics column. Let's try our same trick with joining the ethnicity and race tables.

```{r attempt matching with sex race and ethnicity over 17 for group quarters}
#tr_r_gq_over17_10[,("tr_r_count"):=.N,by=c("tract","re_code","sex","gq_type")]
tr_r_gq_over17_10[re_code=="A",("tr_r_gq_match_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_e_gq_over17_10[re_code=="I",("tr_r_gq_match_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_r_gq_over17_10[re_code=="A",("ethnicity"):=
                    tr_e_gq_over17_10[.SD, 
                                      list(re_code), 
                                      on = .(tr_r_gq_match_id)]]
tr_e_gq_over17_10[re_code=="I",c("matched"):=
                    tr_r_gq_over17_10[.SD, list(re_code), on = .(tr_r_gq_match_id)]]
#now do ones who are re_code=="A" for race and "H" for ethnicity.
tr_r_gq_over17_10[re_code=="A"&is.na(ethnicity),("tr_r_gq_match1_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_e_gq_over17_10[re_code=="H",("tr_r_gq_match1_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_r_gq_over17_10[re_code=="A"&is.na(ethnicity),("ethnicity"):=
                    tr_e_gq_over17_10[.SD, list(re_code), on = .(tr_r_gq_match1_id)]]
tr_e_gq_over17_10[re_code=="H",c("matched"):=
                    tr_r_gq_over17_10[.SD, list(re_code), on = .(tr_r_gq_match1_id)]]
#those count as ethnicity known
table(tr_r_gq_over17_10$re_code,tr_r_gq_over17_10$ethnicity)
test <- table(
  tr_r_gq_over17_10[re_code=="A",tract],
  tr_r_gq_over17_10[re_code=="A",sex],
  tr_r_gq_over17_10[re_code=="A",re_code],
  tr_r_gq_over17_10[re_code=="A",ethnicity]
)==table(
  bgSAR_10[re_code=="A"&household=="In group quarters"&first_age>17,tract],
  bgSAR_10[re_code=="A"&household=="In group quarters"&first_age>17,sex],
  bgSAR_10[re_code=="A"&household=="In group quarters"&first_age>17,re_code],
  bgSAR_10[re_code=="A"&household=="In group quarters"&first_age>17,ethnicity]
)
length(test[test==FALSE])==0
#now do rest with only what we can know - i.e., count of hispanic remaining by gq_type
tr_r_gq_over17_10[is.na(ethnicity),("tr_r_gq_match2_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_e_gq_over17_10[is.na(matched),("tr_r_gq_match2_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_e_gq_over17_10[is.na(matched),("tr_eh_remain_type_count"):=.N,
           by=.(tract,sex,gq_type)]
tr_r_gq_over17_10[is.na(ethnicity),("tr_eh_remain_type_count"):=
                    tr_e_gq_over17_10[.SD, 
                                      list(as.integer(tr_eh_remain_type_count)), 
                                      on = .(tr_r_gq_match2_id)]]
paste0("There are ",
nrow(tr_r_gq_over17_10[!is.na(tr_eh_remain_type_count)]),
" who were assigned a category and could possibly still be Hispanic or Latino.")

tr_r_gq_over17_10[order(tr_eh_remain_type_count),#so no leading NAs
                  ("tr_eh_remain_type_count"):=
             .SD[1,list(as.integer(tr_eh_remain_type_count))],
           by = .(tract,re_code,gq_type)]

paste0("There are ",nrow(tr_r_gq_over17_10)-
(nrow(tr_r_gq_over17_10[!is.na(tr_eh_remain_type_count)])+
  nrow(tr_r_gq_over17_10[!is.na(ethnicity)])),
" people who do not match with any of the gq_types that are in the ethnicity tables, and so must not be Hispanic or Latino")
#add ethnicity to rows that can't be H by elimination on type remaining
tr_r_gq_over17_10[is.na(tr_eh_remain_type_count)&is.na(ethnicity),
                  ("ethnicity"):=fcase(re_code=="B","J",
                                       re_code=="C","K",
                                       re_code=="D","L",
                                       re_code=="E","M",
                                       re_code=="F","N",
                                       re_code=="G","O",
                                       default = "def")]
nrow(tr_r_gq_over17_10[ethnicity=="def"])==0
nrow(tr_r_gq_over17_10[!is.na(ethnicity)])+
  nrow(tr_r_gq_over17_10[!is.na(tr_eh_remain_type_count)])==
  nrow(tr_r_gq_over17_10)

```

Now let's see what we can do to add the two versions of the race and ethnicity files together, and to see how it might point to a better final construction. What we've done is whittled away at the combinations that are known, with more specific categorizations that are within the constructions we were able to use for reduction automatically distributing within those cells without any information lost. So, for example, the information about sex can be distributed without issues within those cells where there are no non-determined individual rows -- when re_code=="A" (White), for example, but other categories within the tracts are also fully determined and so can also have sex distributed with confidence that it will match the original assignments by the Census Bureau. 


```{r match over 17 and full on all known categories}
tr_r_gq_10[,
           ("tr_r17_gq_match_id"):=
                    paste0(tract,re_code,ethnicity,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,re_code,ethnicity,gq_type)]
tr_r_gq_over17_10[,
                  ("tr_r17_gq_match_id"):=
                    paste0(tract,re_code,ethnicity,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,re_code,ethnicity,gq_type)]
tr_r_gq_10[,("sex"):=
                    tr_r_gq_over17_10[.SD,list(sex), 
                     on = .(tr_r17_gq_match_id)]]
tr_r_gq_over17_10[,("matched17"):=
                    tr_r_gq_10[.SD,list(tract), 
                     on = .(tr_r17_gq_match_id)]]
#number successfully matched
paste0("There are ",nrow(tr_r_gq_over17_10)," rows over 17 and with sex given, ",
nrow(tr_r_gq_10[!is.na(sex)]), " in the total table now have sex assigned by matching with the over 17 table, which leaves ",
nrow(tr_r_gq_10[is.na(sex)]), " who do not have sex assigned in the all age table of ",
nrow(tr_r_gq_10))
paste0("From the ethnicity matching, above, we still have ",
nrow(tr_r_gq_10[is.na(ethnicity)]), " who don't have ethnicity assigned and ",
 nrow(tr_r_gq_10[is.na(sex)&is.na(ethnicity)]),
      " that do not have ethnicity matched and did not get sex (or over 18) assigned")
#number that had ethnicity and race, but didn't match - those should get a count
nrow(tr_r_gq_10[!is.na(sex)&!is.na(ethnicity)])
#number in over 17 that had ethnicity but didn't match
paste0("Of the ",nrow(tr_r_gq_over17_10[!is.na(ethnicity)]) ,
       " in the over 17 table with ethnicity assigned, all but ",
nrow(tr_r_gq_over17_10[!is.na(matched17)&is.na(ethnicity)]), " matched. However, ",
nrow(tr_r_gq_10[is.na(sex)]), " still need a final determination for sex in the overall group quarters race table, in order to match with the givens in bggq")
```

Let's try a naive merge with the bgSAR_10 for re_code=="A" and age over 17, since it's what we know most about in both.
```{r check for match on group quarters to bgSAR directly}
nrow(tr_r_gq_10[re_code=="A"])
nrow(tr_r_gq_10[re_code=="A"])==nrow(bgSAR_10[re_code=="A"])-nrow(bgSAR_10[re_code=="A"&household=="In household"])
test <- table(tr_r_gq_10[re_code=="A"&!is.na(sex),tract],
              tr_r_gq_10[re_code=="A"&!is.na(sex),sex],
              tr_r_gq_10[re_code=="A"&!is.na(sex),ethnicity] #same as tr_r_gq_over17
              )==
  table(bgSAR_10[re_code=="A"&household!="In household"&first_age>17,tract],
        bgSAR_10[re_code=="A"&household!="In household"&first_age>17,sex],
        bgSAR_10[re_code=="A"&household!="In household"&first_age>17,ethnicity])
length(test[test==FALSE])==0
```


The idea with tr_r_eh_spec_prob is that if a row by race is showing up in the gq_spec, but not in the same gq_spec by ethnicity, then we know something about how they are distributed. We expect a lot to be determined by the final cast by tract to bgSAR, with the blocks over-riding from here. 

```{r join over17 gq but those without ethnicity and ordered by the tr_r_eh_spec_prob}
tr_r_gq_10[is.na(sex)&order(tr_r_eh_spec_prob),
           ("tr_r17_gq_match_id"):=
                    paste0(tract,re_code,gq_type,as.character(100000+seq.int(1:.N))),
                  by=.(tract,re_code,gq_type)]
tr_r_gq_over17_10[is.na(matched17)&order(tr_eh_remain_type_count),
                  ("tr_r17_gq_match_id"):=
                    paste0(tract,re_code,gq_type,as.character(100000+seq.int(1:.N))),
                  by=.(tract,re_code,gq_type)]
tr_r_gq_10[is.na(sex),("sex"):=
                    tr_r_gq_over17_10[.SD,list(sex), 
                     on = .(tr_r17_gq_match_id)]]
tr_r_gq_over17_10[is.na(matched17),("matched17"):=
                    tr_r_gq_10[.SD,list(tract), 
                     on = .(tr_r17_gq_match_id)]]
#number successfully matched
paste0("There are ",nrow(tr_r_gq_over17_10)," rows over 17 and with sex given, ",
nrow(tr_r_gq_10[!is.na(sex)]), " in the total table now have sex assigned by matching with the over 17 table, which leaves ",
nrow(tr_r_gq_10[is.na(sex)]), " who do not have sex assigned in the all age table of ",
nrow(tr_r_gq_10))
paste0("From the ethnicity matching, above, we still have ",
nrow(tr_r_gq_10[is.na(ethnicity)]), " who don't have ethnicity assigned and ",
 nrow(tr_r_gq_10[is.na(sex)&is.na(ethnicity)]),
      " that do not have ethnicity matched and did not get sex (or over 18) assigned")
#number that had ethnicity and race, but didn't match - those should get a count
paste0("There are ",
nrow(tr_r_gq_10[!is.na(sex)&!is.na(ethnicity)]),
" in the group quarters race file who have race and ethnicity assigned to them")
#number in over 17 that had ethnicity but didn't match
paste0("Of the ",nrow(tr_r_gq_over17_10[!is.na(ethnicity)]) ,
       " in the over 17 table with ethnicity assigned, all but ",
nrow(tr_r_gq_over17_10[!is.na(matched17)&is.na(ethnicity)]), " matched. However, ",
nrow(tr_r_gq_10[is.na(sex)]), " still need a final determination for sex in the overall group quarters race table, in order to match with the givens in bggq.")
```

All we have done up to this point is add as much information as we can deduce from the over 17 group quarters table to the complete group table, knowing that there is not specificity in the age groups. We have given a best guess for sex and age group to the tables where we know all the race and ethnicity designations. 

Our next step is to see if using the data on sex, age range and group quarters types and specifics, by tract, we can further eliminate some of the categories we might assign, in preparation for then using it with the full population and household tables to finally assign all the designations, as narrowed down by matching with the overall population tables. 

```{r matching under 18 by sex, type, and gq_specs on group quarter tables}
tr_r_gq_10[,("tr_sts_gq_match_id"):=
                    paste0(tract,sex,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type,gq_specs)]
bggq_10[age_range!="Under 18 years",("tr_sts_gq_match_id"):=
                    paste0(tract,sex,gq_type,tr_gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type,tr_gq_specs)]
bggq_10[age_range!="Under 18 years",c("sts_re_code","sts_ethnicity"):=
                    tr_r_gq_10[.SD,c(list(re_code),list(ethnicity)), 
                     on = .(tr_sts_gq_match_id)]]
tr_r_gq_10[,("matched_sts"):=
                    bggq_10[.SD,list(tract), 
                     on = .(tr_sts_gq_match_id)]]
#how many matches - put in right paste0 comments, etc.
nrow(tr_r_gq_10[!is.na(matched_sts)])
nrow(tr_r_gq_10[!is.na(matched_sts)])==nrow(bggq_10[!is.na(sts_re_code)])
nrow(tr_r_gq_10[!is.na(matched_sts)])==nrow(bggq_10[!is.na(sts_ethnicity)]) #FALSE b/c a few are NAs
```

There are 1,597 that didn't match by sex at the specifics level. Because the sex assigned to the race tables had come originally from a place where only group quarters type was certain, but we want to not lose any of the group quarter specifics by race and ethnicity, we match those remaining ones without sex. The designation for sex in the bggq table is definitive, after all, and will be respected as the final designation. 

```{r matching without sex for under 18 by gq_type and gq_specs for group quarter tables}
tr_r_gq_10[is.na(matched_sts),("tr_sts_gq_match2_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
bggq_10[age_range!="Under 18 years"&is.na(sts_re_code),("tr_sts_gq_match2_id"):=
                    paste0(tract,gq_type,tr_gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,tr_gq_specs)]
bggq_10[age_range!="Under 18 years"&is.na(sts_re_code),c("sts_re_code","sts_ethnicity"):=
                    tr_r_gq_10[.SD,c(list(re_code),list(ethnicity)), 
                     on = .(tr_sts_gq_match2_id)]]
tr_r_gq_10[is.na(matched_sts),("matched_sts"):=
                    bggq_10[.SD,list(tract), 
                     on = .(tr_sts_gq_match2_id)]]
#how many matches
nrow(tr_r_gq_10[!is.na(matched_sts)])
nrow(tr_r_gq_10[!is.na(matched_sts)])==nrow(bggq_10[!is.na(sts_re_code)])
nrow(tr_r_gq_10[!is.na(matched_sts)])==nrow(bggq_10[!is.na(sts_ethnicity)]) #FALSE b/c a few are NAs
```

We now have a reasonably strong match on race and ethnicity for the group quarters at the tract level, for those over 17. We simply don't have more information for those under 18, so match race and ethnicity with group quarters type and group quarters specifics, which we can then match back to the overall block group tables.
```{r matching without sex for under 18 by gq_type and gq_specs for group quarter tables}
tr_r_gq_10[is.na(matched_sts),("tr_sts_gq_match3_id"):=
                    paste0(tract,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,gq_specs)]
bggq_10[age_range=="Under 18 years"&is.na(sts_re_code),("tr_sts_gq_match3_id"):=
                    paste0(tract,gq_type,tr_gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,gq_type,tr_gq_specs)]
bggq_10[age_range=="Under 18 years"&is.na(sts_re_code),c("sts_re_code","sts_ethnicity"):=
                    tr_r_gq_10[.SD,c(list(re_code),list(ethnicity)), 
                     on = .(tr_sts_gq_match3_id)]]
tr_r_gq_10[is.na(matched_sts),("matched_sts"):=
                    bggq_10[.SD,list(tract), 
                     on = .(tr_sts_gq_match3_id)]]
#how many matches
nrow(tr_r_gq_10[!is.na(matched_sts)])
nrow(tr_r_gq_10[!is.na(matched_sts)])==nrow(bggq_10[!is.na(sts_re_code)])
nrow(tr_r_gq_10[!is.na(matched_sts)])==nrow(bggq_10[!is.na(sts_ethnicity)]) #FALSE b/c a few are NAs
```

#Discussion of some nudges and what they mean

We should also talk, at some point, about the temptation to fidget with the numbers. When do we bring in outside knowledge to give detail to the picture? If there are a handful of 17 year olds - and perhaps one or two exceptional students at 16 - in on-campus housing, that's not inconceivable, but we don't want them to have ages between 1 and 18 distributed to them on a normal distribution of some sort. Even inside the 18 to 64 years group, we know from experience that on-campus housing is overwhelmingly young adults, with many dorms having significantly more in their first and second years of school, as the older students move off-campus. Equally, the handful of 18 year olds who are in correctional facilities intended for juveniles should not be assigned randomly simply any age between 18 and 64, and the youngest ones should still be at least in their teens. Let's look at a few of those distributions, and then think about whether our process of elimination helps capture those differences - or what else we should bring to bear on the process later. If elimination works, then the idea is that the number of 17 year olds who live in a block group that has the college dorms in it will simply be picked up in the last merge with the block group by the 23 age groups available at that level (single year is at the tract level; in this example, "15 to 17 years" would be the relevant distinction at the block group level). Of course, if that block group also includes a nearby subdivision, we simply cannot know if the 15 to 17 year old is on campus or living with their parents without more information. There are things in the family and household files that may help, but we have to be carefult. Here are some obvious places where the age groups are potentially going to cause problems.

```{r difficult cases on age in group quarters}
paste0("Group homes intended for adults")
table(bggq_10[tr_gq_specs=="Group homes intended for adults (801)",age_range])
paste0("Residential treatment centers for adults")
table(bggq_10[tr_gq_specs=="Residential treatment centers for adults (802)",age_range])
paste0("Correctional facilities intended for juveniles")
table(bggq_10[tr_gq_specs=="Correctional facilities intended for juveniles (203)",age_range])
paste0("College/University student housing")
table(bggq_10[tr_gq_specs=="College/University student housing",age_range])
paste0("Residential treatment centers for adults")
table(bggq_10[tr_gq_specs=="Residential treatment centers for adults (802)",age_range])
paste0("Local jails and other municipal confinement facilities")
table(bggq_10[tr_gq_specs=="Local jails and other municipal confinement facilities (104)",age_range])
```

Group quarters are not concentrated, with most block groups having only one kind of residence, even as specified at the detailed level, which means that further specification by age isn't necessary. As part of being as complete as possible, we add a bit of a step to give preference to some things known outside the data tables themselves, as a matter of interpretation. Although the actual distribution still decides the structure of the final distribution, it is possible for some nudge toward one or the other subtype to be given with this technique.

```{r how many tracts have multiple gq_specs}
bggq_10[,("num_gq_specs_tr"):=length(unique(tr_gq_specs)),by=.(tract)]
summary(bggq_10[,num_gq_specs_tr])
bggq_10[,("num_gq_specs_bg"):=length(unique(tr_gq_specs)),by=.(geoid)]
summary(bggq_10[,num_gq_specs_bg])
bggq_10[,("num_gq_type_bg"):=length(unique(gq_type)),by=.(geoid)]
summary(bggq_10[,num_gq_type_bg])
#can look at the ones with the most num_gq_specs_tr and then see how well they matched
```

The exercise of matching on the tables is useful because we found several places where we'd been careless - which gives us the opportunity to emphasize how much of the construction is dependent on details that are hard to automate. The necessity of creating a concrete representation, like the exercise of determining that a formula is decidable (or effectively computable), is a good heuristic, but does not provide its own answer. What we know here is that the Census Bureau constructed its tables from a single grounded representation of the population, and the published tables are all transformations of that underlying overall representation. We can do a few things to clean up the matches where the categories given above point to age ranges that would be significantly misleading if interpreted as equally belonging to the whole range. The small number of 17 year olds living on college campuses should not be confused with the 8 year olds living with their faculty parents next door, but there's no algorithmic determination we can use internally to exclude all possible edge cases.

For the particular case of group quarters, the question is whether there are two types of group quarters in a block group that would potentially have confusion between them based on age or other characteristics. 

Consequently, we will just do some broad assignments of distributions of age, and then order the matching algorithm so that the rows are most likely to follow something like that order. The actual distributions will rule the final assignments, and their structure is what we want to maintain throughout so that we can reproduce the same tables, as needed. 
For the guest quarter specifics (gq_specs), we create a count that incorporates some broad understanding of the categories. We were able to find other published tables of group quarter populations, at higher geographic levels but with 15-19 years as the breakoff, and confirm that the under 18 year olds in college dorms, for example, were not under 15. http://proximityone.com/group_quarters.htm. Our explicit presupposition is that differences at the block group level in housing opportunities for group quarters will constrain the distributions to the right population. We created a beginning age for the group quarters when we first made the tables, and will tweak it here for the ordering, with no very sophisticated process for normalizing distributions or intervening at the individual level.

```{r assigning order to gq_specs}
bggq_10[tr_gq_specs=="Correctional facilities intended for juveniles (203)"&
          age_range=="18 to 64 years",
        ("beg_age_gq"):=17] #does not set value of final age to 17, but orders this row for matching before the rest of the 18 to 64 year olds.
bggq_10[tr_gq_specs=="Group homes for juveniles (non-correctional) (201)"&
          age_range=="18 to 64 years",
        ("beg_age_gq"):=17]
bggq_10[tr_gq_specs=="Residential treatment centers for juveniles (non-correctional) (202)"&
          age_range=="18 to 64 years",
        ("beg_age_gq"):=17]
bggq_10[tr_gq_specs=="Local jails and other municipal confinement facilities (104)"&
          age_range=="Under 18 years",
        ("beg_age_gq"):=17]
bggq_10[tr_gq_specs=="Local jails and other municipal confinement facilities (104)"&
          age_range=="65 years and over",
        ("beg_age_gq"):=64]
bggq_10[tr_gq_specs=="Group homes intended for adults (801)"&
          age_range=="Under 18 years",
        ("beg_age_gq"):=17]
bggq_10[tr_gq_specs=="Workers' group living quarters and Job Corps centers (901)"&
          age_range=="Under 18 years",
        ("beg_age_gq"):=17]
bggq_10[tr_gq_specs=="Workers' group living quarters and Job Corps centers (901)"&
          age_range=="65 years and over",
        ("beg_age_gq"):=64]
bggq_10[tr_gq_specs=="Residential treatment centers for adults (802)"&
          age_range=="65 years and over",
        ("beg_age_gq"):=64]
bggq_10[tr_gq_specs=="College/University student housing"&
          age_range=="Under 18 years",
        ("beg_age_gq"):=17]
bggq_10[tr_gq_specs=="Federal detention centers (101)"&
          age_range=="65 years and over",
        ("beg_age_gq"):=64]
bggq_10[tr_gq_specs=="State prisons (103)"&
          age_range=="Under 18 years",
        ("beg_age_gq"):=17]
bggq_10[tr_gq_specs=="State prisons (103)"&
          age_range=="65 years and over",
        ("beg_age_gq"):=64]
bggq_10[tr_gq_specs=="Correctional residential facilities (105)"&
          age_range=="65 years and over",
        ("beg_age_gq"):=64]
```

We will talk a bit more, below, about how to understand what sort of difference these nudges made, but for right now we should concentrate on finishing the matches at the level that we can know.

#Finishing the bgSAR matches for group quarters and households
We remember that, above, we had only fully matched bgSAR on households for Whites, since we only had complete information for that race and ethnicity. Now we have some knowledge of the race and ethnicity of other individuals in group quarters and can make sure that we assign race and ethnicity with as much precision as possible at the block group level.

```{r testing bgSAR and trHHr and bggq}
test <- table(
  bgSAR_10[re_code=="A"&household=="In household",tract],
  bgSAR_10[re_code=="A"&household=="In household",sex],
  bgSAR_10[re_code=="A"&household=="In household",race],
  bgSAR_10[re_code=="A"&household=="In household",ethnicity],
  bgSAR_10[re_code=="A"&household=="In household",first_age]
)==table(
  trHHr_10[race=="A",tract],
  trHHr_10[race=="A",sex],
  trHHr_10[race=="A",race],
  trHHr_10[race=="A",ethnicity],
  trHHr_10[race=="A",first_age]
)
length(test[test==FALSE])==0
```

DO ALL HH TO BGSAR BY AGE AND RACE BY HOUSEHOLD FIRST - so not accidentally pulling from them
JUST LINE THEM UP AFTER A JOIN, SO THAT SEQ.INT WITHIN THE LARGER AGE GROUPS STARTS WITH THE GQ FOLKS - HAVE TO THINK ABOUT HOW THAT WORKS ON BLOCK GROUPS, THOUGH....

SEE IF YOU CAN DO THESE ONLY ONCE IF THE RE-CODE ISSUE IS FIXED

```{r matching group quarter tables to SAR by bg then tract for 17 y.o.}
bggq_10[,("household"):="In group quarters"]
bgSAR_10[first_age=="15"&re_code!="A",("sar_gq_match_id"):=
                    paste0(geoid,sex,re_code,ethnicity,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,re_code,ethnicity)]
bggq_10[beg_age_gq=="17"&sts_re_code!="A",("sar_gq_match_id"):=
                    paste0(geoid,sex,sts_re_code,sts_ethnicity,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,sts_re_code,sts_ethnicity)]
bgSAR_10[first_age=="15"&re_code!="A",
         c("household","institutionalized","group_quarter_type","group_quarter_specifics"):=
                    bggq_10[.SD,c(list(household),list(institutionalized),list(gq_type),list(tr_gq_specs)), 
                     on = .(sar_gq_match_id)]]
bggq_10[beg_age_gq=="17"&sts_re_code!="A",("SAR_match"):=
                    bgSAR_10[.SD,list(geoid), 
                     on = .(sar_gq_match_id)]]
nrow(bgSAR_10[first_age=="15"&re_code!="A"&!is.na(institutionalized)])==
  nrow(bggq_10[beg_age_gq=="17"&sts_re_code!="A"])
#false because some sts_ethnicity are missing - so do it again without ethnicity
bgSAR_10[first_age=="15"&re_code!="A"&is.na(institutionalized),
         ("sar_gq_match1_id"):=
                    paste0(geoid,sex,re_code,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,re_code)]
bggq_10[beg_age_gq=="17"&sts_re_code!="A"&is.na(SAR_match),
        ("sar_gq_match1_id"):=
                    paste0(geoid,sex,sts_re_code,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,sts_re_code)]
bgSAR_10[first_age=="15"&re_code!="A"&is.na(institutionalized),
         c("household","institutionalized","group_quarter_type","group_quarter_specifics"):=
                    bggq_10[.SD,c(list(household),list(institutionalized),list(gq_type),list(tr_gq_specs)), 
                     on = .(sar_gq_match1_id)]]
bggq_10[beg_age_gq=="17"&sts_re_code!="A"&is.na(SAR_match),
        ("SAR_match"):=
                    bgSAR_10[.SD,list(geoid), 
                     on = .(sar_gq_match1_id)]]
nrow(bgSAR_10[first_age=="15"&re_code!="A"&!is.na(institutionalized)])==
  nrow(bggq_10[beg_age_gq=="17"&sts_re_code!="A"])
#still false, so match by tract
bgSAR_10[first_age=="15"&re_code!="A"&is.na(institutionalized),
         ("sar_gq_match2_id"):=
                    paste0(tract,sex,re_code,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,re_code)]
bggq_10[beg_age_gq=="17"&sts_re_code!="A"&is.na(SAR_match),
        ("sar_gq_match2_id"):=
                    paste0(tract,sex,sts_re_code,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,sts_re_code)]
bgSAR_10[first_age=="15"&re_code!="A"&is.na(institutionalized),
         c("household","institutionalized","group_quarter_type","group_quarter_specifics"):=
                    bggq_10[.SD,c(list(household),list(institutionalized),list(gq_type),list(tr_gq_specs)), 
                     on = .(sar_gq_match2_id)]]
bggq_10[beg_age_gq=="17"&sts_re_code!="A"&is.na(SAR_match),
        ("SAR_match"):=
                    bgSAR_10[.SD,list(geoid), 
                     on = .(sar_gq_match2_id)]]
nrow(bgSAR_10[first_age=="15"&re_code!="A"&!is.na(institutionalized)])==
  nrow(bggq_10[beg_age_gq=="17"&sts_re_code!="A"])
#tried matching by tract and only got one more
#let's change first_age to 18, but keeping the SAR_match so overall numbers match - will be 13 off on age...
bgSAR_10[first_age=="18"&re_code!="A"&is.na(institutionalized),
         ("sar_gq_match3_id"):=
                    paste0(geoid,sex,re_code,ethnicity,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,re_code,ethnicity)]
bggq_10[beg_age_gq=="17"&sts_re_code!="A"&is.na(SAR_match),
        ("sar_gq_match3_id"):=
                    paste0(geoid,sex,sts_re_code,sts_ethnicity,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,sts_re_code,sts_ethnicity)]
bgSAR_10[first_age=="18"&re_code!="A"&is.na(institutionalized),
         c("household","institutionalized","group_quarter_type","group_quarter_specifics"):=
                    bggq_10[.SD,c(list(household),list(institutionalized),list(gq_type),list(tr_gq_specs)), 
                     on = .(sar_gq_match3_id)]]
bggq_10[beg_age_gq=="17"&sts_re_code!="A"&is.na(SAR_match),
        ("SAR_match"):=
                    bgSAR_10[.SD,list(geoid), 
                     on = .(sar_gq_match3_id)]]
nrow(bgSAR_10[!is.na(institutionalized)])==
  nrow(bggq_10[beg_age_gq=="17"&sts_re_code!="A"])

```


Let's compare the two ways of doing the matching and see what difference it makes for a couple of very specific places.

We know the area around our own campus well, and there are some group quarters for students mixed in with residential neighborhoods with a broad range of children. First, let's see if in that block group, the elimination through assignment of people listed as "In households" provides enough distinction. We find that everyone in that census block who is in group quarters is in student housing, and that no one under 15 is listed as such; however, we also find a number of older people. The totals for the populations match, and can be represented as commuting in terms of the relations between age, sex, race, and ethnicity. 

```{r looking at group quarters after households removed for age range}
nrow(bggq_10[geoid=="48_201_312000_2"&sts_re_code=="A"])
nrow(bggq_10[tract=="48201312000"&sts_re_code=="A"])
nrow(tr_r_gq_10[tract=="48201312000"&re_code=="A"])
nrow(bgSAR_10[geoid=="48_201_312000_2"&household=="In group quarters"])
nrow(bggq_10[tract=="48201312000"&sts_re_code=="A"])
nrow(bgSAR_10[tract=="48201312000"&household=="In group quarters"])
table(bggq_10[geoid=="48_201_312000_2",tr_gq_specs])
table(bgSAR_10[geoid=="48_201_312000_2"&household=="In group quarters",age_range])
table(bggq_10[tract=="48201312000",tr_gq_specs])
table(bgSAR_10[tract=="48201312000"&household=="In group quarters",age_range])
table(bggq_10[tract=="48201312000",institutionalized])
table(bgSAR_10[tract=="48201312000"&household=="In group quarters",age_range])
```

For this group, it doesn't seem like any further specification by age range helps - it's all done by virtue of the specific characteristics of the block group. Let's also look at one with something that feels like an odd distribution around the ages - that is, where we have a group home intended for adults, at the block group level, the totals don't work, so we'll look at it at the tract level, with a reminder for ourselves that we'll need to account for the block groups better later.

```{r looking at group quarters after households removed for age range}
table(bggq_10[geoid=="48_201_531700_1"&sts_re_code=="A",tr_gq_specs])
table(bggq_10[geoid=="48_201_531700_1"&sts_re_code=="A",age_range])
table(bgSAR_10[geoid=="48_201_531700_1"&household=="In group quarters"&re_code=="A",age_range])
#and by tract
table(bggq_10[tract=="48201531700"&sts_re_code=="A",tr_gq_specs])
table(bggq_10[tract=="48201531700"&sts_re_code=="A",age_range])
table(tr_sa_gq_10[tract=="48201531700",gq_specs])
table(tr_sa_gq_10[tract=="48201531700",age_range])
table(bgSAR_10[tract=="48201531700"&household=="In group quarters"&re_code=="A",age_range])
#and by type 
table(bggq_10[tract=="48201531700"&sts_re_code=="A",gq_type])
table(bggq_10[tract=="48201531700"&sts_re_code=="A",age_range])
table(tr_sa_gq_10[tract=="48201531700",gq_type])
table(tr_sa_gq_10[tract=="48201531700",age_range])
table(bgSAR_10[tract=="48201531700"&household=="In group quarters"&re_code=="A",age_range])
#and a different tract
table(bggq_10[tract=="48201312300"&sts_re_code=="A",tr_gq_specs])
table(bggq_10[tract=="48201312300"&sts_re_code=="A",age_range])
table(tr_sa_gq_10[tract=="48201312300",gq_specs])
table(tr_sa_gq_10[tract=="48201312300",age_range])
table(bgSAR_10[tract=="48201312300"&household=="In group quarters"&re_code=="A",age_range])
#and another
table(bggq_10[tract=="48201252500"&sts_re_code=="A",tr_gq_specs])
table(bggq_10[tract=="48201252500"&sts_re_code=="A",age_range])
table(tr_sa_gq_10[tract=="48201252500",gq_specs])
table(tr_sa_gq_10[tract=="48201252500",age_range])
table(bgSAR_10[tract=="48201252500"&household=="In group quarters"&re_code=="A",age_range])
#and another
table(bggq_10[tract=="48201320200"&sts_re_code=="A",tr_gq_specs])
table(bggq_10[tract=="48201320200"&sts_re_code=="A",age_range])
table(tr_sa_gq_10[tract=="48201320200",gq_specs])
table(tr_sa_gq_10[tract=="48201320200",age_range])
table(bgSAR_10[tract=="48201320200"&household=="In group quarters"&re_code=="A",age_range])
```

This tells us that there is some mismatch between the block group and the tract level, but that otherwise things match up as we had hoped. What we can do is create a secondary table where we join at the tract level, and then redo the block group merge. We could also just do this on the tract level, and then do the block group merge only at the very end. 















SHOULD WE DO RE_CODE=="A" FIRST????
THEN JUST HAVE TO DO ORDER(LOWER TO HIGHER ON COMBO) by at smallest level (re_code)

The first part is to use the broad age range by sex and group quarter specifications. Then we'll do two things to the other rows that could have matched, with the plan to integrate the information from multiple tables based on that representation. 

```{r test some tables for possible join}
tr_r_gq_over17_10[,("tr_sa_count"):=.N,by=c("tract","re_code","sex","gq_type")]
tr_r_gq_10[,("tr_rr_gq_match_id"):=
                    paste0(tract,re_code,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,re_code,gq_type)]
tr_r_gq_over17_10[,("tr_rr_gq_match_id"):=
                    paste0(tract,re_code,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,re_code,gq_type)]
tr_r_gq_10[,c("sex_tr17_r","tr_e_sa_gq","tr_e_sa_gq_count"):=
                    tr_r_gq_over17_10[.SD, 
                                      c(list(sex),list(tr_e_gq),list(as.integer(tr_sa_count))), 
                                      on = .(tr_rr_gq_match_id)]]
tr_r_gq_10[!is.na(tr_e_sa_gq),("age_over17"):="first pass match"]
nrow(tr_r_gq_10[!is.na(age_over17)])
nrow(tr_r_gq_10[!is.na(sex_tr17_r)])
nrow(tr_r_gq_10[!is.na(tr_e_sa_gq)])
##tests
test <- table(
  tr_r_gq_10[!is.na(tr_e_sa_gq),tract],
  tr_r_gq_10[!is.na(tr_e_sa_gq),re_code],
  tr_r_gq_10[!is.na(tr_e_sa_gq),gq_type]
)==table(
  tr_r_gq_over17_10[!is.na(tr_e_gq),tract],
  tr_r_gq_over17_10[!is.na(tr_e_gq),re_code],
  tr_r_gq_over17_10[!is.na(tr_e_gq),gq_type]
)
length(test[test==FALSE])==0
#then add something to the blanks so we know what they might have been
#could put something in here as part of the testing, to show amount of difference from join??
test <- table(tr_r_gq_10$tr_e_sa_gq) 
tr_r_gq_10[,c("sex_tr17_r","tr_e_sa_gq","tr_e_sa_gq_count","tr_re_count"):=
             .SD[1,c(list(sex_tr17_r),list(tr_e_sa_gq),list(as.integer(tr_e_sa_gq_count)),.N)],
           by = .(tract,re_code,gq_type)]
table(tr_r_gq_10$tr_e_sa_gq)-test

```

Need a general discussion of why we're still treating this as a descriptive problem with zeroing in on what can be known and not propagating growing p-values as marks of uncertainty - i.e., that the problem we have is how can we move between different ways of talking, given that we know that there is a commutative level where the individuals have a way in which they are all described consistently and coherently. We are not asking about whether the sample we have has the same structure as the whole - at least not yet - but whether the structures for talking about the population can maintain their commutativity (whether, more broadly and as we will exploit later, they are isomorphic, and thus can be said to translate back and forth between the various "ways of representing"). This should both let us build the representation and teach us something about the broader framings within which such things are arbitrated, as we assess potential policy, understand residual bias or other systemic weaknesses in approach to the design, and look for ways to represent what is dynamic and changing in a population, as opposed to merely consistently describing that which is. [rules and patterns?]
The idea is that the counts give you the probability, but let's think about what it means to do them "with memory" - T-D Bradley's density. By that metaphor, what we're doing at this stage is zeroing in on the core. We'll confirm the ones that are determined twice in the same way, and then save enough information on each row to know it's probability, with "memory." The age_range designation let's you see where a first pass match would happen, too. 

```{r reducing by eliminating non-matches on race and ethnicity in race file}
#not sure what to do with sex and age?? do ethnicity first
paste0("The distribution of the full ethnicity file is in tr_e_gq: ")
table(tr_r_gq_10$tr_e_gq,tr_r_gq_10$re_code)
paste0("The distribution of the over 17 ethnicity file is in tr_e_sa_gq and includes sex: ")
table(tr_r_gq_10$tr_e_sa_gq,tr_r_gq_10$re_code,tr_r_gq_10$sex_tr17_r)
paste0("Where they don't match: ")
table(tr_r_gq_10$tr_e_gq,tr_r_gq_10$tr_e_sa_gq)
paste0("Where they don't match on first pass: ")
table(tr_r_gq_10[!is.na(age_over17),tr_e_gq],tr_r_gq_10[!is.na(age_over17),tr_e_sa_gq])
```

Then do add to hh - and resolve with the bgSAR??

MOVE THE COUNTS FROM HR AND GQ to bgSAR, then do some reductions... so everyone that has a match (and there will be a lot more) has a "may_match_gq_sa" with the number from the gq side?

CAN DO THE SAME FOR HOUSEHOLDS AND GROUP QUARTERS, WITH THEM STACKED IN TERMS OF GRANULARITY - HOW MANY FIT IN THE .N FROM THE OTHERS

IF .N = ONE OF THE COUNTS, 

tract   sex   end_age by 3  race  ethnicity  - something about that adds potential for the matching on specs...

geoid   sex   end_age by 3

have them ordered by more specific to more general, then fill up as they know - whatever the smallest number is?
fcase(.N<bg_sa_count,fcase(.N<bg_se_count))





```{r test for completeness and symmetry with households and full population}
test <- table(
  tr_r_gq_over17_10[,tract],
  tr_r_gq_over17_10[,sex],
  tr_r_gq_over17_10[,institutionalized],
  tr_r_gq_over17_10[,gq_type]
)==table(
  bggq_10[age_range!="Under 18 years",tract],
  bggq_10[age_range!="Under 18 years",sex],
  bggq_10[age_range!="Under 18 years",institutionalized],
  bggq_10[age_range!="Under 18 years",gq_type]
)
length(test[test==FALSE])==0
```


OR: COULD JUST DO THE TRIANGLES AGAIN, AND THINK ABOUT WHAT IT MEANS TO ALLOW THE RACE, ETC., TO CHANGE WITH MORE INFORMATION AS IT GOES AROUND???

Let's create an indexed count by cell, given what we know, and then do a reduction to the smallest counts, then uncount

Or maybe: the total by cell from the right should be divided by total by cell from the left? - that loses information about the relative differences
If you order them by the left adjoint, 

```{r testing lists}
tr_sa_gq_10[, #have to see if it's following an internal order - explicitly doing it doesn't seem to change it... 
            ("gq_spec_list_count_sa"):=
              paste0(.N,"!_!",tract,"!_!",sex,"!_!",age_range,"!_!",institutionalized,"!_!",gq_type,"!_!",gq_specs),
            by=c("tract","sex","age_range","institutionalized","gq_type","gq_specs")]
tr_sa_gq_10[333,strsplit(gq_spec_list_count_sa,"!_!")][4]=="Under 18 years"
tr_sa_gq_10[333,strsplit(gq_spec_list_count_sa,"!_!")][1]=="8"
max(as.numeric(tr_sa_gq_10[,strsplit(gq_spec_list_count_sa,"!_!")][1]))
tr_sa_gq_10[444:446,strsplit(gq_spec_list_count_sa,"!_!")][1:4]
```

```{r add gq_spec_count_sa to over 17 gq from tr_sa_gq}
tr_sa_gq_10[, #have to see if it's following an internal order - explicitly doing it doesn't seem to change it... 
            ("gq_spec_count_sa"):=.N,
            by=c("tract","sex","age_range","institutionalized","gq_type","gq_specs")]
tr_r_gq_over17_10[,("bg_sa_gq_17_match_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_sa_gq_10[age_range!="Under 18 years",("bg_sa_gq_17_match_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_r_gq_over17_10[,("gq_spec_count_sa"):=
                    tr_sa_gq_10[.SD, list(gq_spec_count_sa), on = .(bg_sa_gq_17_match_id)]]
#tests
nrow(tr_r_gq_over17_10[is.na(gq_spec_count_sa)])==0
test <- summary(tr_sa_gq_10[age_range!="Under 18 years",gq_spec_count_sa])==
  summary(tr_r_gq_over17_10[,gq_spec_count_sa])
length(test[test==FALSE])==0
```

If you have sa spec count, and you assign every row before .N to the value from the sa, and after to "no_match", 


```{r add gq_spec_count_r and gq_spec_count_e to over 17 gq from tr_r_gq and tr_e_gq}
tr_r_gq_10[order(tract,race,institutionalized,gq_type,gq_specs), #test to see what's needed
            ("gq_spec_count_r"):=.N,
            by=c("tract","race","institutionalized","gq_type","gq_specs")]
tr_r_gq_over17_10[,("bg_r_gq_17_match_id"):=
                    paste0(tract,race,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,race,gq_type)]
tr_r_gq_10[age_range!="Under 18 years",("bg_r_gq_17_match_id"):=
                    paste0(tract,race,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,race,gq_type)]
tr_r_gq_over17_10[,("gq_spec_count_r"):=
                    tr_r_gq_10[.SD, list(gq_spec_count_r), on = .(bg_r_gq_17_match_id)]]
#tests
nrow(tr_r_gq_over17_10[is.na(gq_spec_count_r)])==0
test <- summary(tr_r_gq_10[age_range!="Under 18 years",gq_spec_count_r])==
  summary(tr_r_gq_over17_10[,gq_spec_count_r])
length(test[test==FALSE])==0
```

Once it has both spec totals, how do we multiply possible combinations and assign?

If we take the one that already has sex and race and order it that way, and int.seq up to ...



REDOING!!!

#also do a spec count from tr_r to bggq; then the specifics can be used to ground the triangles

```{r add gq_spec_count_r to bggq from tr_r_gq}
tr_r_gq_10[,("gq_spec_count_r"):=.N,by=c("tract","race","institutionalized","gq_type","gq_specs")]
bggq_10[,("bg_r_gq_match_id"):=
                    paste0(tract,race,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,race,gq_type)]
tr_r_gq_10[,("bg_r_gq_match_id"):=
                    paste0(tract,sex,age_range,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range,gq_type)]
bggq_10[,("gq_spec_count_r"):=
                    tr_r_gq_10[.SD, list(gq_spec_count_r), on = .(bg_r_gq_match_id)]]
#tests
nrow(bggq_10[is.na(gq_spec_count_r)])==0
test <- summary(bggq_10[,gq_spec_count_r])==
  summary(tr_sa_gq_10[,gq_spec_count_r])
length(test[test==FALSE])==0
```
#then will just have 

Joining PCT20 and PCT22, since they have the same race, ethnicity, and group quarters type. PCT22 is for age over 17 only, but the under 18 group is relatively small (1,927, which is less than 5%). After doing the joins on race, we'll be able to replicate a version by ethnicity, which will help determine the main table's missing ethnicity values.

Let's quickly ensure that the variables we want to match on are written the same way for each table. (We had to fix this, and did so silently, above, because of some idiosyncratic capitalization)
```{r unique on variable tags for tr_r_gq and over17}
#test to ensure matching variables match
test <- unique(tr_r_gq_10[order(institutionalized),institutionalized])==
  unique(tr_r_gq_over17_10[order(institutionalized),institutionalized])
length(test[test==FALSE])==0
test <- unique(tr_r_gq_10[order(gq_type),gq_type])==
  unique(tr_r_gq_over17_10[order(gq_type),gq_type])
length(test[test==FALSE])==0
```
Have to add group quarter specifics to the over 17. 

```{r add gq_spec_count to bggq from tr_sa_gq}
tr_r_gq_over17_10[,("bg_sa_gq_17_match_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
bggq_10[age_range!="Under 18 years",("bg_sa_gq_17_match_id"):=
                    paste0(tract,sex,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type)]
tr_r_gq_over17_10[,("gq_specs"):=
                    bggq_10[.SD, list(gq_specs), on = .(bg_sa_gq_17_match_id)]]
#tests
nrow(tr_r_gq_over17_10[is.na(gq_specs)])==0
test <- table(
  tr_r_gq_over17_10[,tract],
  tr_r_gq_over17_10[,sex],
  tr_r_gq_over17_10[,institutionalized],
  tr_r_gq_over17_10[,gq_type],
  tr_r_gq_over17_10[,gq_specs]
)==table(
  bggq_10[age_range!="Under 18 years",tract],
  bggq_10[age_range!="Under 18 years",sex],
  bggq_10[age_range!="Under 18 years",institutionalized],
  bggq_10[age_range!="Under 18 years",gq_type],
  bggq_10[age_range!="Under 18 years",tr_gq_specs]
)
length(test[test==FALSE])==0
```
START HERE!! - Problem is that this gets by race, and we had moved it earlier without race!!

Now that the matching variables are typed out correctly for the match - may just get rid of _count
```{r add sex and age_range (by opp to Under 18) to tr_r_gq_10 from tr_r_gq_over17_10}
#tr_r_gq_over17_10[,("gq_s_a_type_count"):=.N,by=c("tract","re_code","institutionalized","gq_type")]
tr_r_gq_over17_10[,("tr_r_t_gq_match_id"):=
                    paste0(tract,re_code,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,re_code,gq_specs)]
tr_r_gq_10[,("tr_r_t_gq_match_id"):=
                    paste0(tract,re_code,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,re_code,gq_specs)]
tr_r_gq_10[,c("sex"):=
                    tr_r_gq_over17_10[.SD, c(list(sex)), on = .(tr_r_t_gq_match_id)]]
tr_r_gq_10[is.na(sex),("age_range"):=.("Under 18 years")]
#tests
nrow(tr_r_gq_10[!is.na(sex)])==nrow(tr_r_gq_over17_10)
```
Determine sex and age range by matching on the tract level by group quarters type (tr_r_gq from bggq) and specifics (from bggq, put there from the sex and age range tables). There is some chance that the not under 18 age groups will not match by race, but no more detailed determinations are available for these subgroups. There will be some refinement as more specific ages are generated, below. 
[[not using the counts, yet]] 

```{r determine sex and age range on tr_r_gq}
#now we'll create matching ids with a conditional clause for age_range
#remember gq_s_a_type_count was by=c("tract","re_code","institutionalized","gq_type") 
tr_r_gq_10[is.na(age_range),("trgqsa_match_id"):=
             paste0(tract,institutionalized,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,institutionalized,gq_type,gq_specs)]
bggq_10[age_range!="Under 18 years",("trgqsa_match_id"):=
                    paste0(tract,institutionalized,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,institutionalized,gq_type,gq_specs)]
tr_r_gq_10[is.na(age_range),c("sex","age_range"):=
                    bggq_10[.SD, c(list(sex),list(age_range)), on = .(trgqsa_match_id)]]
#that should make all the age_range complete on tr_r, with the Under18 not having sex assigned.
nrow(tr_r_gq_10[is.na(sex)])==nrow(bggq_10[age_range=="Under 18 years"])
#so just add the last 1,927 sex for under18
tr_r_gq_10[is.na(sex),("trgqs_match_id"):=
             paste0(tract,age_range,institutionalized,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,age_range,institutionalized,gq_type,gq_specs)]
bggq_10[age_range=="Under 18 years",("trgqs_match_id"):=
                    paste0(tract,age_range,institutionalized,gq_type,gq_specs,as.character(100000+sample(1:.N))),
                  by=.(tract,age_range,institutionalized,gq_type,gq_specs)]
tr_r_gq_10[is.na(sex),c("sex"):=
                    bggq_10[.SD, c(list(sex)), on = .(trgqs_match_id)]]
#tests
nrow(tr_r_gq_10[is.na(sex)])==0
test <- table(
  tr_r_gq_10$tract,
  tr_r_gq_10$sex,
  tr_r_gq_10$age_range,
  tr_r_gq_10$institutionalized,
  tr_r_gq_10$gq_type,
  tr_r_gq_10$gq_specs
)==table(
  bggq_10$tract,
  bggq_10$sex,
  bggq_10$age_range,
  bggq_10$institutionalized,
  bggq_10$gq_type,
  bggq_10$gq_specs
)
length(test[test==FALSE])==0
```

Now let's put group quarter race on to bggq_10 from the tract level race group quarters. It's really just filling out the relations that have already been established - closing the commutative relations. The steps that add ethnicity are the last before joining and moving up to the next level with households and the entire population.

```{r determine group quarter specifics on bggq}
#now we'll create matching ids with a conditional clause for age_range
#remember gq_s_a_type_count was by=c("tract","re_code","institutionalized","gq_type") 
tr_r_gq_10[,("trgqr_match_id"):=
             paste0(tract,institutionalized,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,institutionalized,gq_type)]
bggq_10[,("trgqr_match_id"):=
                    paste0(tract,institutionalized,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,institutionalized,gq_type)]
bggq_10[,c("sex","age_range"):=
                    tr_r_gq_10[.SD, c(list(sex),list(age_range)), on = .(trgqr_match_id)]]


#that should make all the age_range complete on tr_r, with the Under18 not having sex assigned.
nrow(tr_r_gq_10[is.na(sex)])==nrow(bggq_10[age_range=="Under 18 years"])
#so just add the last 1,927 sex for under18
tr_r_gq_10[is.na(sex),("trgqs_match_id"):=
             paste0(tract,age_range,institutionalized,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,age_range,institutionalized,gq_type)]
bggq_10[age_range=="Under 18 years",("trgqs_match_id"):=
                    paste0(tract,age_range,institutionalized,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,age_range,institutionalized,gq_type)]
tr_r_gq_10[is.na(sex),c("sex"):=
                    bggq_10[.SD, c(list(sex)), on = .(trgqs_match_id)]]
#tests
nrow(tr_r_gq_10[is.na(sex)])==0
test <- table(
  tr_r_gq_10$tract,
  tr_r_gq_10$sex,
  tr_r_gq_10$age_range,
  tr_r_gq_10$institutionalized,
  tr_r_gq_10$gq_type,
  tr_r_gq_10$gq_specs
)==table(
  bggq_10$tract,
  bggq_10$sex,
  bggq_10$age_range,
  bggq_10$institutionalized,
  bggq_10$gq_type,
  bggq_10$gq_specs
)
length(test[test==FALSE])==0
```
Do the ethnicity triangles for what had just been the race triangles??

Now we have all the possible specifications for adding race at the tract level to the block level - think about how that relates to the _initial???



```{r add gq_race_count to bggq from tr_r_gq}
tr_r_gq_10[,("gq_race_count_type"):=.N,by=c("tract","race","institutionalized","gq_type")]
tr_r_gq_10[,("gq_race_count_spec"):=.N,by=c("tract","race","institutionalized","gq_type","gq_specs")]
bggq_10[,("bg_r_gq_match_id"):=
                    paste0(tract,sex,age_range,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type,age_range)]
tr_sa_gq_10[,("bg_r_gq_match_id"):=
                    paste0(tract,sex,gq_type,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type,age_range)]
bggq_10[,c("gq_race_count_type","gq_race_count_spec"):=
                    tr_sa_gq_10[.SD, c(list(gq_race_count_type),list(gq_race_count_spec)), on = .(bg_r_gq_match_id)]]
#tests

```


```{r add gq_spec_count to bggq from tr_sa_gq}
tr_sa_gq_10[,("gq_spec_count"):=.N,by=c("tract","sex","age_range","institutionalized","gq_type","gq_specs")]
bggq_10[,("bg_sa_gq_match_id"):=
                    paste0(tract,sex,age_range,gq_type,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type,age_range)]
tr_sa_gq_10[,("bg_sa_gq_match_id"):=
                    paste0(tract,sex,gq_type,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,gq_type,age_range)]
bggq_10[,("gq_spec_count"):=
                    tr_sa_gq_10[.SD, list(gq_spec_count), on = .(bg_sa_gq_match_id)]]
#tests

```

THEN put the remains stuff on to the _initial table on tract and solve... the nested fcases...
THEN put the _initial back to bgSAR

Here, we'll use the conditional clauses to determine the dependent types. [[may be worth a longer exegesis]]

##Some of this may be recap from the ACS_probs Rmd - have to think about the structure of the overall argument and whether to try to recap here.
At this point, we have several options, all of which we can characterize in terms of the types of structures preserved by the different approaches. If we think of every way of talking about the representation as an object, and our task as being about how we understand the various constructions of representations, then we are being guided by the demand for an overall coherence in our representation and not by the connection between our representation and a set of objects at a particular point. There are philosophical reasons to justify that choice - after all, the connection between our representation and the set of objects would, in every particular case, also have to be a representation, so being guided by the coherence of the ensemble of representations gives you a more encompassing overall approach. That turn toward the coherence of the representation need not be a turn toward relativism, however, even though it deliberately celebrates the proliferation of modes of representation. Ordinary high school mathematics can be a guide here; as you transform an algebra problem, following rules for reshaping each side of the equation, you're dealing with the coherence of the representations and looking for a particular form - or mathematical object - that helps resolve the equations into something more useful. On the other side, one is tempted by the idea that each transformation should be tied to a determination or judgment about the connection of the world to the proposition or sets of propositions being made. 
In the latter case, for example, one would approach the reconstruction of census data with an eye toward maintaining throughout all the representations the statistical likelihood that any particular representation would be correctly reflected by the ground truth at that step. You would, in our particular conundrum, choose either race, ethnicity, age, sex, or geography (or some known combination of them) to set as a base and then add the other attributes step-wise while tracking the statistical uncertainty introduced. This could be relatively detailed, and derived from the same dataset:

```{r solving race eth statistically}
#create percentages of people in each tract who are there by age or by race, 
#or by race, ethnicity, and age - one can go down in the cells to a very detailed level
#code snippet

#create each individual by saying they have that percentage chance to be 
#in each of the boxes at that level.
#code snippet

#see the totals diverge in the way that leads us to the ACS unusability - 
#where the confidence interval makes some sense at the highest level, i.e., 
#at the tract by age_range, but stops being realistic as that range is 
#further spread out when projecting below that level into block_groups or 
#more specific ages
#code snippet
```

In our way of speaking, what that approach does is privilege the structure of the individual confronting an unknown distribution of objects and asking whether that distribution falls within an expected range. We are not questioning the legitimacy of that statistical question, but we are asking what it means to privilege other structures of coherence. Another way of thinking about the problem is to say that the statistical estimation is the place where the encoding of the functions is projected onto the place where an individual subject looks at some particular distribution and describes its embeddedness in another level of an individual subject looking at a distribution describing it - I know what percent by race live in the tract, so I can use that percentage to give an estimation at the block_group level. 

The problem with that set of presuppositions is that it sees the function of assigning characteristics to individuals as a projection of sets and then looks for the coherence in terms of comparing the distributions in the sets. One can make a judgment about whether the assigning function is effective or not, by looking at the outcomes as either being within acceptable distributions as determined by comparison with another set's output, but one is not taking advantage of the way the functions work as an ensemble of potential representations, allowing intermediate steps which conserve other elements of the relations, for example, before we get to the place where the statistical estimation is the right framework for judging efficacity. Treating the ensemble of mappings as a mathematical object is what it means, mathematically, to move from looking at the rules for producing a representation (for example, the arithmetic operations on a collection of objects) and asking whether the rules were correctly applied to looking at the way different rules for producing the same representation cohere and using the tools from the various mappings to move back and forth between operations until a more effective representation can be implemented (for example, solving a problem algebraically by working through the abstract possibilities of the combinations of variables until necessary structures for their possible combination become evident and can be employed to create a solution).  

We look to the household data to see if there's a way to help with understanding how to assign the Hispanic or Latino designation. What we will find is that there is a distribution of householders by race, a distribution of all the population in the households, and a distribution of all adults over 18 in the households, all by block_group. If we conserve the structures of relations between these designations by noting that there is a single distribution stored at the census that has each individual with all characteristics (within the range of problems they have with the initial collection), what we see are clues to the construction of each representation from the full set of characterized individuals to the particular subset produced in the tables. 

As we mentioned in passing, before, the function of subsetting is conceived differently for dependent type theory than for set theory. Does type theory let us think all the way to adjointness or is that the category theory? What I'm trying to take advantage of is triangle relations that are articulated at different levels and so aren't immediately commutative - but must happen within an overall space where an effective commutativity did exist (their representation, on their protected server, of the entire population). Could draw that in a diagram, but the point is that the commutativity is the first "algebraic" structure to maintain, as opposed to the representability of each produced representation in terms of its statistical relations to the whole (with its troubled attempt to guide its development by tracking the growing levels of uncertainty) - again, have to think about how much of this is already in ACS_probs.

We start with P28 because it captures the relation between race eth hhtype and hhsize. The others have more specificity to add to each.
We'll do this in the next in making_sam_avgs.Rmd (where we go through an illustrative example) and making_sam_hh.Rmd, for more effective computation, and save the block_group data, for use later.

```{r save bgSAR Harris csv}
#need to check if exists and replace, if needed
file_path <- valid_file_path(censusdir,vintage="2010",state,api_type="dec/sf1",block="block_group",groupname="bgSAR_Harris",path_suff="wrk.csv")
if(file.exists(file_path))
  {file.remove(file_path)}
write_csv(bgSAR_10,file_path)
```




Figure out how to insert, above:

If we add households to group quarters, we get the totals. 




The redistricting data collected in 2010 reports population estimates in yet a different format. It has one table that has races and ethnicity data for everyone over 18 and one that has it for everyone, with no ages attached. The race data gives more details on the composition of "Two or More Races" but nothing else new. The ethnicity file gives block level data for Hispanic or Latino who don't identify as White. Gives slightly more contour, but not much, and risks big differences on age and ethnicity or race per block group. (Although some contour is better than none!)

```{r pl data downloaded}
    eth_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                      groupname = "P2",county_num = county,
                                                                      block="block_group",api_type="dec/pl",path_suff="est.csv")
    race_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                groupname = "P1",county_num = county,
                                                                block="block_group",api_type="dec/pl",path_suff="est.csv")
    eth_over17_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                      groupname = "P4",county_num = county,
                                                                      block="block_group",api_type="dec/pl",path_suff="est.csv")
    race_over17_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                groupname = "P3",county_num = county,
                                                                block="block_group",api_type="dec/pl",path_suff="est.csv")
 
    
```



Just because of how limitations work on computer memory, let's save the modified SAR_10 data. 
```{r save SAR Harris csv}
#    file_path <- valid_file_path(censusdir,vintage="2010",state,api_type="dec/sf1",block="tract",groupname="SAR_Harris",path_suff="wrk.csv")
#    write_csv(SAR_10,file_path)
```

NOT USED???


```{r matching chunk example}

#and for Hispanic, will put a tmp_ethnicity, knowing that the bgSAR is more complete
trHHr_10[is.na(ethnicity),("trHHsae1_match_id"):=
                    paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHe_10[ethnicity=="H",("trHHsae1_match_id"):=
                    paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHr_10[is.na(ethnicity),("tmp_ethnicity"):=
                    trHHe_10[.SD, list(ethnicity), on = .(trHHsae1_match_id)]]
nrow(trHHr_10[!is.na(tmp_ethnicity)])==nrow(trHHe_10[ethnicity=="H"])
test <- table(trHHr_10[!is.na(tmp_ethnicity),tract],
              trHHr_10[!is.na(tmp_ethnicity),sex],
              trHHr_10[!is.na(tmp_ethnicity),age_range],
              trHHr_10[!is.na(tmp_ethnicity),tmp_ethnicity]
) == table(
  trHHe_10[ethnicity=="H",tract],
  trHHe_10[ethnicity=="H",sex],
  trHHe_10[ethnicity=="H",age_range],
  trHHe_10[ethnicity=="H",ethnicity]
)
length(test[test==FALSE])==0

```




```{r get structure from bgSAR_10 to add rows to trHHr_10}
#on this one, doing a left join is the fastest way to expand; it does a couple of unexpected things that we can deal with
bgSAR_initial_10 <- bgSAR_10[,c("tract","race","ethnicity","sex","age_range","first_age","last_age")]
trHHr_10 <- trHHr_10[bgSAR_initial_10, on=.(tract,race,ethnicity,sex,age_range,first_age,last_age)]
nrow(trHHr_10)==nrow(bgSAR_10)
test <- table(trHHr_10[,tract],
              trHHr_10[,race],
              trHHr_10[,sex],
              trHHr_10[,age_range]
) == table(
  bgSAR_10[,tract],
  bgSAR_10[,race],
  bgSAR_10[,sex],
  bgSAR_10[,age_range]
)
length(test[test==FALSE])==0
test <- table(trHHr_10[race=="A",tract],
              trHHr_10[race=="A",ethnicity],
              trHHr_10[race=="A",sex],
              trHHr_10[race=="A",age_range]
) == table(
  bgSAR_10[race=="A",tract],
  bgSAR_10[race=="A",ethnicity],
  bgSAR_10[race=="A",sex],
  bgSAR_10[race=="A",age_range]
)
length(test[test==FALSE])==0
```
Let's begin by calculating how many non-White individuals should be listed as Hispanic householders, and how many as Hispanic or Latino in group quarters.
Should draw the below as a diagram:
START HERE SLOWLY - IS THE INITIAL NEEDED??? MAY GET RID OF ALL _COUNT and not use initial at all!!

```{r move num_eth over to tract level household}
#then do num_eth_A on r, and then join for the num_eth totals from e [should equal the Hs]; then subtract for num_H_remain
trHHr_10_initial[,("num_eth_A_hh") := .N, by = c("sex","age_range","ethnicity","tract")]
trHHr_10_initial[,("trHHsae2_match_id"):=
                    paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHe_10[,("trHHsae2_match_id"):=
                    paste0(tract,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age_range)]
trHHr_10_initial[,("num_eth_hh"):=
                    trHHe_10[.SD, list(num_eth), on = .(trHHsae2_match_id)]]
trHHr_10_initial[,("num_H_remain_hh"):=num_eth_hh - num_eth_A_hh]
#somehow, need num_H_remain_gq to be by right subcategories for .N to make sense...
#trHHr_10_initial[,("num_H_remain"):=num_H_remain_hh+num_H_remain_gq (but need to do stuff for race=="A"on gq)]
#think about what is left
#summary(trHHr_10[race!="A",num_H_remain])
```

