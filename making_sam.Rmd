---
title: "Making Sam"
author: "Dan Price"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
geometry: margin=1in
fontsize: 11pt
documentclass: article
header-includes: 
  - \usepackage{tikz}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Preliminaries -- very broad - 


#every so often this chunk causes R to abort!!! Not sure why - perhaps something about the "source"? 
```{r prelims, include=FALSE}
library(tidyverse)
library(data.table)
library(gtsummary)
maindir = "~/Downloads/UH_OneDrive/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at home
#maindir = "~/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at work
housingdir = paste0(maindir,"HCAD/")
houstondatadir = paste0(maindir,"HoustonCityData/") 
censusdir = paste0(maindir,"Census/") 
vintage = "2010"
#numberOfCores = 1
state = "48" #48 Texas; 22 Louisiana
county = "201" #8 county region: 201 Harris; 157 Fort Bend; 167 Galveston; 039 Brazoria; 071 Chambers; 291 Liberty; 339 Montgomery; 473 Waller ; other place FIPS are longer
st_county = paste0(state,county) #"48201"
tract = "*"
Sam_seed = 135
#you don't need a censuskey if you're not pulling new files down; you can only use this one if you have correct access to the OneDrive
censuskey <- readLines(paste0(censusdir, "2017", "/key"))
source('BaseScripts/Census_Data.R') #move out of BaseScripts?
source('tests.R')
```

Probably use the second one, but need to think about - and maybe only in context, below:
Include and comment on diagram: https://q.uiver.app/?q=WzAsOSxbMSwwLCJmdWxsXFw7cG9wdWxhdGlvblxcXFxhc1xcO3BlclxcXFxDZW5zdXNcXDtCdXJlYXUiXSxbNSwyLCJ0cmFjdFxcO3hcXDthZ2VcXF8xMTBcXDt4XFxcXHJhY2VcXF83XFxcXGV0aG5pY2l0eVxcXzhcXFxcUENUMTIiXSxbMTAsMCwiYmxvY2tcXDt4XFw7YWdlXFxfMjNcXDt4XFxcXHJhY2VcXF83XFxcXGV0aG5pY2l0eVxcXzJcXFxcUDEyIl0sWzEsNiwiYWdlXFw7eFxcO3NleFxcO3hcXFxccmFjZVxcXzdcXDt4XFw7ZXRobmljaXR5XFxfOFxcO3hcXFxcdHJhY3RcXDt4XFw7YmxvY2siXSxbMTAsNSwidHJhY3RcXDt4XFw7YmxvY2tcXDt4XFxcXHJhY2VcXF83XFxcXGV0aG5pY2l0eVxcXzgiXSxbMSw1XSxbNCwzLCJyYWNlXFw7YW5kXFxcXGV0aG5pY2l0eVxcXFxmdWxseVxcXFxjb21tdXRlXFxcXGJ5XFw7ZGlzanVuY3Rpb24iXSxbOSwxLCJyYWNlXFw7YW5kXFxcXGV0aG5pY2l0eVxcXFxkb24ndCBcXDtjb21tdXRlXFxcXGZvciBcXDtub25cXF93aGl0ZVxcXFxIaXNwYW5pY1xcO29yXFw7TGF0aW5vIl0sWzAsNSwiYXNzaWduXFw7YmxvY2tcXDtsZXZlbFxcXFxldGg2XFw7YnlcXDtzYW1wbGVcXFxcdHJhY3RcXDthbmRcXFxccmVzdFxcO29mXFw7YmxvY2tcXFxcZnVsbHlcXFxcY29tbXV0ZXMiXSxbMCwxLCJyZXBvcnRzXFxcXChzdWJzZXRzKSIsMSx7ImNvbG91ciI6WzEyMCw2MCw2MF19LFsxMjAsNjAsNjAsMV1dLFswLDIsInJlcG9ydHNcXFxcKHN1YnNldHMpIiwxLHsiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzEyMCw2MCw2MCwxXV0sWzEsNCwiam9pbnNcXFxcKHNvbWUgXFw7bG9zc1xcXFxibG9ja1xcO3RvXFw7dHJhY3QpIiwxLHsiY29sb3VyIjpbMjQwLDYwLDYwXX0sWzI0MCw2MCw2MCwxXV0sWzIsNCwiam9pbnNcXFxcKHNvbWUgXFw7bG9zc1xcO2V0aDYpIiwxLHsiY29sb3VyIjpbMjQwLDYwLDYwXX0sWzI0MCw2MCw2MCwxXV0sWzQsMywiY3JlYXRlc1xcXFxpbml0aWFsIiwxLHsiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzEyMCw2MCw2MCwxXV0sWzAsMywidW5rbm93blxcXFxmdW5jdG9yXFxcXHN1YnNldHMiLDEseyJjb2xvdXIiOlswLDYwLDYwXX0sWzAsNjAsNjAsMV1dLFs2LDEsImNvbW1lbnQiLDEseyJjb2xvdXIiOlszMDAsNjAsNjBdfSxbMzAwLDYwLDYwLDFdXSxbNywyLCJjb21tZW50IiwxLHsiY29sb3VyIjpbMzAwLDYwLDYwXX0sWzMwMCw2MCw2MCwxXV0sWzgsMywiY29tbWVudCIsMSx7ImNvbG91ciI6WzMwMCw2MCw2MF19LFszMDAsNjAsNjAsMV1dXQ==
or: [Tract and block group merge](https://q.uiver.app/?q=WzAsMTAsWzEsMCwiUENUMTJcXFxcdHJTQVJFIl0sWzAsMiwidHJTQVJcXFxccmFjZVxcXzdcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8xMTAiXSxbMiwyLCJ0clNBRVxcXFxldGhuaWNpdHlcXF84XFw7eFxcXFxzZXhcXDt4XFw7YWdlXFxfMTEwIl0sWzEsMSwidG90YWxzXFxcXHNhbWVcXDtieVxcXFx0cmFjdFxcO2FuZFxcXFxjb21wbGV0ZVxcO2J5XFxcXGJvdGhcXDtyYWNlXFw7YW5kXFxcXGV0aG5pY2l0eSJdLFsxLDIsIlxcYnVsbGV0Il0sWzEsNywiUDEyXFxcXGJnU0FSRSJdLFswLDUsImJnU0FSXFxcXHJhY2VcXF83XFw7eFxcXFxzZXhcXDt4XFw7YWdlXFxfMjMiXSxbMiw1LCJiZ1NBRVxcXFxldGhuaWNpdHlcXF8yXFw7eFxcXFxzZXhcXDt4XFw7YWdlXFxfMjMiXSxbMiw0XSxbMiwzLCJcXGJ1bGxldCJdLFswLDEsInN1YnNldHMiLDEseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbMCwyLCJzdWJzZXRzIiwxLHsiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzEyMCw2MCw2MCwxXV0sWzEsMl0sWzIsMSwiY29tbXV0ZXMiXSxbMyw0LCJ0YWJsZXMgPSIsMSx7ImNvbG91ciI6WzMwMCw2MCw2MF19LFszMDAsNjAsNjAsMV1dLFs1LDYsInN1YnNldHMiLDAseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbNSw3LCJzdXNldHMiLDIseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbNiw3XSxbNyw2LCJkb2VzXFw7bm90XFxcXGNvbW11dGUiLDAseyJjb2xvdXIiOlswLDYwLDYwXX0sWzAsNjAsNjAsMV1dLFsyLDcsImpvaW5cXDtieVxcXFxzZXhcXDt4XFw7YWdlXFxfMjNcXDt4XFxcXHJhY2UiLDEseyJjb2xvdXIiOlsyNzAsNjAsNjBdfSxbMjcwLDYwLDYwLDFdXSxbOCw2LCJjb21tdXRlc1xcXFxzb21lXFw7bG9zc1xcXFxhc3NpZ25lZCIsMV0sWzEsOV1d)

Probably the second, and "some loss assigned" means that there's a potential for non-commutativity going forward at the block level for race and ethnicity - but it's minimized by the tract level maintaining its commutativity. Nothing is lost, in terms of possible future joins, at levels / places where full commutativity is kept and the specifications within do not change other relations.
Could start a file that marks that loss, with some numbers!

In a broad sense, we're asking - at a practical level - what's it mean to use .map on the construction of epidemiology's domain and co-domain as explicitly about how we talk about vs. how relations build on each other. Not the convergence toward a single perspective, but toward a shared commitment to speaking about what matters in the relations.

For restructuring of argument part - that the construction "from the outside" of the type theory lets you resolve the arguments without positing the consistency of the inside as given - you don't have to believe the axiom of reducibility or that there is a zero-degree homotopy (should we try to show that things like that are equal?)

Plan: find smallest units of "space" (geographic and/or conceptual) from 2010 census and build a cohesive representation of Harris County (called Sam City) by piecing together the various representations output by the census. 
By "space" we mean something that helps situate a characterization within a structure, not the placement of an individual thing within a bunch of other individual things that happen to be "close" in some assigned sense of distance. A context is an example of a distance-based space, as your neighborhood provides context for your dwelling, but part of the point of our exercise with Sam City is to show other ways to think about the structuring of "space" and the ways in which an attention to space allows one to better engage in structuring data representations that are fully responsive to their originating questions. Such a project has basic implications for how we think about neighborhood effects on individuals, most immediately in terms of the granularity we build into the representations and our sense for whether that matters for the interpretation.
We will follow a long trajectory from mathematics since the 1960s, and think about the objects of mathematics as having structure, and themselves being amenable to analyses of various sorts. This is one (of many) attempts in the academic disciplines to not begin with atomistic individuals and then build a representation of the world as the interaction of those individuals. In the social sciences one runs into discussions of the primacy of fields, relations, ecological systems, emergent forms, or complex adaptive systems. They (and others) seek to have a way of representing the social reality as having a structure that isn't simply the aggregation of individual effects. This can be because they want to find better ways of performing the aggregation - i.e., they still believe that the underlying reality is composed of individuals doing things - or it can be that they believe that the structures precede and produce the individuals in some way - i.e., that there is a natural order, a shared set of interests, commonalities in ways of acting, or constraints on the field of becoming that will result in only certain outcomes being available to the individual.  
We will see a paired problem - or opportunity - of what we may call "computability." It has to do with the choice of the basic unit around which other computations will be performed. It is a determination that is in terms of the space, as determined above, but that gives that space its "best" metric, and grounds calculation in terms of operations that fit that metric. In this case, the natural choice is the individual person, although there are times when it might be more effective to use "family" or "household" - that the households and families are ultimately made up of individuals is what grounds the final choice of the computational metric, but that choice makes it harder to model people who don't live in traditional arrangements. They end up being either implicitly or explicitly excluded from models of social dynamics based on household and family data, at sometimes hard to measure loss in the utility of the models produced. There are other ways of thinking about computability, mostly having to do with whether the internal logic of the computation guarantees completion, with famously difficult cases arising from the way that computational approaches handle infinity. Here we note that there is a complement to that question, which is merely that the sense of having completed a computation requires a meaningful unit upon which the computation terminates. That every thing that we can represent to ourselves can be represented in terms of binary code jumps to the end of the argument, and although we'll touch on that, later, the point here is to note that there are mundane senses in which computation depends on knowing what the unit of computation is, and how it relates to the operations performed on those units.
As we saw in the previous section on problems with the American Community Survey, there are practical reasons we need to move back to the 2010 decennial census, especially it's detailed geographic reports. There are two main technical documents for the tables created for smallest area (block group) demographic data. 
The two have similar tables, but different levels of detail provided. They provide some information on how to download the files, although we're using the R library censusapi in our script, CensusData.R, to make calls.
If you want to look at the details, go to the List of Tables (Matrices) to see what sorts of reports are produced. The first is populations summarized (with a few exceptions) at the block level: [block level summary](https://www2.census.gov/programs-surveys/decennial/2010/technical-documentation/complete-tech-docs/summary-file/sf1.pdf)
and the second is for the tract level summaries: [tract level summary](http://www2.census.gov/programs-surveys/decennial/2010/technical-documentation/complete-tech-docs/summary-file/sf2.pdf)
The table names that begin with PCT never go below the tract level, but occassionally have more detail in some of the concepts. The Census Bureau is bound by law to produce a count for the decennial census and it pairs that with only the basic demographic, household, and family data. Things like income, employment, and education (among many others) are modeled by the Census Bureau only for the American Community Survey, which tries to prevent the reconstruction of individual identities and location of individuals in particular tracts. They did this because early on there was a general recognition that such information would be used to violate people's expectations of privacy - they would be subject to advertising or perhaps harassment or prejudice - based on their honest answers to the census. Unfortunately, the financial interest in producing such information is quite large and most of that expected privacy has already been lost. We have no interest in breaking down even the walls that the census puts up. We are not calling out the Census Bureau for some sort of mistake. We are saying that in one of the most foundational products that people use for representing U.S. society, there is a way to think about how the structure pervades the construction of tables and other representations. Understanding that structure helps us point to some hidden opportunities in using population-level data, but also to limitations to thinking about a population of individuals with relations at a given time (the fundamental metaphor of the census, after all) instead of thinking about people on lived trajectories in relation with others being constrained or guided by structures. That one, for example, should address a problem by removing unnecessary barriers instead of adding capacity to the individual facing the barrier, is a (perhaps) surprising result of the method. 
Let's begin with PCT12, which is tract level data but has every age by year, gender, ethnicity and race. It has 101 possible age groups, with 1-99 years and then one for "100 to 104 years" and one for "105 to 109 years". It then has categories for all the built in race and ethnicity categories, with each person counted in the total, and every person categorized by race (there are tables for how many people had to have a race assigned by the census bureau, since they refused to provide one, but they tell us how many people have an assigned race and describe their protocols); Hispanic is a problematic category for counting, as well as for historical reasons, and is dealt with differently in the decennial and ACS reporting. Let's download table PCT12, do some quick checks, and then look at it.  

```{r look at PCT12}
#this gives you by every year at the tract level - could be a good example for adding together - PCTs never go below tract level
trSAR_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "PCT12",county_num = county,
                         block="tract",api_type="dec/sf1",path_suff="est.csv")
trSAR_data_from_census_10 <- trSAR_data_from_census_10 %>%
      select(name,label,concept,starts_with(st_county))
#needs to be numeric for the sum in the test
trSAR_data_from_census_10 <- as.data.table(trSAR_data_from_census_10)
trSAR_data_from_census_10[,4:ncol(trSAR_data_from_census_10)] <- 
  trSAR_data_from_census_10[,lapply(.SD[,4:ncol(trSAR_data_from_census_10)], as.numeric)]
head(trSAR_data_from_census_10[,1:5])
```

A quick glance shows that this has reporting for every year of age and at fifteen different levels for race and ethnicity, which is considerably more specificity than the reporting under ACS or at the block group level, reported as the number of people who fall into that category for every census tract in the state. That specificity provides a very granular embedding context (or structured space) on the conceptual level but is less specific geographically. It has every tract for the entire state, but some of the larger tracts are divided into 6 block_groups, which provides considerably more geographic context. One of the goals of our current approach is to show why thinking about the reporting on basic concepts like sex and age should be thought of as a similar problem as geographic boundaries - i.e., that both should be thought of as an articulation of structure within the context where the individual is placed in the representation. By paying close attention to that problem of articulating a coherent and meaningful structure, we can make explicit which mathematical operations are justified when transforming between the various possible representations of the data. 

Just to give a sense for how testing for coherent structure arises from and resonates with the broader process of building the data representation, we include some of the very basic ways that we can test for consistency. The following are repeated for each download from the census, as we look for obvious problems with the file either as reported or as a result of an improper download (like asking for the wrong file). In this version, we have put a data.table wrapper around the census data and sum the number of NAs in the columns. Because of how the Census Bureau allows access to its APIs, public users without permission for certain levels of granularity will sometimes return a file with NAs but not consider that a problem and not produce an error. In this case, we decided to represent the number of NAs as a percentage, and not just an absolute count, because sometimes we want to use data that is reported for only some of the rows, even if NAs are reported in some of the other rows. We don't want the download to fail unless there are only NAs, but if there are any at all, we want to know about it and know where to look for the problem. In later downloads, several of these tests are gathered in a single function call in tests.R as census_table_check. One of our ongoing tasks is to better automate those tests.

```{r test for na}
    percent_na <- trSAR_data_from_census_10[,sum(is.na(.SD))] / 
      (trSAR_data_from_census_10[,sum(!is.na(.SD))]+trSAR_data_from_census_10[,sum(is.na(.SD))])
    paste("Percentage of NAs in file:",as.integer(100*percent_na))
    rm(percent_na)
```

After seeing that there are no NAs, we need to check to make sure that the reported totals at least add up - every so often the census makes mistakes that we need to clean up in some way, including very occasionally putting the wrong values in aggregation cells. As we look at the "label" column, there's a row that contains the total for each concept and for the whole. The rows that say only "Total" are given for every version of "concept" that is given. This means that the representations can be embedded in several different ways, while still counting as "total," and we need to account for that in our representation. In the rows that say more than just "total," there are also rows for total female and male, which means that adding all that side up automatically is twice as large as the rows that say "label" only. We had the whole state, so for our case we'll select down to county first. In the GEOID that contains the tract numbers, Texas is 48 and Harris county is 201. https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html

```{r check for consistency on totals}
#there's a total for each concept and for the whole, but in the ones without a total, there are also total female and male.
test <- colSums(trSAR_data_from_census_10[label=="Total",4:ncol(trSAR_data_from_census_10)])*2 ==
      colSums(trSAR_data_from_census_10[label!="Total",4:ncol(trSAR_data_from_census_10)])
#see if any of the tests don't match; if false, need to go back and check on what happened
length(test[test==F])==0
```

If we want to know only the row that gives us the official census total population per census tract, we can add in the designation for concept. We know more having looked at the calculations involving all rows, as above, because if any of the totals had not been equal, we could look for a data problem in the specific place in the tables where the error emerged. Let's get the official totals in a separate file, just so we can use it to check later. (We'll find that we need it right away). 

```{r get totals by tract}
pop_totals_tract <- trSAR_data_from_census_10[label=="Total"&concept=="SEX BY AGE",4:ncol(trSAR_data_from_census_10)]
paste0("Population total for Harris County by tract: ", sum(pop_totals_tract[,]))
```

The same convenience set lets us know some other facts about the variation in size of the tracts in Harris County. Shortly, we'll find better shortcuts for displaying summary statistics, but for now we just create a few quick measures.

```{r Harris tract stats}
paste0("Number of Harris County tracts: ", ncol(trSAR_data_from_census_10[,4:ncol(trSAR_data_from_census_10)]))
paste0("Average population size for Harris County tracts: ", as.integer(mean(as.numeric(pop_totals_tract[,]))))
paste0("Median population size for Harris County tracts: ", as.integer(median(as.numeric(pop_totals_tract[,]))))
paste0("Maximum population size for Harris County tract: ", max(as.numeric(pop_totals_tract[,])))
paste0("Minimum population size for Harris County tract: ", min(as.numeric(pop_totals_tract[,])))
```

The population total for Harris County is what you'll find on the internet as the official population of Harris County in 2010, as long as it's referring back to the Census Bureau (cf, https://www.census.gov/quickfacts/harriscountytexas, accessed on 1/14/22).

Now that we have the information at the tract level, let's also pick up the block level data.

```{r block_group sex by age}
bgSAR_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P12",county_num = county,
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
bgSAR_data_10 <- as.data.table(bgSAR_data_from_census_10)
bgSAR_data_10[,4:ncol(bgSAR_data_10)] <- 
  bgSAR_data_10[,lapply(.SD[,4:ncol(bgSAR_data_10)], as.numeric)]
rm(bgSAR_data_from_census_10)
```

It would be nice to use this immediately as our base, but we quickly see it doesn't have as many rows breaking out race and ethnicity. It has nine total categories for race and ethnicity - losing most of the categories in ethnicity - and age is reported in odd aggregations that will make comparison difficult with other data reporting. Let's run a few quick tests on the download.

```{r demography test problems}
check_summary <- census_table_check(bgSAR_data_10, "SEX BY AGE","individuals") #should get a function that fails appropriately
cat(check_summary[1])
pop_totals_bg <- bgSAR_data_10[label=="Total"&concept=="SEX BY AGE",4:ncol(bgSAR_data_10)]
sum(pop_totals_tract[,])==sum(pop_totals_bg[,])
paste0("Number of Harris County block groups: ", ncol(bgSAR_data_10[,4:ncol(bgSAR_data_10)]))
paste0("Average population size for Harris County block groups: ", as.integer(mean(as.numeric(pop_totals_bg[,]))))
paste0("Median population size for Harris County block groups: ", as.integer(median(as.numeric(pop_totals_bg[,]))))
paste0("Maximum population size for Harris County block group: ", max(as.numeric(pop_totals_bg[,])))
paste0("Minimum population size for Harris County block group: ", min(as.numeric(pop_totals_bg[,])))
rm(pop_totals_bg)
rm(pop_totals_tract)
```

We note, very quickly, that the median tract is almost three times as large as the median block group, but there is a good deal of variability in size on both. 

#A Philosophical-Mathematical Digression - Speaking About 
##(the more practically-minded may simply skip this section - or maybe we should put it later, anyway)
We should give a quick nod to what it means to say one representation is "equal" to another representation, even at the most basic levels. Clearly the files at the tract level and the block group level are not simply equal to each other, since they have a different number of rows and refer to different geographies, but they are presented in such a way as to make it possible to compare arithmetically - we simply ask whether, when we set the concepts or boundaries to the same shape or conditions, do we get the same number of people counted in that category. The total number they project can be equal (or not), and that is perhaps the simplest way to think about two different representations still being "equivalent" - that is, when they have "equal" results of appropriate arithmetic operations performed on appropriate substructures.

But we need to think carefully about the several ways we talk about "appropriate," here. Even when the sums are equal to each other, we face important decisions about the structure of the representation that allows us to calculate the sums, and whether what is being counted has changed across differing structures of representation. Much of that structure, in this case, is literally a "space," because it's the difference between the geographic characteristics of block groups and tracts. The fact that both were created by large committees of government officials with a variety of opportunities for input from other stakeholders and the public, and that they were deliberately created to embed smoothly into each other for the purposes of aggregation, allows us to do the aggregations across the structures with confidence. That same structure also allows us to think more fully about other possibilities for representing the inner connections and the potential calculations that are enabled by those structures. We hope to eventually also show that these structures, even though they explicitly belong only to the choices about mathematical representation, will give us more substantial clues to otherwise hidden structures at the neighborhood level, as the accumulation of structures refines what can be said about the relations between the space and the individual, as well as what is implicit in the previous descriptions but still has to be made explicit through some determined process. 

We should also note that this touches on all sorts of political questions, with different ways in which public input is accepted into the decision-making process. For example, the Census Bureau took steps to make these boundaries coherent and complete (both the geographic boundaries and the delimitation of the conceptual categories), but that is justified by the need for a coherent and mathematically consistent representation. The necessary calculations on demographics (which are at the base of so much else in public health, after all) are often difficult or impossible in less well-structured spaces, for example, where cities and counties don't share all geographic boundaries but have large shared areas, as with the City of Houston and Harris County. Finding ways to report numbers, even the basics of population, have real world funding consequences, which become tied into the presuppositions of the models at multiple levels. However, those bigger questions remain at the horizon. For the moment, we want to take our building process slowly, for the purpose of eventually understanding what is at stake in these first and seemingly trivial decisions that in turn rely on other structures - after all, we have to be confident that there are overarching mathematical structure that are sustained across the representations in order for the larger interpretation to make sense, even when that interpretation is the almost immediate and seemingly trivial act of aggregation. 

One of our philosophical inspirations comes from the way that type theory differs from set theory. Both of them provide ways to encode other mathematics into a single way of talking and making proofs, with many outward similarities but important differences in how the process of proving mathematical propositions is expressed. In the encoding of mathematics based on set theory there are objects (called sets) which have a membership relation to other objects. In type theory, [Per Martin-Löf's intuitionist version](https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf) is especially clear in its motivation , types are the way that elements can be described without positing the existence of a set or of objects that have the property of belonging to that set. For a common example of where it makes a difference, if you encounter a group of points in a set, you cannot make sense of them in a full and meaningful way just by knowing that they all have the attribute of being a point; the Banach-Tarski paradox, for a motivating example, showed that using the rules of set theory and a few suppositions about infinity and the axioms of mathematics, a sphere of any size can be divided and then recreated into a sphere of any other size. In our terms, set theory preserved the structure of membership in the set of points that are found on the surface of a sphere, but sacrificed the relation of those points to the size of the sphere, since the structure that sustained size wasn't part of the set, although an infinite number of points, by definition, was included. In type theory, on the other hand, the objects that are manipulated, including the numbers, must be accounted for - the space that the points are found within lets you describe them as close together or far apart, as countably infinite or as generated by the relation to a fixed center, and (more broadly) Martin-Löf's type theory allows a collection of typing declarations to construct or encode any mathematical object in ways that account for the structure one wants to preserve. 

The constructive process does not imply ontological relativism, but does rely on the idea that there are many ways of talking about things and that being conscious in those choices between ways of talking, and attentive to the motivations guiding those choices, can ground the practice of mathematics as a search for structure. In the history of the theory of mathematics, constructivism is usually opposed to one or another sort of Platonism, but for our purposes, we're striving to motivate references to mathematical theory or its philosophical background only through encountering and responding to issues with the concrete understanding of creating Sam City. It is hoped that this can be done in a way that could be useful for our students, or for any of the fields, like population health, that rely on a representation of demographic attributes for a population. Here, we are underlining the fact that certain methods become available to us because of the conscious decisions of the U.S. Census Bureau to create a database that will be useful for calculations - the fact that every person is assigned at least one race, and that the reporting includes all people in its categorizations, means that you can use the law of the excluded middle and say that if someone isn't one of the other 6 races, they must be the one that's left. That is constructed by the Bureau to be useful for the purposes of the aggregation. The addition of ethnicity in the 1980 census, after a much contested attempt to add "Mexican" as a racial category in 1930, was a politically motivated attempt to account for ways in which racism against people of Latino origin was impossible to track without having some way of counting how many people fit into the category - which is to say, it's a politically meaningful choice whether to include it or not; there is no simply "non-political" side to the act of category construction or the choice to either maintain or modify the categories around race and ethnicity. (cf. [Changing Race](Rodriguez, Clara E. Changing Race: Latinos, the Census, and the History of Ethnicity in the United States. New York University Press, 2000.) E184.S75 R64 in library). In the more recent accounts of race and ethnicity, it is striking that there is simply no clear account of how people see themselves, except to say that they aren't particularly satisfied with the way that the census divided race and ethnicity in its attempts to deal with the fraught political history. (cf. [Pew Research Survey: The Many Dimensions of Hispanic Racial Identity](https://www.pewresearch.org/social-trends/2015/06/11/chapter-7-the-many-dimensions-of-hispanic-racial-identity/))

These questions are separate from the still difficult question of undercounting (cf. [NYTimes on 2020 undercount](https://www.nytimes.com/2022/03/10/us/census-undercounted-population.html)). The collection itself is riddled with difficulties that effect the final accuracy of the tables. We will try to speak, first, to what is known by virtue of the tables themselves, and then see if insight about that process, and about what it means to think in mathematically and philosophically coherent ways about these tables, can reflect back on the larger social and political context of motivating questions.

We said earlier that part of the initial impulse behind the construction of Sam City was to make a more "friendly" version of Houston for pedagogical purposes, and it's worth being explicit that there are deeply racist components to Houston's past that should not be erased in the name of that friendliness. Our hope, by contrast, was that a more honest accounting could be made possible by understanding how the construction of the basic demographic categories in the population of the simulated city could reflect the complexity at hand. Accordingly, just as the addition of ethnicity as a category was troubled by a triangulation of political issues around segregation throughout the U.S., Houston's White political leadership attempted to achieve court-ordered segregation in the schools by integrating Latino and Black schools with each other, while leaving the White schools untouched. This sleight of hand was achieved by counting the Latino students as White, but ignoring the long-standing inequities in the schools in Latino neighborhoods. There were considerable political and legal actions against Houston at the time [NYTimes, March 1970](https://www.nytimes.com/1970/03/01/archives/houston-moves-to-desegregate-schools-amid-clamor-of-protest-angry.html) and [NYTimes, September, 1970](https://www.nytimes.com/1970/09/06/archives/houston-huelga-schools-open-in-a-mexicanamerican-protest.html), but the schools remain deeply segregated throughout the United States, and especially the South, as documented by [UCLA's Civil Rights Project](https://www.civilrightsproject.ucla.edu/research/k-12-education/integration-and-diversity/brown-at-60-great-progress-a-long-retreat-and-an-uncertain-future/Brown-at-60-051814.pdf), as well as in Houston's larger metro area. These histories all have continuing ramifications on current social, educational, and health outcomes, as explained by the [City of Houston's Health Department](https://www.houstontx.gov/health/chs/documents/Health-Disparities-Data-Report-I-2019-Root-Causes.pdf).

To say that "race" is embedded in a biological determination and "ethnicity" in a cultural context, clearly, is to miss how the structuring questions are motivated and thus to obscure at least some potential pathways toward a more acceptable general approach to embedding relations between individuals and social structures than as sedimentation of layers of causes and effects. At a social level, we need a way to refer to the history of determinations that have affected individual people's life courses, as others create barriers based on those ways of talking about who does or does not belong in a group. To do so, however, risks reifying those determinations, or creating a situation where one way of talking -- for example, the idiosyncratic development of the idea of race and ethnicity in the U.S. -- is taken as the reference from which all other approaches deviate.

Here, the scientific temptation is to try for more complexity -- to determine variance in reference to an encompassing schema that provides more information and allows for more finely tuned subsetting, such as a genetic characterization which tracks ancestry. In such an analysis, one might speak of 68 distinct ethnicities in Mexico, with recently refined mathematical techniques allowing researchers to show distinct genetic characteristics, tuned to different geographic regions in Mexico [The genomic landscape of Mexican Indigenous populations brings insights into the peopling of the Americas](https://www.nature.com/articles/s41467-021-26188-w). These genetic characteristics are plausibly linked to a number of biomedical traits, with considerable scientific resources committed to finding pathways to providing genomic medicine or enabling precision medicine through incorporation of genetic analyses [The Genetics of Mexico Recapitulates Native American Substructure and Affects Biomedical Traits](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4156478/). Of course, if one is to embed these mechanisms within a more fundamental physical process of biochemistry, there are orders of magnitude more levels of complexity to account for in epigenetic terms than just the transcription of the traditional base pairs &mdash; or &#151; cf., for a recent but not final account, [Mapping the epigenetic modifications of DNA and RNA](https://europepmc.org/article/med/32440736). More fundamentally, it is clear that such a scientific account can inadvertantly end up masking the social and political dynamics, which have frequently been driven by erasing these sorts of fine-grained nuances and lumping people who are not seen as part of the accepted group(s) into an "other" category. Some sort of account that tracks what the distinctions are being used for, and in what context they make sense as being useful for answering a problem, needs to precede the questions about whether the distinctions fully exhaust the particular frame. That preceding question, for etymological reasons, we will call difficult (it is literally not easy and resists being calculated) instead of complicated (which implies a multitude of layers, which are resolved through unfolding and determining single values within a multi-dimensional space).

We would prefer to resist that temptation to speak about complexity, and seek to focus on the difficulty of the question: how do we think about the embedding so that it doesn't reify a sense of determination in isolation? And how do we bring that difficulty into focus as a pathway that addresses questions of value to social, economic, and health outcomes? Let's take a second to think about how to approach the problem of embedding, before moving back to our proposal to think about the construction of the data tables as "ways of talking."

Type theory gives us one well-articulated alternative way of thinking about the embedding, which is popular precisely because it still allows a unifying understanding of how to encode any proposition about the world in the straightforward and consistent language of types. Its modern articulation is made possible by the recognition (starting with Tarski) that truth is a property of a language that is "higher" than the language in which we describe the objects. This was meant to address foundational problems with set theory, but also has to do with how one tries to separate syntax (form) from semantics (content). Tarski says that claims made in a given language about given objects are embedded in an encompassing model that provides rules for what is true or false. Intuitionistic type theory points out that there is a process of construction that undergirds both, and thus makes it possible to understand embedding as a choice about what structures matter in the model - so that, in our example, we can ask explicitly how the relations of race to ethnicity are embodied as countable in the census, and also use that artificial construct to infer other aspects of the structure that aren't immediately given in the tables. In that sense, we are trying to understand the types as constructed categories, and then to understand what the limits of that construction show - both positively, in terms of how operations performed on the data within those bounds can create new understanding of internal relations, and negatively, in terms of understanding what can't be said in that language.

Very briefly, it's worth remembering that type theory began as an answer to Russell's paradox, although two steps removed, as Alonzo Church first developed untyped lambda calculus and then typed lambda calculus. The underlying task, stemming from Georg Cantor's foundational formulations of set theory, and also found in the continuum hypothesis, is that when you create sets, you need to be able to understand their internal structure in order to perform operations on the set. Set theory had been designed with the idea of capturing predication - one seeks to understand what it means to say that something has a predicate by saying that it belongs to the set of objects with that predicate. If one can generalize this process, then the use of numbers to ensure structure inside the sets (simply put, that I can assign a number as a predicate that "represents" each object and then, with suitable caveats, shows how the set of objects can be well-ordered) can itself be trusted. Cantor saw that you needed to be able to say that the power set of a set - that is the multiplication of all the combinations of categories - had to be larger than the elements of the set. It had to be possible to create more subsets than elements in the set. This may seem trivially true, but runs into difficult problems where the problem of representing infinite subsetting meets the problem of ordering (i.e., with Dedekind) and one has two competing ways to order what is infinite suggesting that the ways to subset the infinite do not converge to a single rule that would encapsulate the meaning of the infinite. Or more importantly, the idea is that if you are trying to understand how a type is constructed, you are asking how it is calculated, and if you calculate without attention to the construction, you fall into logical traps that prevent effective computability. (Martin-Löf, before the invention of Haskell in 1990: [Constructive Math](https://www.cs.tufts.edu/~nr/cs257/archive/per-martin-lof/constructive-math.pdf)). [Martin-Löf, Constructive mathematics and computer programming, in: Logic, Methodology and Philosophy of Science, vol. VI, 1980, pp. 153–175.] 
[[need to think about: W.A. Howard, The formulae-as-types notion of construction, in H.B. Curry: Essays on Combinatory Logic, Lambda-calculus and Formalism, 1980, pp. 479–490; there's an overview "Propositions as Types," at https://www.youtube.com/watch?v=IOiZatlZtGU by Philip Wadler that strikes me as very clear. He frames it in terms of the capacity to reduce logical propositions to a computable form and how the lambda calculus duplicates the formalism - he rather bizarrely asserts that it shows that math is discovered, and not invented, even though it's all about constructive math - including a digression on lambda as the universal programming language. It maybe be worth talking about Haskell as Curry's first name; that functional programming was looking for a way of being on a first-name basis with a dominant figure in the field, perhaps in a parallel with Sam, with some sense of the strengths and weaknesses...
Need also to think about how explicit Martin Löf is about the problem of hierarchies and how they get resolved: "156 - continuing from 155 - concisely: "'if a set is understood in Zermelo’s way as a member of the cumulative hierarchy, then a set cannot be the same kind of thing as a data type.'"; that we aren't dealing with membership, and the truth about membership, but about labeling, and the consistency of labeling - and doing it constructively]]

Our point, with this extended digression into assigning race and ethnicity, is that as we try to assign attributes to individuals in consistent ways, we must assume the legitimacy and applicability of the categories - an assumption we have strong reasons not to make! If set theory can be used, it's because it presents the possibility of infinite interior correction of categories - we can add complexity to the description, produce more factors for analysis, and refine the idea of membership - all within our overarching confidence that there are more ways to name things than there are things to be named. The problem is that this assumes that the whole problem is about coming up with the right names - the right way to say things. The idea that type theory introduced was that the higher orders of embedding could be typed in such a way as to ensure order (cf. Church's typed lambda calculus); they also introduced the idea that the order of calculation could be from the outside in, whereas set theory had created the situation where things had to be solved from the inside - which precluded some of the most useful steps in construction.

#MAYBE THE IDEA OF ORDER OF CALCULATION IS A SUBTLE WAY TO INTRODUCE THE NEED TO CONSTRUCT; THAT ONE CONSTRUCTS BOTH THE DOMAIN AND THE CODOMAIN IS KEY IN LONG RUN, AND DIFFERENT FROM TYPE THEORY, WHICH WANTS THEM TO BE THE SAME; CATEGORY THEORY ASKING HOW YOU MOVE FROM ONE WAY OF TALKING TO ANOTHER!!!
##not just that you have lazy evaluation; it's the idea that evaluation happens along the line of the construction and not along the line of a presupposed what it must have been like to have been built from first order primitives / homotopy degree zero
##for longest arc: what does it mean to stretch toward saying something? To be engaged in a question?

There's also the question of formalism as inappropriately relying on the infinite and the law of the excluded middle as purely formal.

Could talk about the "algebraic" as in Lawvere's Conceptual Mathematics in terms of multiplying objects and mappings between ways of multiplying the objects. Then the Cartesian product, below, is seen as multiplying objects and the structure is maintained.

It may be that the algebraic approach toward embedding in category theory gives us yet more powerful tools, especially in the idea that we are looking for the ways that descriptions can be transformed into or mapped onto equivalent descriptions embedded in different models as the way of specifying the meaning of an object: in other words, the proposition is not true or false depending on the embedding within a metalanguage, but rather the algebraic possibilities that govern the totality of possible ways of talking about the same thing will tell you what a mapping that conveys a true proposition would mean. That is, in Lawvere's articulation of the idea of a mapping from hyperdoctrines, as in ["Diagonal Arguments and Cartesian Closed Categories,"](https://doi.org/10.1007/BFb0080769) from 1969, (Lawvere, F. William. “Diagonal Arguments and Cartesian Closed Categories.” Category Theory, Homology Theory and Their Applications II, Springer Berlin Heidelberg, 2006, pp. 134–45.), the algebraic possibilities that govern the composition of the steps that create a representation tell one where the limits of a way of talking about things lies. Russell's paradox and Gödel's incompleteness theorem both become instances of an attempted transformation of talking about sets, where the attempt to describe the set of all sets becomes self-referential and paradoxical, into the problem of how to transform sets through mappings where the presupposed notion of truth is a mapping of points to points. 

The limitation of that approach can be seen by turning to algebraic topology and to ideas of mapping that don't reduce to point sets - or to binary representational functions. Here we hope to capture something of the humanities criticism of traditional set theory, as with all the descendants of Heidegger who decried the technological approach to language as mere predication, and say that there is also a mathematical approach that seeks to situate set theory within a broader understanding of the possibilities of speaking about. Broadly, it is the idea that the ways of talking about something are structured through the process of describing and that the attention to that structure is the core of the scientific process. Of course, it's not the only way to talk about things, and we'll try to point to the motivating questions as opposed to the dogmatic elements of any of the tools that have inspired this inquiry. How we can practically encode a claim about mathematical relation, however, is the guiding reason that Sam City was developed, as we pushed to understand what it would mean to create a reusable database of demographic, social, and economic information about residents and neighborhoods in our area that somehow also captured dynamic change and the sense of individuals that they somehow exceeded the objectifying categories assigned to them by the system. How do we make that sense of organic development and internal self intelligible to others? How do we compute the results of aggregation over large groups of individuals, each pursuing their own paths? How do we represent the effects and interactions? Those are practical and driving questions for ourselves, our students, and our community partners.

What then counts as the guiding principle for scientific research on the sort of dynamic system that constitutes a city and its inhabitants? Instead of looking for a predicate that describes the object and asking whether it is true or not - as when you ask if "Hispanic" is the right term for a Catalan on a temporary work visa - you look for the structure that enables true or false predication to be about the individuals who are being talked about. At that point, one can identify the limits of the framing questions and the limitations of the way that the domain of possible answers was constructed, and ask effectively what changes can be made in the process of talking about that would better match the motivating questions.

For our case, we propose to return to the example of making Sam City, and of asking explicitly about the structure and what emerges. In the somewhat simplified case before us, we are tempted to ask how to join ethnicity with race at the block level.


#Embedding Calculations in Structured Spaces. (that any and all "space" is "structured" will come out later; how that "pairs" with the computational metrics is foundational to the reworking of criticism of idea of economic idea of individual)

In our current project, we are trying to identify and leverage the structures in place in the data tables. For the next step, we need to transform the counts we have for demography so that each person is represented as having properties, including geography and demography. From a purely technical side, this is just transposing the columns and the rows, so that each row is a census tract, and the columns correspond to the attributes that had been in the columns. Then, we will expand the number of individuals with those characteristics by just adding a row for each individual represented in the count. One can think about this equivalently as having created individuals with predicates, following the logic of the tables, or - as we propose as an alternative - we can take the predicates as structures of a shared space, where individuals expand into that structure according to rules that govern the individuals' potential pathways. Those "rules" are given either directly or indirectly from other tables, with other predicates, but in some way still referring to the same individuals within the same census tract. How to combine those predicates is the task that ran us aground the first time; how to construct a space of possible predication in terms of the individuals inhabiting that space is the question we are now pursuing. It may be helpful, though, to recount the shipwreck from the initial effort, and to place it into its explicit and practical context.

Let's take a closer look at the tract-level data, as the most specific for year and race category, and then add the block_group information because that gives us important geographic differences. We'll work with only Harris County at this point, since we assume that the structure is repeated, by explicit design of the Census Bureau, across the entire population.

First, we need to identify the rows which, if added together, give us a complete and non-repeating set of the population. There is one way to do that with the overall total row, with the two gender total rows, and with all of the individual age rows without the totals. Unfortunately, the census reports have a somewhat idiosyncratic way of reporting the data, and the way to identify these rows is not always simple. This case is relatively easy, and a good example to start with. The "label" column has three components, separated by double exclamation points. (They made changes in the way the separations work in 2019, so these small scripts will have to be modified for later years. The decennial and ACS also have small differences that have to be accounted for, as we will see. A lot of the difficult work is the tedious effort involved in cleaning up the data for processing, but cleaning and structuring are similar tasks, and they provide a good example of what it means to be clear about the structure that sustains the potential analyses).

We can use a little trick because of how the R library dyplyr performs the translation and expansion of the representation, so that we're looking at rows of people with geographic and demographic properties and not rows of categories against columns of geography. We will get a warning that NAs were introduced by coercion, but it happens that those will be the rows that we want to exclude in any case. We'll show the warnings, here, but suppress them later.

```{r expand SARE for the tracts in PC12, warning = FALSE}
trSARE_10 <- trSAR_data_from_census_10 %>%
  pivot_longer(4:ncol(trSAR_data_from_census_10),names_to = "tract", values_to = "number_sams",
               names_transform = list(tract=as.character)) %>%
  separate(label, c("total","sex","age"), sep = "!!", remove = F, convert = FALSE) %>%
  mutate(re_code = substr(name,7,7),
         race = str_replace(concept,"SEX BY AGE \\(",""),
         race = str_replace(race,"\\)",""),
         age = str_replace(age, "Under 1 year", "0"),
         age = str_replace(age,"year"," year"),
         age = as.integer(substr(age,1,3))
         ) %>%
  filter(number_sams > 0, !is.na(age))
paste0("There are ",length(unique(trSARE_10$age)), " unique categories for age (each year up to 100 and then two over 100 groups).")
paste0("There are ",length(unique(trSARE_10$re_code)), " unique categories for race and ethnicity.")
paste0("Along with two categories for sex, we have ",length(unique(trSARE_10$age))*length(unique(trSARE_10$re_code))*2," different categories people can fall into.")
rm(trSAR_data_from_census_10)
```

Once we have the file for the entire population, we'll want to make one complete population with ethnicity and one complete population with race. It is worthwhile to take a second to think about what it means for the two representations of the entire population to be "equal" to each other. That they have the same number of people in each tract is part of the justification for saying they are equal but might be better phrased as saying that both tables are about the "same" people. There is also the way of saying they are equal because the schemas for classification have the same number of individuals in them, regardless of what you call the person in the particular category (i.e., "Hispanic" or "Latino" are treated as the same thing). And then, yet further, there is the sense in which the distinctions, and the combinations of the distinctions (i.e., what is called the power set produced by multiplying categories by each other) are equivalent if they divide the individuals considered as objects in the schema in the same way. That is, if you slice up the sets in different ways, but still come up with the same number of people in each cell, there's a certain type of "equality" there. To speak of "equality" in "ways of talking" is an attempt to make clear how mathematical objects, their structures, and the ways that their structures are conserved (or not) when moving between these "ways of talking" are all at stake even in fairly mundane classification tasks.

The fact that both the ethnicity tables and the race tables are about the same individuals, as presented in complete form through aggregations of individuals who have shared predicates, tells us that the "ways of talking about" those individuals are grounded in that sense of the individual. Some census tables don't share that same sense of individuality - some talk about households, for example, or about the housing stock - and yet there are still ways of delimiting the "sameness" of the tables - for instance, if they each talk about and characterize in some sense individual census tracts, which can be mapped back to individuals or other sorts of objects that can be counted, and can be considered the ground of saying that we're still talking about the same thing. This fact opens the door to talking about the relations that are mapped, with the individuals as endpoints of the mapping; to speak concretely about the relations first, and allowing the individuals to emerge into systems of possible relations, is the task we set ourselves with the construction of Sam City.

This table, we admit, caught us by surprise. All the other tables by race and ethnicity had only two ethnicities reported - one for "Hispanic or Latino," and another for "White Alone, not Hispanic or Latino." This led us to considerable difficulties in sorting out who might be, for example, answering "Some Other Race" and "Hispanic or Latino," or "Black or African American" and "Hispanic or Latino." These are admittedly smaller segments of the overall population, but important sectors to capture in the representation, if we are going to claim to understand details about the ways that race and ethnicity effect outcomes within the context of neighborhood dynamics. Even though we had been working extensively with the American Community Survey, we simply hadn't come across these tables. They don't solve all of our problems, but give information directly that will be of great value, below, and would have been very helpful in other contexts, earlier. Before getting to that very difficult question, we remember that at this point we're trying to understand how to maximize the information contained in overlapping ways of talking: how do we understand the addition of the idea of ethnicity on top of the idea of race? First, just be understanding in what sense it still refers to the same individuals and in what sense it constitutes new information.

Here, we're doing the expansion of race first, because in all the other tables, the race categorization is more "complete" than the representation by ethnicity. Which is to say that some people in the population are not represented in terms of their ethnicity, while everyone has their race reported. The sense in which the representation by race and by ethnicity are equal at the tract level, but not at the block_group level, gives us a chance to think about what it means for the representations to be embedded in conceptual spaces on rigorous mathematical terms, and to begin to see the ways that our representations are limited by those conceptual spaces as well as pointing to techniques that help us expand those representations. The equality of the representations come from the way in which they are structured by their endpoints as represented in a certain total number of individuals; it forces us not to think about the equality of two numbers in arithmetic, but the structure of embedding claims in the same underlying space. Taking another clue from recent turns in mathematics, we can say that it is, in certain conditions, equivalent to speak of the same individuals in different ways, to the extent that they both serve to describe the same individuals. The difference in describing those individuals by ethnicity instead of race emerges in the ways that the context of that designation matters, and not as a function of individuals essentially being in one set or another. The Census Bureau provides a helpful explanation, including the race iteration as [table ids explained](https://www.census.gov/programs-surveys/acs/guidance/which-data-tool/table-ids-explained.html)

```{r expand tract SAR}
    race_codes <- c("A","B","C","D","E","F","G")
    trSAR_10 <- trSARE_10 %>%
      filter(re_code %in% race_codes) %>%
      uncount(number_sams,.id = "sams_race_id") #will create different id later
    paste0("Total population in this representation is: ",nrow(trSAR_10))
```

Interestingly, because the total wasn't right the first time we processed this example, we were able to look for bugs in the code - in this case, not having noticed that there was an age called "Under 1 year" that needed to be made into a 0 to fit the pattern of the other ages (it's now fixed, above). There's a very practical side that drove us to triangulating - i.e., to checking things from multiple sides to see if they converged on the same answer - as well as the emerging understanding that moving between possible representations was itself a way of understanding how the objects of our inquiry were emerging as such.

We fixed that and then did one complete population for ethnicity: SET UP TABLE HERE FOR CODES, JUST LIKE DID FOR RACE; AND THEN SHOW THE AVERAGES ASSIGNED TO THE RACE, AND HOW THAT COMPARES TO THE GROUND TRUTH AT THE CENSUS LEVEL

```{r expand tract Harris SAE}
#one for Hispanic and then not Hispanic or Latino for each race; H and I do not match race codes in ACS
ethnicity_codes <- c("H","I","J","K","L","M","N","O") 
trSAE_10 <- trSARE_10 %>%
  filter(re_code %in% ethnicity_codes) %>%
  mutate(race = str_remove_all(race,", NOT HISPANIC OR LATINO")) %>%
  uncount(number_sams,.id = "sams_ethnicity_id") #will create different id later
paste0("Total population in this representation is: ",nrow(trSAE_10))
#which is conveniently the same as the totals above
nrow(trSAE_10)
#let's also clean up
trSAR_10 <- as.data.table(trSAR_10)
trSAE_10 <- as.data.table(trSAE_10)
rm(trSARE_10)
```

This table reports on 8 categories of ethnicity. The necessity to aggregate when reporting certainly drives much of the problem, but we do want to begin to disentangle some of the issues. Here are the further characteristics they offer:

```{r table for ethnicity types}
dt <- as.data.table(list(unique(trSAE_10$re_code),unique(trSAE_10$concept)))
setnames(dt, c("V1","V2"),c("re_code","concept"))
dt[order(re_code)]
rm(dt)
```

This is, perhaps, still not as straightforward as one would like, because it gives us one part of a disjunctive syllogism and there are still potentially multiple categories on the other side of the disjunction (Hispanic with three races, for example). There is a table reported as part of the redistricting data (dec/pl in the API) that has separate categories for these combinations and for a large number of combinations for multi-racial individuals, but it does not give age at the same time and doesn't help with the difficult "SOME OTHER RACE ALONE." The ACS has tables for specific ethnicities and for multiple places of origin for ancestry, but they are not given in the decennial tables, and we can't match them exhaustively because of how the ACS is reported (and constructed). For the moment, we want to stick with the initial tables, and talk about how to construct our overall approach. In Harris County, 33% of the population were categorized as "Hispanic or Latino" in 2010, and 44% in 2020. Some individual census tracts are over 95% of one race or ethnicity [[should do the calculation; it would be interesting to know]]. If we look at only race, we could get a very misleading sense of what the dynamics of a neighborhood were at the cultural level. So we wanted to somehow take the race tables, which had the virtue of a complete representation of all individuals, and add them to the ethnicity tables, which for the majority of tables are reported as only "White Alone, not Hispanic or Latino" and "Hispanic or Latino." 

Talk a little bit about how it doesn't work to just give every White person a 40% chance of also being Hispanic or Latino....
At some point, will also want to talk about using data.table vs. dyplyr / tidyr and what, in general one is looking for in rows, columns, and vectors, etc.

What we do, instead, is to order each of the sets by their shared characteristics and assign an id, with numbers assigned sequentially for rows that match on all characteristics. We create the id by ordering the collections of properties in the same way for each set, and then assigning an id. The final sampled number in the id uses all the possible numbers between 1 and the number in that cell (the base r sample function defaults to "without replace"), so that all individuals are given a unique id but in a way that takes advantage of the many ways that the two sets can be ordered internally to find equivalent representations, which in turn allow for assigning properties that were not originally assigned in the given representations. The attentive reader will note that there is no information lost here at the tract level, but that the ways in which race and ethnicity are joined at the block level could be lost.

We'll look at some of the relations in detail in the course of distributing non-White Hispanic or Latino ethnicity designations, but note immediately that very few people would have gone through the steps we just took to reconstruct those cross tabulations, even though the data is given. That the vast majority of people who answered "Some Other Race" were classified as "Hispanic or Latino" should give people pause for understanding what the composition of neighborhoods by race and ethnicity is really representing. [NPR article on "Some Other Race"](https://www.npr.org/2021/09/30/1037352177/2020-census-results-by-race-some-other-latino-ethnicity-hispanic) and [U.S. Census Bureau's explanation and tables](https://www.census.gov/library/stories/2021/08/improved-race-ethnicity-measures-reveal-united-states-population-much-more-multiracial.html), which includes a working paper, ["Race Reporting Among Hispanics: 2010"](https://www.census.gov/content/dam/Census/library/working-papers/2014/demo/shedding-light-on-race-reporting-among-hispanics/POP-twps0102.pdf). The working papers are relatively dense and strike a tone of neutrality, but are also used to justify and explain the process chosen by the Bureau, as opposed to arguing for change. The political questions are nested and thorny, of course, and one imagines that the bureaucratic stance is fully justified within the overarching political considerations of the production of the data tables. We are trying to fight against some of that bureaucratic framing, but only to understand how the choices resonate with the statistical and computational questions that are necessary precursors to using the data for analyses of social, economic, and health disparities.

In terms of the literal computations, we're doing a join, and some times the join could be done with one of the built-in merge functions in R or one of the R libraries. Here, because we want to emphasize the questions about the ordering and relations in terms of those orders, we make explicit the process of assigning numbers to each individual within a subset and then matching on those numbers to add variables across tables. As we move through the process, we'll find more precise control over the numbering within those final subtypes will matter.

```{r join trSAE and trSAR}
trSAR_10[,("sar_match_id"):=
                    paste0(tract,sex,age,race,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age,race)]
trSAE_10[,("sar_match_id"):=
                    paste0(tract,sex,age,race,as.character(100000+sample(1:.N))),
                  by=.(tract,sex,age,race)]
trSAR_10[,("ethnicity"):=
                    trSAE_10[.SD, list(re_code), on = .(sar_match_id)]]
#By the disjunctive syllogism, the ones that don't match are the ones that are Hispanic
trSAR_10[is.na(ethnicity),("ethnicity"):="H"]
length(table(trSAR_10[re_code=="A",ethnicity]))==2
length(table(trSAR_10[re_code=="B",ethnicity]))==2
length(table(trSAR_10[,ethnicity]))==8
```

Just as we did by summing up the different ways that the total populations could be represented and then comparing the outputs to determine if the two representations are equal, we can use the table function from the base R library to get the number of people in each tract in the smallest cell created by combining factors (the power set). This will tell us if we successfully represented every person from the representation in terms of ethnicity in terms of race, with the same embedding in terms of tract, sex, and age. (Could put a little bit more on why that is better captured by types designating enclosing spaces than by set membership - although, importantly, both are possible, and equivalent for practical purposes within the current problem; in long run, the question is whether more complexity solves the problem, or if the construction can point to the proper levels of complexity)

```{r trSAE trSAR join tests}
test <- table(trSAR_10[,tract],
              trSAR_10[,sex],
              trSAR_10[,age],
              trSAR_10[,ethnicity]
)==table(
  trSAE_10[,tract],
  trSAE_10[,sex],
  trSAE_10[,age],
  trSAE_10[,re_code]
)
length(test[test==F])==0
#and test that the race codes still match for the non-Hispanic or Latino population
test <- table(trSAR_10[ethnicity!="H",race])==
  table(trSAE_10[race!="HISPANIC OR LATINO",race])
length(test[test==F])==0
```

For sake of completeness, we want to take the people who are over 100 and assign them single years of age, as well. We're going to be using SAR_10 as our base for combinations, so we only do it on that one side. We're using sample with replace, so there will be a different size for the age by year every time the sampling is done. Since we want it to be reproducible, even at this level, we set the seed for the random number generator, so the random variation will be reproduced. There are only a handful of centenarians in the Harris County 2010 census and no reason to try to chase down more specifics in their true age distribution.

```{r centenarian age}
set.seed(Sam_seed)
trSAR_10[age==100,("age"):=sample(c(100,101,102,103,104),size = .N,replace = TRUE,prob = c(.29,.24,.19,.16,.12))]
trSAR_10[age==105,("age"):=sample(c(105:109),size = .N,replace = TRUE,prob = c(.29,.24,.19,.16,.12))]
trSAR_10[age==110,("age"):=sample(c(110:113),size = .N,replace = TRUE,prob = c(.6,.27,.1,.03))]
table(trSAR_10[age>100,age])
```

We know that it's fully commutative with the SAE file at the tract level, and that any specification to the block group would maintain that tract level commutativity, simply because the blocks are fully contained within the tract. We will find it helpful to draw some commutative triangles, as the tasks will get more and more complicated. In this, we just represent the fact that dividing the tables into one for ethnicity and one for race is still exhaustive on sex and age_range. We'll add more later. [Commutative Triangle: Tract level sex age race and ethnicity.](https://q.uiver.app/?q=WzAsNyxbMSwwLCJQQ1QxMlxcXFx0clNBUkUiXSxbMCwyLCJ0clNBUlxcXFxyYWNlXFxfN1xcO3hcXFxcc2V4XFw7eFxcO2FnZVxcXzExMCJdLFsyLDIsInRyU0FFXFxcXGV0aG5pY2l0eVxcXzhcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8xMTAiXSxbMSwxLCJ0b3RhbHNcXFxcc2FtZVxcO2J5XFxcXHRyYWN0XFw7YW5kXFxcXGNvbXBsZXRlXFw7YnlcXFxcYm90aFxcO3JhY2VcXDthbmRcXFxcZXRobmljaXR5Il0sWzEsMiwiXFxidWxsZXQiXSxbMiw0XSxbMCw0LCJ0clNBUlxcXFxyYWNlXFxfN1xcO3hcXFxcc2V4XFw7eFxcO2FnZVxcXzExMFxcXFxldGhuaWNpdHlcXF84Il0sWzAsMSwiZGl2aWRlcyIsMSx7ImNvbG91ciI6WzEyMCw2MCw2MF19LFsxMjAsNjAsNjAsMV1dLFswLDIsImRpdmlkZXMiLDEseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbMSwyXSxbMiwxLCJjb21tdXRlcyJdLFszLDQsInRhYmxlcyA9IiwxLHsiY29sb3VyIjpbMzAwLDYwLDYwXX0sWzMwMCw2MCw2MCwxXV0sWzIsNiwiYWRkXFw7YnlcXFxcbWF0Y2hpbmciLDEseyJjb2xvdXIiOlsxMjAsNjAsNjBdfSxbMTIwLDYwLDYwLDFdXSxbMSw2XV0=)
Latex:
[\begin{tikzcd}
	& PCT12\\trSARE \\
	& {totals\\same\;by\\tract\;and\\complete\;by\\both\;race\;and\\ethnicity} \\
	{trSAR\\race\_7\;x\\sex\;x\;age\_110} & \bullet & {trSAE\\ethnicity\_8\;x\\sex\;x\;age\_110} \\
	\\
	{trSAR\\race\_7\;x\\sex\;x\;age\_110\\ethnicity\_8} && {}
	\arrow["divides"{description}, color={rgb,255:red,92;green,214;blue,92}, from=1-2, to=3-1]
	\arrow["divides"{description}, color={rgb,255:red,92;green,214;blue,92}, from=1-2, to=3-3]
	\arrow[from=3-1, to=3-3]
	\arrow["commutes", from=3-3, to=3-1]
	\arrow["{tables =}"{description}, color={rgb,255:red,214;green,92;blue,214}, from=2-2, to=3-2]
	\arrow["{add\;by\\matching}"{description}, color={rgb,255:red,92;green,214;blue,92}, from=3-3, to=5-1]
	\arrow[from=3-1, to=5-1]
\end{tikzcd}\]

Let's set this representation of the population aside for a moment and look at the block group level. We'll follow a very similar pattern, although there are small differences that make it impossible to run it simply through the same script. 


```{r expand bgSARE, warning = FALSE}
bgSARE_10 <- bgSAR_data_10 %>%
  pivot_longer(4:ncol(bgSAR_data_10),names_to = "geoid", values_to = "number_sams",
                   names_transform = list(geoid=as.character)) %>%
      separate(label, c("total","sex","age_range"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(re_code = substr(name,5,5),
             race = str_replace(concept,"SEX BY AGE \\(",""),
             race = str_replace(race,"\\)",""),
             age_range = str_replace(age_range,"Under 5 years","0  to  4 years"), 
             age_range = str_replace(age_range,"5 to 9 years","05 to  9 years"),
             age_range = str_replace(age_range,"18 and 19 years","18 to 19 years"),
             age_range = str_replace(age_range,"20 years","20 to 20 years"),
             age_range = str_replace(age_range,"21 years","21 to 21 years"),
             age_range = str_replace(age_range,"85 years and over","85 to 110 years"),
             age_range = str_replace(age_range,"and","to"),
             first_age = as.integer(substr(age_range,1,2)),
             last_age = as.integer(substr(age_range,7,8)),
             tract = str_remove_all(geoid,"_"),
             tract = substr(tract,1,11)
             ) %>%
      filter(number_sams > 0, !is.na(age_range))
rm(bgSAR_data_10)
```

Now we filter and construct distinct tables by race and ethnicity codes.

```{r SAR expand to bg}
race_codes <- c("A","B","C","D","E","F","G")
bgSAR_10 <- bgSARE_10 %>%
  filter(re_code %in% race_codes) %>%
  uncount(number_sams,.id = "sams_race_id")
bgSAR_10 <- as.data.table(bgSAR_10)
paste0("Total population in this representation is: ",nrow(bgSAR_10))
#which is also the same as the totals above
nrow(bgSAR_10)==nrow(trSAR_10)
```

Let's put in ethnicity codes for bgSAE and expand, so that the tables from tract and block group match as far as possible, even if we don't know all the demographic specifics (age by year and race of non-White Hispanics) that had been available at the tract level. Notice that the population totals do not match; the ethnicity description is not complete for the population as it had been for race at the block group and for both race and ethnicity at the tract levels. We will have to account for that structuring choice in how the tables are reported in order to construct our own complete representation for the simulation.

```{r SAE expand to bg}
eth_codes <- c("H","I")
bgSAE_10 <- bgSARE_10 %>%
  filter(re_code %in% eth_codes) %>%
  uncount(number_sams,.id = "sams_race_id")
bgSAE_10 <- as.data.table(bgSAE_10)
paste0("Total population in this representation is: ",nrow(bgSAE_10))
#see if age_ranges line up, because sometimes they do not between ethnicity and race reporting by the Census Bureau
test <- sort(unique(bgSAE_10$age_range))==sort(unique(bgSAR_10$age_range))
length(test[test==FALSE])==0
rm(bgSARE_10)
```

Part of what's interesting, here, is that the tract level wanted to keep straight all the potential Hispanic or Latino combinations - for example, Black Hispanic is not listed in the block_group, but is available in the tract (albeit by disjunction). At the tract level, you have both "BLACK OR AFRICAN AMERICAN ALONE" and "BLACK OR AFRICAN AMERICAN ALONE, NOT HISPANIC OR LATINO"), which is what lets you impute Black and Hispanic from the remainder. Tract level reporting did not include that extra level. The race_ethnicity codes (re_code) for ethnicity is only either H or I at the block group level.

```{r table eth_codes}
dt <- as.data.table(list(unique(bgSAE_10$re_code),unique(bgSAE_10$race)))
setnames(dt, c("V1","V2"),c("re_code","concept"))
dt[order(re_code)]
rm(dt)
```

This means that the only information we know at the block group level is the number of people who are "Hispanic or Latino" and the number who are "White Alone, Not Hispanic or Latino." By the disjunctive syllogism, we can know the number of "Not White Alone "Hispanic or Latino," which will include all the subtypes available at the tract level in table PCT12.  Because we do know their sex and age_group, we can add them to the block_group by race representation on White Alone (which, in the race representation, includes many, but not all, of the Hispanic or Latino population). Since no other information is known about the individuals, we are constructing a representation that will not lose the structures of any of the originating embeddings, as reported. As we add more contexts to construct a more complicated description of the population, that task will become more difficult (and more interesting). This example is one of the simpler versions, and is not controversial. 

We note, also, that the two tables are not commutative in the way that the tract level was, because the total populations are not the same, but that the subsets for the White population are knowable. We can draw it, echoing are drawing above, [block group race and ethnicity](https://q.uiver.app/?q=WzAsNixbMSwwLCJQMTJcXFxcYmdTQVJFIl0sWzAsMiwiYmdTQVJcXFxccmFjZVxcXzdcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8yMyJdLFsyLDIsImJnU0FFXFxcXGV0aG5pY2l0eVxcXzJcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8yMyJdLFsxLDEsInRvdGFsc1xcXFxkaWZmZXJlbnRcXDtieVxcXFx0cmFjdFxcO2FuZFxcXFxvbmx5XFw7b3ZlcmxhcFxcXFxvblxcO3JhY2VcXDthbmRcXFxcZXRobmljaXR5Il0sWzEsMiwiXFxidWxsZXQiXSxbMiw0XSxbMCwxLCJkaXZpZGVzIiwxLHsiY29sb3VyIjpbMTIwLDYwLDYwXX0sWzEyMCw2MCw2MCwxXV0sWzAsMiwiZGl2aWRlcyIsMSx7ImNvbG91ciI6WzEyMCw2MCw2MF19LFsxMjAsNjAsNjAsMV1dLFsxLDJdLFsyLDEsImRvZXNcXDtub3RcXFxcY29tbXV0ZSIsMCx7ImNvbG91ciI6WzQsOTcsNjBdfSxbNCw5Nyw2MCwxXV0sWzMsNCwidGFibGVzIFxcbmVxIiwxLHsiY29sb3VyIjpbNCw5Nyw2MF19LFs0LDk3LDYwLDFdXV0=) 
Latex: 
[\begin{tikzcd}
	& P12\\bgSARE \\
	& {totals\\different\;by\\tract\;and\\only\;overlap\\on\;race\;and\\ethnicity} \\
	{bgSAR\\race\_7\;x\\sex\;x\;age\_23} & \bullet & {bgSAE\\ethnicity\_2\;x\\sex\;x\;age\_23} \\
	\\
	&& {}
	\arrow["divides"{description}, color={rgb,255:red,92;green,214;blue,92}, from=1-2, to=3-1]
	\arrow["divides"{description}, color={rgb,255:red,92;green,214;blue,92}, from=1-2, to=3-3]
	\arrow[from=3-1, to=3-3]
	\arrow["{does\;not\\commute}", color={rgb,255:red,252;green,67;blue,54}, from=3-3, to=3-1]
	\arrow["{tables \neq}"{description}, color={rgb,255:red,252;green,67;blue,54}, from=2-2, to=3-2]
\end{tikzcd}\]

#here or later??
We note, by the way, that one could encode the descriptions in terms of sets, as well. Only as the construction gets yet more complicated will the distinction between types and sets really matter. [want to get, eventually, at what it means to work through syntax vs. semantics, with a brief nod to Tarski (https://en.wikipedia.org/wiki/Semantic_theory_of_truth) and Lawvere (who took classes with Tarski for a year in Berkeley and is responding to that same question of semantic vs. syntactic in 1963) - the subtlety is that the initial idea of structural embedding I'm trying to pull out is what Tarski would have assigned to the formal idea of semantics, and that Lawvere (following Grothendieck) would have given a better mathematical grounding (although we still have to talk about what gets past the "formal" idea of semantics in Tarski's sense); this could also be the place to talk about Martin Löf, and why ZF set theory (with or without C for the axiom of choice, he is clear!), doesn't work because of membership and stratification. That the strata can be cashed out - i.e., the axiom of reducibility - doesn't need to be brought in if you're doing constructive (or intuitive) type theory] That Lawvere doesn't mention anything about "membership" in his definition of the category of set, for example in the co-authored introductory textbook, is no doubt a remnant of his time thinking about Tarski's project, and about what it means to understand mathematical structure in terms of the semantics of possible transformations in "ways of talking" as opposed to building the definition internally from axioms [Awodey or McClarty or Marquis (from a Geometrical Point of View)]

#here or later??
We should do things like talk about symmetric monoidal categories, and perhaps more broadly about Abelian groups, Lebesgue measure theory and sigma algebras: https://en.wikipedia.org/wiki/%CE%A3-algebra - just not sure where

#here or later??
why the faith in complexity, and in adding more dimensions as one slowly builds more complicated representations, comes from an idea of projecting onto a surface of individuation - not that one is against the idea of number, but against the process of taking the objects as individuated as the final arbiter of what it means for something to be! Earlier work has been used to explore the idea that Difficulty is a better way of characterizing embodied contact with the world - as opposed to an original simplicity that is complicated by adding more points of contact - and the general idea for the students in our courses has been that some of the most important questions we face shouldn't be thought of as "complicated" - they should be thought of as "difficult," or as requiring sustained attention to the forms in which things are presented, and the ways in which individuation emerges from the transformative relations that comprise society. 

As an aside that we will take up more at length, later, the race is allocated by the census taker (or sometimes an automated process) when the surveyed person refuses to give a race (or if the answer given doesn't agree with the algorithms). For the 2010 census, that activity was reported under the table, "Allocation of Race" (P46). There's not much we can do to dive into the detail about why it happened, except to note that a fairly large percentage had to be allocated, and that it was quite variable by tract. Deciding whether there is a source of error also depends on whether the ground truth would be the individual's self-designation, assuming they trusted the census-taker enough to tell them the truth, or whether there is an objective (or objectively consistent) designation that should be followed for the classification. For example, a person who prefers to be called "Latinx" would be grouped with "Hispanic or Latino" in this approach, and the political differences that were already inherent in "Hispanic" vs. "Latino" were collapsed into an awkward category that just had both names, just as "Black or African American" collapses many important distinctions about how people self-identify.[https://news.gallup.com/poll/353000/no-preferred-racial-term-among-black-hispanic-adults.aspx] The technical details for the Census Bureau's own framework can be found here: https://www.socialexplorer.com/data/C2010/metadata/?ds=SF1&table=P0460, but we will also have a more detailed conversation, below. The important thing to note is that the Census Bureau tried to conserve structures of consistency and completeness by carefully articulating rules for the data collection and ensuring that everyone fit into some category. This concern for consistency and completeness is completely justified, given the goals of the census, but we should be attentive to the other structures of representation that are not conserved but could have been, including people's right to self-identification, genetic profiles, either patrilineal or matrilineal descent, adoptive or biological descent, or self-identification with particular cultural tropes or characters from popular culture. Then, at the limit, what things are not amenable to this sort of "naming" should also be articulated explicitly, so that we can construct better ways of talking about the world than just more and more complicated names attached to individuals. 

For instance, in the tract tables, we see that there is a category for "SOME OTHER RACE ALONE" (re_code=="F") that has 583,566 people in it. Of those people, fewer than 8,000 were listed as "NOT HISPANIC OR LATINO" - just over 1%. Similarly, "TWO OR MORE RACES" (with 131,332 total and re_code=="G") has more than 60% (82494) listed as "HISPANIC OR LATINO."  Do we really imagine that the designations make the same sense to the people who are talking about themselves as it does to the Census Bureau? This table shows the cross-tabs for race and ethnicity, where not "H" indicates that they are the race listed, but "NOT HISPANIC OR LATINO."

```{r table for race and ethnicity and test for zeros}
test_table <- table(trSAR_10$re_code,trSAR_10$ethnicity)
test_table
length(test_table[test_table==0])==
  length(table(trSAR_10$ethnicity))*length(table(trSAR_10$re_code))-
  (length(table(trSAR_10$re_code))*2)
```

The final articulation we are struggling toward is understanding the goal and purpose of the distribution of names, so that we can assess whether the names assigned answer the questions that originally framed the collection and analyses of the data - or whether some particular extension retains the sense of being "about" the same data in enough ways to justify that secondary use. For the Census Bureau, there is some attempt to retain the sense of capturing how people want to describe themselves, but a greater technological imperative to ensure that the names are complete (i.e., no one is simply not assigned one of the race categories used for their analysis). 

```{r race allocated 2010}
race_allocation_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                  groupname = "P46",county_num = county,
                                                                  block="block_group",api_type="dec/sf1",path_suff="est.csv")
race_allocated_data <- race_allocation_data_from_census_10 %>%
  pivot_longer(4:ncol(race_allocation_data_from_census_10),names_to = "geoid", values_to = "totals")
race_allocated_data <- as.data.table(race_allocated_data)
race_allocated_total <- race_allocated_data[label=="Total"]
race_allocated <- race_allocated_data[label=="Total!!Allocated"]
race_allocated[,("total"):=race_allocated_total[.SD,list(as.numeric(totals)),on=.(geoid)]]
race_allocated[,("percent"):=round(as.numeric(totals)/total*100,2)]
summary(race_allocated[,percent])
paste0("number of block groups with higher than 10% attributed race is ",
nrow(race_allocated[percent>10])," and the number with 1% or less is ",
nrow(race_allocated[percent<=1]),
" of ",nrow(race_allocated))
```

There is one block group that is an outlier at 72%, with the next highest at 26%, and only 86 with higher than 10%. It would be interesting to know how those block_groups correlate with other factors about the neighborhood demographics. 

```{r join allocated to other demographics, eval = FALSE}
bgSAR_10[,("percent_allocated_race"):=race_allocated[.SD,list(percent),on=.(geoid)]]
bgSAR_10[, ("sum") := .N, by = geoid]
bgSAR_10[, ("proportion") := .N, by = c("race","geoid")]
bgSAR_10[, ("proportion") := proportion/sum]
```

```{r draw plots of percent_allocate and proportion_race, eval = FALSE}
#setDT(bgSAR_10)[, sum := NULL]
ggplot(bgSAR_10, aes(proportion,percent_allocated_race,color=re_code,group=re_code)) + #[re_code%in%c("A","B")]
  geom_smooth() +
  labs(title = "Block groups by proportion of race and number who had race allocated",color = "Race (by re_code)")
dt <- as.data.table(list(unique(bgSAR_10$re_code),unique(bgSAR_10$race)))
setnames(dt, c("V1","V2"),c("re_code","race"))
dt[order(re_code)]
ggplot(bgSAR_10, aes(proportion,sum,color=re_code,group=re_code)) +
  geom_smooth()
rm(list = ls(pattern="race_allocat"))
```

##could break out the proportion by population, too. Not sure the graphs help, at all. 

The second chart shows the proportion of a given race inside the block group, and although there may be reasons to think they correlate - perhaps because of patterns in urbanization that have also effected the choice of boundaries for block groups - it is generally meant to show what the general curves for distributions look like. The expected bell curves are not exact, but you can't claim that the first chart shows that as a block group has a higher percentage of blacks, it increases the percentage who have allocated race, until it gets to about 30%, when it starts going down again. After all, the percent allocated by race isn't reported by race, and there will be some tendency for the block groups that are grouped around the mean to have some sort of bell curve shape. Perhaps there's something to look at in depth about F - "some other race alone" - which is saying that as that percentage grows in the block group reports, the allocation by race grows. That number, after all, could easily be an artifact of how the "some other race" is collected, for example around people who consider themselves Arab, as opposed to people who say "put me down as 'some other race'". 

The graphs, frankly, bring up more questions than they answer, but they are only meant to show their limitations in this case. In the language that we are using to describe our overall process, the graphs have set rigid naming structures in place as the outer frame and structure of the visibility of the objects seen as "within" the frame. Then we have both mathematical representations (i.e. percentages) and visualizations (i.e., lines drawn) to help us think about how that conceptual frame captures variation as it is applied to smaller subsets. A large portion of the work of statistics, judging from typical publications in the admittedly subjective position of an outsider to the field, tends to be about using those techniques to make a judgment about whether the population fits the descriptive frame. If something about the population doesn't fit, then that tells us about the population. Only occasionally will one see in the publications that the fit is so obviously bad that a different conceptual frame altogether had to be used. That could be a version of publication bias - one only submits the final story, after all the frames that don't work have been tried and discarded, or one is already working inside a subfield where the decision about the conceptual frame has been well adjudicated in the founding publications for that subfield. For educational purposes, we were of the opinion that we should make these moments of early exploration and decisions about the framing be made explicit - and techniques associated with resolving the issues be explicitly taught. We felt compelled, over the course of almost a decade, to expand the scope and refine the methods, however, as the general paradigm of "testing within" the given frame obscured fundamental questions about how the details of the framing were being constructed. For example, teasing out the contribution of race to the percent of the population for whom race was allocated by the professional staff at the Census Bureau, requires constructing the conceptual space of the question with considerable care to variations - like the size of the block group and the reasons for the need to allocate - that don't rise to the level of questioning the overall frame. How, we asked ourselves, do we understand the tasks associated with constructing the conceptual frame, and where did issues like granularity and question design overlap with the more obviously dubious (but highly important) categories like race and ethnicity? 

We are consciously putting these questions to the side, with the expectation that a careful analysis of how we are constructing the mathematical objects will point to an eventual better way to construct the measures associated with demographics, and that the internal analysis we associate with classical forms of statistical analysis will find their appropriate place in the overall process. (For more philosophically-minded audiences, Van Fraassen, Bas C. The Scientific Image. Clarendon Press, 1980., provides a coherent picture of how the scientific process could mirror broadly "constructive empiricism"; there remains the question of whether that is the best path for science, but it places us within a tradition, for this first step through constructing our explicit model.) We point, by way of adumbration, to the interesting proposition from Tai-Danae Bradley [https://arxiv.org/abs/2004.05631] that the algebraic construction of probability (resulting in the reduced density operator) carries more information than the classical statistical construction, where she helpfully uses quite mundane characteristics (marginal probabilities on the colors of fruit) to show the point behind drawing in the arcane-sounding mathematics. For our purposes, she shows convincingly that a construction that conserves the algebraic commutativity retains more information than one that conserves the statistical representations in isolation.

For now, we want to focus on the prosaic process of construction. To read the data.table script, below, you'd say that for the rows in bgSAR_10 that match the White only group (re_code=="A"), assign a match_id with the components of tract, sex, age, and an appropriate random number chosen between 1 and the total number of individuals in that subgroup (the default is replace=FALSE, which is to say that all those numbers are assigned). Then you do the same for bgSAE_10. Then the folks left in each tract in the category of Hispanic or Latino should be more than the number of White Alone that are left (presumably that portion of the Hispanic population was listed as some other racial category).

With these caveats in mind, let's look at how to best move the detailed tract data onto the block_group, with some discussion of other approaches and inherent dangers to the representations. We begin with the joins on ethnicity and race that are available at the block_group level. An overall view of the first two steps, as we join what is known, can be drawn: [Add White Ethnicity to Race Tables by Block Group](https://q.uiver.app/?q=WzAsOSxbMSwwLCJiZ1NBUlxcXFxyYWNlXFxfN1xcO3hcXFxcc2V4XFw7eFxcO2FnZVxcXzIzIl0sWzMsMCwiYmdTQUVcXFxcZXRobmljaXR5XFxfMlxcO3hcXFxcc2V4XFw7eFxcO2FnZVxcXzIzIl0sWzMsMl0sWzEsMiwiV2hpdGVcXDtBbG9uZSxcXFxcbm90XFw7SGlzcGFuaWNcXFxcb3JcXDtMYXRpbm8iXSxbMSw0LCJXaGl0ZVxcO0Fsb25lLFxcXFxIaXNwYW5pY1xcXFxvclxcO0xhdGlubyJdLFszLDUsImJnU0FFXFxcXGV0aG5pY2l0eVxcXzJcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8yM1xcXFx4XFw7cmFjZVxcXzEiXSxbMSw1LCJiZ1NBUlxcXFxyYWNlXFxfN1xcO3hcXFxcc2V4XFw7eFxcO2FnZVxcXzIzXFxcXHhcXDtldGhuaWNpdHlcXF8xLjUiXSxbMCw1LCJOb3RcXDtXaGl0ZSxcXFxcSGlzcGFuaWNcXDtvclxcXFxMYXRpbm9cXFxcbm90XFw7a25vd24iLFswLDYwLDYwLDFdXSxbMSw2LCJjb3VudFxcO0hpc3BhbmljXFxcXG9yXFw7TGF0aW5vXFxcXHJlbWFpbmluZ1xcXFx4XFw7YmxvY2tcXDtncm91cFxcXFx4XFw7c2V4XFw7eFxcO2FnZSJdLFsxLDMsIkV0aG5pY2l0eT09IElcXFxcbWF0Y2hcXFxcc2V4XFw7eFxcO2FnZSIsMV0sWzAsMywicmFjZT09QSIsMV0sWzMsNCwiQnlcXDtkaXNqdW5jdGlvblxcXFxyZW1haW5pbmdcXFxcYXJlOiIsMV0sWzEsNV0sWzQsNSwibWF0Y2hcXFxcc2V4XFw7eFxcO2FnZSIsMV0sWzQsNl0sWzYsOF1d)
Latex: 
[\begin{tikzcd}
	& {bgSAR\\race\_7\;x\\sex\;x\;age\_23} && {bgSAE\\ethnicity\_2\;x\\sex\;x\;age\_23} \\
	\\
	& {White\;Alone,\\not\;Hispanic\\or\;Latino} && {} \\
	\\
	& {White\;Alone,\\Hispanic\\or\;Latino} \\
	\textcolor{rgb,255:red,214;green,92;blue,92}{Not\;White,\\Hispanic\;or\\Latino\\not\;known} & {bgSAR\\race\_7\;x\\sex\;x\;age\_23\\x\;ethnicity\_1.5} && {bgSAE\\ethnicity\_2\;x\\sex\;x\;age\_23\\x\;race\_1} \\
	& {count\;Hispanic\\or\;Latino\\remaining\\x\;block\;group\\x\;sex\;x\;age}
	\arrow["{Ethnicity== I\\match\\sex\;x\;age}"{description}, from=1-4, to=3-2]
	\arrow["{race==A}"{description}, from=1-2, to=3-2]
	\arrow["{By\;disjunction\\remaining\\are:}"{description}, from=3-2, to=5-2]
	\arrow[from=1-4, to=6-4]
	\arrow["{match\\sex\;x\;age}"{description}, from=5-2, to=6-4]
	\arrow[from=5-2, to=6-2]
	\arrow[from=6-2, to=7-2]
\end{tikzcd}\]

```{r join bgSAE and bgSAR}
bgSAR_10[re_code=="A",("sar_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[re_code=="I",("sar_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAR_10[re_code=="A",("ethnicity"):=
                    bgSAE_10[.SD, list(re_code), on = .(sar_match_id)]]
#move back to SAE, so we can track sex and age_range combinations that have been eliminated
bgSAE_10[re_code=="I",("r_code"):=
                    bgSAR_10[.SD, list(re_code), on = .(sar_match_id)]]
nrow(bgSAR_10[!is.na(ethnicity)&re_code=="A"])==nrow(bgSAE_10[re_code=="I"])
nrow(bgSAE_10[re_code=="I"])==nrow(bgSAE_10[!is.na(r_code)])
test <- table(bgSAR_10[re_code=="A"&!is.na(ethnicity),geoid],
              bgSAR_10[re_code=="A"&!is.na(ethnicity),sex],
              bgSAR_10[re_code=="A"&!is.na(ethnicity),age_range]
) == table(
  bgSAE_10[re_code=="I",geoid],
  bgSAE_10[re_code=="I",sex],
  bgSAE_10[re_code=="I",age_range]
)
length(test[test==FALSE])==0
#all the individuals who identify as both White and Hispanic or Latino are accounted for
```

Having assigned the part of the population that we know to be White Alone, Not Hispanic or Latino at the block group level, we can now give the White Alone, Hispanic or Latino an "H" for ethnicity and mark off the Hispanics in the ethnicity block group tables by those sex and age_ranges. Since the groups are complements of each other (adjoint), and there are only a few things ascribed to each table, the subsetting can be done without loss of information. 

```{r assign rest of White alone ethnicity and do counts for Hispanic or Latino remaining in block}
bgSAR_10[is.na(ethnicity)&re_code=="A",("ethnicity"):="H"]
#put a marker back on SAE, so that age and sex for the others stays same
bgSAR_10[re_code=="A"&ethnicity=="H",("sar2_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[re_code=="H",("sar2_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[re_code=="H",("r_code"):=
                    bgSAR_10[.SD, list(re_code), on = .(sar2_match_id)]]
#check all moved over for White Alone rows
nrow(bgSAE_10[!is.na(r_code)])==nrow(bgSAR_10[re_code=="A"])
#Now let's do some tests first to ensure we've got what we think we have
nrow(bgSAR_10[ethnicity!="I"&re_code=="A"])==nrow(trSAR_10[ethnicity!="I"&re_code=="A"])
nrow(bgSAR_10[is.na(ethnicity)&re_code=="A"])==0
#SAE_Harris (tract) for just H and I has the same total size as bgSAE for blocks.
nrow(trSAE_10[re_code%in%c("H","I")])==nrow(bgSAE_10)
#check that White Alone has right number totals on both sides
nrow(bgSAE_10[re_code=="H"&!is.na(r_code)])+nrow(bgSAE_10[re_code=="I"])==
  nrow(bgSAR_10[re_code=="A"])

#move number of total non-White Hispanic or Latino onto all rows in block by sex and age_range
bgSAE_10[is.na(r_code),("H_block"):=.N,by=.(geoid,sex,age_range)]
bgSAE_10[is.na(r_code),("H_tract"):=.N,by=.(tract,sex,age_range)]
bgSAR_10[re_code!="A",("sar3_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAE_10[is.na(r_code),("sar3_match_id"):=
                    paste0(geoid,sex,age_range,as.character(100000+sample(1:.N))),
                  by=.(geoid,sex,age_range)]
bgSAR_10[re_code!="A",c("H_block"):= 
                    bgSAE_10[.SD, list(H_block), on = .(sar3_match_id)]]
summary(bgSAR_10[,H_block])==summary(bgSAE_10[,H_block]) #should be TRUE for all except NA's
nrow(bgSAE_10[!is.na(H_block)])==nrow(bgSAR_10[!is.na(H_block)])
#put the number remaining Hispanic or Latino by block group, sex, and age_range on every row in SAR without ethnicity
bgSAR_10[is.na(H_block),("H_block"):=0] #something about .SD working with NAs is funky
bgSAR_10[order(-H_block),("bg_H_remain"):=.SD[1,H_block], #should be only one number per group, but don't know what row it's in
         by=.(geoid,sex,age_range)]
#some summaries and tests
paste0("number of Hispanic or Latino who are not White alone: ",
       nrow(bgSAE_10)-nrow(bgSAR_10[!is.na(ethnicity)]))
#which should equal the same in the tract table
nrow(trSAR_10[ethnicity=="H"])-nrow(trSAR_10[re_code=="A"&ethnicity=="H"])==
  nrow(bgSAE_10)-nrow(bgSAR_10[!is.na(ethnicity)])
#the ones who are neither H nor I are all the non-White categories minus the Hispanics
H_A <- nrow(bgSAR_10[ethnicity=="H"&re_code=="A"])
paste0("The number of people listed as White by race and Hispanic or Latino by ",
       "ethnicity at the block level, ",
       H_A,", should equal the total from tracts")
H_A==nrow(trSAR_10[re_code=="A"])-nrow(trSAR_10[ethnicity=="I"])
#which means the complete White population has ethnicity assigned
paste0("This leaves ",nrow(bgSAE_10[re_code=="H"]) - H_A," non-White, Hispanic or Latino,",
       " out of ",nrow(bgSAR_10[re_code!="A"])," non-White individuals. Of that remaining Hispanic or Latino population, ", nrow(trSAR_10[re_code=="F"&ethnicity=="H"]), 
       " consider themselves Some Other Race Alone")
#also check to see that the number of non-White Hispanic or Latino matches
nrow(trSAR_10[ethnicity=="H"&re_code!="A"])==nrow(bgSAE_10[is.na(r_code)])
```

We can add in the tract data at this point, matching on ethnicity, with race assigned within the subgroups where we don't know ethnicity by random sample, but there's a potential that some of the folks (1,774,203) who don't have ethnicity assigned at the block group will be misassigned within the tract to the wrong block group. We've been able to identify non-White Hispanic or Latino individuals (702,930), but don't know how they distribute by race by block group (only by tract). That is, we can easily create a commutative representation grounded in both race and ethnicity at the tract level, but it might break with other commutative relations at other levels that depend on how race is combined with ethnicity at the block group level. 

The temptation is to add the tract and group level data together. We have a bit more information to account for, though, and some other sources for triangulation. We know how many people are Hispanic in each block group, which means that we also know how many are non-Hispanic, but we don't know which races should also be listed as Hispanic. We do know where races are by block group, however. We could estimate by calculating the statistical likelihood of race x ethnicity x age for each race at the tract level and assigning it to each race x age at the block level. Surely better than a simple random distribution - it takes into account some strong priors - it still favors the individual calculation of likelihood to encounter an individual with certain characteristics instead of maintaining the structure of relations between race and ethnicity at the block group level. If, instead, we think of every relation at the tract level as given, so there is a total number of people with each combination of race x ethnicity x age at the tract level, and we know the White populations distribution at the block level, what you have is not the percentage that are in each block group, but the actual number that the block group provides, and our issue is to distribute them. 

We have each race x age total and each race x age, non-Hispanic at tract - so know Hispanic by disjunction at tract, but not what individuals to attach them to at the block group. This is a recurring issue for later, of course. 

We have each race x age total and White x age x ethnicity at the block group - so know non-White x ethnicity by disjunction. 
Could create a small data.table with the white x age x ethnicity and then the Hispanics and non-Hispanics counted out at the block level; then join that back to the tract level to pick up the race x age x ethnicity distribution at that level; then back to the bg. Are there any other places to test for lost relationality? Should we run a quick test on the subsection of race and re_code to demonstrate no loss before setting to "temp"? 

Point is that we get the overall distribution by tract solid; the HI non_HI then gives structure at block level; the particular race/eth combinations still come from tract level, but given that HI is known at block level, it should be roughly equivalent (if not equal):

[Assigning Hispanic or Latino for non-White Population by Block and Tract](https://q.uiver.app/?q=WzAsMTIsWzEsMCwiYmdTQVJcXFxccmFjZVxcXzdcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8yMyJdLFszLDAsImJnU0FFXFxcXGV0aG5pY2l0eVxcXzJcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8yMyJdLFszLDJdLFsxLDIsIldoaXRlXFw7QWxvbmUsXFxcXG5vdFxcO0hpc3BhbmljXFxcXG9yXFw7TGF0aW5vIl0sWzEsNCwiV2hpdGVcXDtBbG9uZSxcXFxcSGlzcGFuaWNcXFxcb3JcXDtMYXRpbm8iXSxbMyw1LCJiZ1NBRVxcXFxldGhuaWNpdHlcXF8yXFw7eFxcXFxzZXhcXDt4XFw7YWdlXFxfMjNcXFxceFxcO3JhY2VcXF8xIl0sWzEsNSwiYmdTQVJcXFxccmFjZVxcXzdcXDt4XFxcXHNleFxcO3hcXDthZ2VcXF8yM1xcXFx4XFw7ZXRobmljaXR5XFxfMS41Il0sWzAsNSwiTm90XFw7V2hpdGUsXFxcXEhpc3BhbmljXFw7b3JcXFxcTGF0aW5vXFxcXG5vdFxcO2tub3duIixbMCw2MCw2MCwxXV0sWzEsNiwiY291bnRcXDtIaXNwYW5pY1xcXFxvclxcO0xhdGlub1xcXFxyZW1haW5pbmdcXFxceFxcO2Jsb2NrXFw7Z3JvdXBcXFxceFxcO3NleFxcO3hcXDthZ2UiXSxbMiw2LCJjb3VudFxcO2lzXFxcXHJlc3VsdFxcO29mXFxcXHN0cnVjdHVyZVxcXFxvblxcO2Fkam9pbnRcXFxcc2lkZSIsWzAsNjAsNjAsMV1dLFszLDYsImJsb2NrXFw7c3RydWN0dXJlXFxcXHdpdGhpblxcXFx0cmFjdHNcXFxcY3JlYXRlc1xcXFxkaXN0cmlidXRpb24iXSxbMSw3LCJ0cmFjdFxcO2xldmVsXFxcXGdpdmVzXFxcXGluZGl2aWR1YWxcXFxcdmFsdWVzXFxcXGRpc3RyaWJ1dGVkXFxcXHdpdGhcXDtzdHJ1Y3R1cmUiXSxbMSwzLCJFdGhuaWNpdHk9PSBJXFxcXG1hdGNoXFxcXHNleFxcO3hcXDthZ2UiLDFdLFswLDMsInJhY2U9PUEiLDFdLFszLDQsIkJ5XFw7ZGlzanVuY3Rpb25cXFxccmVtYWluaW5nXFxcXGFyZToiLDFdLFsxLDVdLFs0LDUsIm1hdGNoXFxcXHNleFxcO3hcXDthZ2UiLDFdLFs0LDZdLFs2LDhdLFs4LDExLCJkZXRlcm1pbmUiLDFdXQ==)

The idea is that you want the values to have been determined by the structure provided from both sides, with the values assigned from the one that constrains toward the individual values. [[it's a weird question about what the precise language should be]] Perhaps it's more like the idea that you have channels, like sieves, that come from different structures, but attach to the row as it passes through the alluvial. 

What we mean by structure is embodied in how the names can circulate in relation to the named - not the Kantian (as diagnosed by Sellars) moment of simple and single contact with the noumenal, but a channel within which the names circulate according to a structure. That is, we are not trying to find a place where the name is correct, but where the names have a logic of application and the structure that makes that application make sense is captured. This allows for the movement between different ways of naming to make sense. Here, we do it one at a time, so that the determination from the ethnicity as given in the table for tracts has the relation to race moved over from the block group tables. When we match block and ethnicity from the block group level for the group quarters, below, we are combining the two structured ways of naming by capturing how one moves between them. As we find more difficult situations, that sense of movement will allow us to speak to the sense of a dynamic dialogue between different ways of naming that demonstrates more capacity for eliciting and understanding structures. We are not trying to become relativists by refusing the idea of naming as the place where a truth is instantiated, but showing that ways of naming allow us to see structures that encompass multiple strategies for structuring how the names are attached, and to see the comparison in algebraic or topological terms (as ways of combining structures) and not as a single action of determination in an individual's act of naming what is as the result of an act of naming from a determined perspective on the whole. The structure for ethnicity by race is highly determinative for "Some Other Race" (almost 99% in Harris County), but not exhaustive, and just less than two-thirds for "Two or More Races," with a large cohort for "Black or African American" and "Hispanic or Latino" (21,234 people, which is just less than 3% of all "Black or African American").  

```{r add age_range onto trSAR so that it matches}
bgSAR_10[order(tract,first_age),("age_match_id"):=
                    paste0(tract,sex,re_code,as.character(100000+seq.int(1:.N))),
                  by=.(tract,sex,re_code)]
trSAR_10[order(tract,age),("age_match_id"):=
                    paste0(tract,sex,re_code,as.character(100000+seq.int(1:.N))),
                  by=.(tract,sex,re_code)]
trSAR_10[,("age_range"):=
                    bgSAR_10[.SD, list(age_range), on = .(age_match_id)]]
test <- table(bgSAR_10[,tract],
              bgSAR_10[,sex],
              bgSAR_10[,re_code],
              bgSAR_10[,age_range]
              )==
        table(trSAR_10[,tract],
              trSAR_10[,sex],
              trSAR_10[,re_code],
              trSAR_10[,age_range]
              )
length(test[test==FALSE])==0
```

There is some possibility, in the age matching, that someone from one of the block groups has an ethnicity that will be misaligned with the ages by race, since we weren't matching on ethnicity (it isn't known for the non-White population at the block group levels, after all). That would be a narrow window for the difference, however, and we will be able to show that no known information is lost, even if there is some potential for wrong attribution at the block level. This is not the same as "overfitting" a model, the process that happens when you tailor a model to a subset in a way that cannot be sustained for the larger group. What we are doing comes from the other "direction." The specificity is given in the tables, and we are attempting to reconstruct an unknown model, where we know something about the structure because it was used to create the tables, but we do not know the underlying distribution in perfect detail. We are not, in other words, trying to guess at the underlying structure by inducing the algorithm (in this case, the simple subsetting by race and ethnicity) and then applying that same algorithm to other parts of the population. Instead, we are looking at how the adjoint structures of the subsets provide clues to how specific representations were made, and how those specific (but unknown) tables must have been in order to operate in the way that they do. We don't look for a characteristic algorithm and attempt to apply it, or ask whether it is proper to apply it; we look for the structures that the algorithm conserved (in this case, the commutativity of the representation of the individuals), and see how those structures in their totality (including things like adjoint relations, which include disjunctive syllogisms as their basic parts in logic) can be used to understand that unknown distribution.    

Now, in order to combine the structures we have identified, we join the tract table to the block groups, by age_range but with count by race with Hispanic or Latino Remaining. 

If we order by H_tract remaining in tract by re_code and sample from "notHvI" on the block_group, they should match

```{r join bgSAR with trSAR for ethnicity by block group}
#move H_re_code from trSAR to bgSAR
trSAR_10[ethnicity=="H"&re_code!="A",("H_re_code"):=.N,by=.(tract,sex,age_range,re_code)]
#then merge
trSAR_10[ethnicity=="H"&re_code!="A",("ethSAR_match_id"):=
           paste0(tract,sex,age_range,re_code,as.character(100000+sample(1:.N))),
           by=.(tract,sex,age_range,re_code)]
bgSAR_10[re_code!="A",("ethSAR_match_id"):=
           paste0(tract,sex,age_range,re_code,as.character(100000+sample(1:.N))),
           by=.(tract,sex,age_range,re_code)]
bgSAR_10[re_code!="A",("H_re_code"):=
                    trSAR_10[.SD, list(H_re_code), on = .(ethSAR_match_id)]]
trSAR_10[ethnicity=="H"&re_code!="A",("ethSAR_match_poss"):=
           bgSAR_10[.SD, list(H_re_code), on = .(ethSAR_match_id)]]
#test that they all moved over
nrow(trSAR_10[!is.na(ethSAR_match_poss)])==nrow(!is.na(bgSAR_10[!is.na(H_re_code)]))
bgSAR_10[re_code!="A"&is.na(H_re_code),("H_re_code"):=0]
bgSAR_10[order(-H_re_code),("H_re_code"):=.SD[1,H_re_code],
         by=.(tract,sex,age_range,re_code)]
paste0("We know by elimination that ",nrow(bgSAR_10[re_code!="A"&H_re_code==0]),
       " are not possible matches on tract, sex, age, race (re_code).")
paste0("Taken together with the ones where multiple matches are possible (",
       nrow(bgSAR_10[re_code!="A"&H_re_code!=0]),") minus the ones that needed to match (",
       nrow(trSAR_10[!is.na(ethSAR_match_poss)]),"), we get: ",
       nrow(bgSAR_10[re_code!="A"&H_re_code==0])+
         nrow(bgSAR_10[re_code!="A"&H_re_code!=0])-
         nrow(trSAR_10[!is.na(ethSAR_match_poss)]),
       " which equals the number of rows that don't have ethnicity of H or I: ",
       nrow(trSAR_10[!ethnicity%in%c("H","I")]))
```

```{r assign HvI and order bgSAR by block group with number left and count misaligned re_code and ethnicity}
setorder(bgSAR_10,-bg_H_remain,-H_re_code)
bgSAR_10[re_code!="A",("HvI"):=if_else(1:.N>bg_H_remain,"notHvI","HvI"),by=.(geoid,sex,age_range)]
trSAR_10[re_code!="A",("HvI"):=if_else(ethnicity=="H","HvI","notHvI")]
#a few tests
table(bgSAR_10[,HvI])==table(trSAR_10[,HvI])
test <- table(bgSAR_10[,tract],
              bgSAR_10[,sex],
              bgSAR_10[,age_range],
              bgSAR_10[,HvI])==
  table(trSAR_10[,tract],
        trSAR_10[,sex],
        trSAR_10[,age_range],
        trSAR_10[,HvI])
length(test[test==FALSE])==0
test <- table(bgSAR_10[,tract],
              bgSAR_10[,sex],
              bgSAR_10[,age_range],
              bgSAR_10[,re_code])==
  table(trSAR_10[,tract],
        trSAR_10[,sex],
        trSAR_10[,age_range],
        trSAR_10[,re_code])
length(test[test==FALSE])==0
#however, the re_code is not correctly matched to HvI
test <- table(bgSAR_10[,tract],
              bgSAR_10[,re_code],
              bgSAR_10[,HvI])==
  table(trSAR_10[,tract],
        trSAR_10[,re_code],
        trSAR_10[,HvI])
length(test[test==FALSE])==0
#we can measure that difference by counting how many don't match and dividing it by the number we tried to match:
#(which we could compare with other methods of sorting, naive joins, etc., if time)
sum_missed <- sum(abs(table(bgSAR_10[,tract],
              bgSAR_10[,re_code],
              bgSAR_10[,HvI])==
  table(trSAR_10[,tract],
        trSAR_10[,re_code],
        trSAR_10[,HvI])))
sum_missed
#percent of whole:
round(sum_missed /
  nrow(trSAR_10[ethnicity=="H"&re_code!="A"]),5)*100
#but what it has done is put the block structure somewhat in place by race
bgSAR_10[is.na(ethnicity),("ethSAR_id"):=
           paste0(tract,sex,age_range,re_code,HvI,as.character(100000+sample(1:.N))),
         by=.(tract,sex,age_range,re_code,HvI)]
trSAR_10[re_code!="A",
         ("ethSAR_id"):=
           paste0(tract,sex,age_range,re_code,HvI,as.character(100000+sample(1:.N))),
           by=.(tract,sex,age_range,re_code,HvI)]
bgSAR_10[is.na(ethnicity),("ethnicity"):=
                    trSAR_10[.SD, list(ethnicity), on = .(ethSAR_id)]]
trSAR_10[re_code!="A",
         ("ethSAR_match"):=
           bgSAR_10[.SD, list(H_re_code), on = .(ethSAR_id)]]
nrow(trSAR_10[!is.na(ethSAR_match) | re_code=="A"])==nrow(bgSAR_10[!is.na(ethnicity)])
paste0("There are a relatively few (",nrow(trSAR_10[is.na(ethSAR_match)&re_code!="A"])," or ",
       round(nrow(trSAR_10[is.na(ethSAR_match)&re_code!="A"])/nrow(trSAR_10),5)*100,
       "%) that didn't match. We take the relation of ethnicity to race as canonical, however,", 
       "so finish assigning those from trSAR. Luckily, none of the unmatched are Hispanic or Latino (",
       nrow(trSAR_10[is.na(ethSAR_match)&re_code!="A"&ethnicity=="H"])==0, ") so we can just join without HvI")
paste0("It is also worth just looking at how this ordering distributed ethnicity, first remembering tract level race by ethnicity: ")
table(trSAR_10$re_code,trSAR_10$ethnicity)
paste0("Then the block level up to this point - which looks visually to be in the right direction: ")
table(bgSAR_10$re_code,bgSAR_10$ethnicity)
```

```{r finish assigning ethnicity to bg and do tests}
bgSAR_10[is.na(ethnicity),("ethSAR2_id"):=
           paste0(tract,sex,age_range,re_code,as.character(100000+sample(1:.N))),
         by=.(tract,sex,age_range,re_code)]
trSAR_10[is.na(ethSAR_match)&re_code!="A",
         ("ethSAR2_id"):=
           paste0(tract,sex,age_range,re_code,as.character(100000+sample(1:.N))),
           by=.(tract,sex,age_range,re_code)]
bgSAR_10[is.na(ethnicity),("ethnicity"):=
                    trSAR_10[.SD, list(ethnicity), on = .(ethSAR2_id)]]
test <- table(bgSAR_10[,tract],
              bgSAR_10[,sex],
              bgSAR_10[,age_range],
              bgSAR_10[,re_code],
              bgSAR_10[,ethnicity])==
  table(trSAR_10[,tract],
        trSAR_10[,sex],
        trSAR_10[,age_range],
        trSAR_10[,re_code],
        trSAR_10[,ethnicity])
length(test[test==FALSE])==0
#test with bgSAE to see how far off it is - first, double check on tract
test <- table(bgSAR_10[ethnicity%in%c("H","I"),tract],
              bgSAR_10[ethnicity%in%c("H","I"),sex],
              bgSAR_10[ethnicity%in%c("H","I"),age_range],
              bgSAR_10[ethnicity%in%c("H","I"),ethnicity])==
  table(bgSAE_10[,tract],
        bgSAE_10[,sex],
        bgSAE_10[,age_range],
        bgSAE_10[,re_code])
length(test[test==FALSE])==0
#then get count of how many are off within the block groups
sum(abs(table(bgSAR_10[ethnicity=="I",geoid],
              bgSAR_10[ethnicity=="I",sex],
              bgSAR_10[ethnicity=="I",age_range])-
  table(bgSAE_10[re_code=="I",geoid],
        bgSAE_10[re_code=="I",sex],
        bgSAE_10[re_code=="I",age_range])))==0
bg_sum_eth <- sum(abs(table(bgSAR_10[ethnicity=="H",geoid],
              bgSAR_10[ethnicity=="H",sex],
              bgSAR_10[ethnicity=="H",age_range])-
  table(bgSAE_10[re_code=="H",geoid],
        bgSAE_10[re_code=="H",sex],
        bgSAE_10[re_code=="H",age_range])))
#which is still less than 1% misaligned on Hispanic or Latino (or not) for non-White individuals
bg_sum_eth/nrow(bgSAE_10[re_code=="H"])
```

That some of the ones that don't match may also be a function of small misalignments that are possible by age_range...

Somewhere above, about Type theory and computability - the only operations allowed to do the matching are things that you can place on a single row and then can match with a different table uniquely by that row. That is a complement or adjoint of the sense of complementarity itself.... You can give a whole group a name, and then count each individual, and you can do things that allow you to count multiple times as you subset, but each individual has to be somehow given a value, so that the row can be assessed. That this is also part of what drives imperative programming and nominalistic epistemology to think that the task is to test whether a name is correctly used as opposed to asking what patterns emerge. (think of difference between adding in imperative and functional languages, or in Peano vs. Church, and the order of calculation; and then how that means that our question, here, is different)

```{r clean up bgSAR and extra files}
rm(bgSAE_10)
rm(trSAE_10)
rm(trSAR_10)
bgSAR_10[,c("name","label","total","concept","sams_race_id","sar_match_id",
            "sar2_match_id","sar3_match_id","H_block","bg_H_remain","age_match_id",
            "ethSAR_match_id","H_re_code","HvI","ethSAR_id","ethSAR2_id"):=NULL]
if (file.exists(paste0(censusdir,vintage,"/working/bgSAR_10.RDS"))){
  rm(paste0(censusdir,vintage,"/working/bgSAR_10.RDS"))
}
if (file.exists(paste0(censusdir,vintage,"/working"))){
    print(sprintf("found folder %s", paste0(censusdir,vintage,"/working")))
    saveRDS(bgSAR_10,paste0(censusdir,vintage,"/working/bgSAR_10.RDS"))
  }else{
    dir.create(paste0(censusdir,vintage,"/working"))
    print(sprintf("created folder %s", paste0(censusdir,vintage,"/working")))
    saveRDS(bgSAR_10,paste0(censusdir,vintage,"/working/bgSAR_10.RDS"))
  }

```

Continue with making_sam_bg_hh
