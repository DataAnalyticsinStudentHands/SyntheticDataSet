---
title: "Making Sam Households"
author: "Dan Price"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
geometry: margin=1in
fontsize: 11pt
documentclass: article
header-includes: 
  - \usepackage{tikz}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Preliminaries -- very broad
https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html for options on the knit - results="hide"; echo=FALSE; include=FALSE, etc.

```{r prelims}
source('~/Documents/Projects/SyntheticDataSet/BaseScripts/Census_Data.R')
library(tidyr)
library(dplyr)
library(stringr)
library(data.table)
#maindir = "~/University Of Houston/Price, Daniel M - Social Network Hypergraphs/"
maindir = "~/Downloads/UH_OneDrive/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at home
#maindir = "~/Downloads/OneDrive\ -\ University\ Of\ Houston/Social\ Network\ Hypergraphs/" #Dan at work
housingdir = paste0(maindir,"HCAD/")
houstondatadir = paste0(maindir,"HoustonCityData/") 
censusdir = paste0(maindir,"Census/") 
vintage = "2019"
housingStockFromRDS = TRUE 
#numberOfCores = 1
state = 48 #48 Texas; 22 Louisiana
county = 201 #8 county region: 201 Harris; 157 Fort Bend; 167 Galveston; 039 Brazoria; 071 Chambers; 291 Liberty; 339 Montgomery; 473 Waller ; other place FIPS are longer
tract = "*"
Sam_seed = 135
#you don't need a censuskey if you're not pulling new files down; you can only use this one if you have correct access to the OneDrive
censuskey <- readLines(paste0(censusdir, "2017", "/key"))
```


For new file, and need a bit of a preamble

We're going to begin with the 2010 block group data table, P28 - Householdtype by Household Size by Race and Ethnicity. We start here not because it provides the greatest granularity, although it is by block group, but because it has the best structured relationality, around which commutative triangles can be built. The longer idea is to see the census bureau's copy of the full data representation as being both perfectly commutable in the ways it can be represented and granular at an individual level (again, setting aside difficulties with their collection and category choice). There are then ways in which the census bureau produces tables, with different concerns at different stages and with different products, but they both try to ensure the structure of commutability is maintained within the particular representation and to ensure that the people interpreting the data understand how it relates statistically as a subset - which is to say, how the granularity of the presentation is related to the underlying granularity as the chance that when you see someone from a particular subset, they fit into the demographics of the larger group. [that could be better phrased, but trying to get at difference between set theory and type theory with the categorical spin]

Section title - Structure and representation

Let's download P28 (Household Type by Size by Race and Ethnicity) and look at it.

```{r download hhtype hhsize race eth bg}
dec_bgHH_type_size_re_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P28",county_num = "201",
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
```

Then do our basic checks:
```{r hh_type_size_re test problems}
dec_bgHH_data_10 <- as.data.table(dec_bgHH_type_size_re_data_from_census_10)
    percent_na <- dec_bgHH_data_10[,sum(is.na(.SD))] / 
      (dec_bgHH_data_10[,sum(!is.na(.SD))]+dec_bgHH_data_10[,sum(is.na(.SD))])
    paste("Percentage of NAs in file:",as.integer(100*percent_na))
#make the numbers into numeric type - some operations won't work with either integer or character, although we could play with optimizing it
    dec_bgHH_data_10[,4:ncol(dec_bgHH_data_10)] <- dec_bgHH_data_10[,lapply(.SD[,4:ncol(dec_bgHH_data_10)], as.numeric)]
#multiplying the side labeled only "Total" by 2:
test <- colSums(dec_bgHH_data_10[label=="Total",4:ncol(dec_bgHH_data_10)])*2 ==
      colSums(dec_bgHH_data_10[label!="Total",4:ncol(dec_bgHH_data_10)])
#see if any of the tests don't match; if false, need to go back and check on what happened
length(test[test==F])==0
hh_totals_bg <- dec_bgHH_data_10[label=="Total"&concept=="HOUSEHOLD TYPE BY HOUSEHOLD SIZE",4:ncol(dec_bgHH_data_10)]
paste0("Number of Households total for Harris County by block_group: ", sum(hh_totals_bg[,]))
```

Now we need to convert the census representation into a representation of individual households.

```{r expand P28 into dec_bgHH_10}
    race_codes <- c("A","B","C","D","E","F","G")
    dec_bgHHr_10 <- dec_bgHH_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      mutate(
        race = substr(name,5,5),
        label = str_remove_all(label,"Total!!")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      separate(label, c("family","hh_size"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(hh_size_n = as.numeric(substr(hh_size,1,1))) %>%
      filter(!is.na(hh_size) & race%in%race_codes) %>% #to get rid of aggregations by family
      uncount(number_sams,.id = "hhr_size_id",.remove = TRUE) 
    dec_bgHHr_10 <- as.data.table(dec_bgHHr_10) #dyplyr had stripped it of dt
    paste0("Number of households in file: ", nrow(dec_bgHHr_10))
    #break into race and ethnicity files
    nrow(dec_bgHHr_10)==sum(hh_totals_bg[,])
    #and same for ethnicity
    dec_bgHHe_10 <- dec_bgHH_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      mutate(
        ethnicity = substr(name,5,5),
        label = str_remove_all(label,"Total!!")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      separate(label, c("family","hh_size"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(hh_size_n = as.numeric(substr(hh_size,1,1))) %>%
      filter(!is.na(hh_size) & ethnicity%in%c("H","I")) %>% 
      uncount(number_sams,.id = "hhe_size_id",.remove = TRUE) 
    dec_bgHHe_10 <- as.data.table(dec_bgHHe_10) #dyplyr had stripped it of dt
    paste0("Number of households with ethnicity attributes in file: ", nrow(dec_bgHHe_10))
    #should clean up
    rm(dec_bgHH_data_10)
    rm(dec_bgHH_type_size_re_data_from_census_10)
```

sub-section - Triangle Commutativity.

Could talk about history of Symmetric Monoidal Categories, and how they are based in that original thought about triangles and enabling algebraic structures like commutativity. Could also do a diagram or two here.

One of our basic tools for constructing Sam City is using a relation as the unit that we add to, with the individual level (in this case a household) serving as an endpoint for the relation, and something that only fully emerges as more and more of the relations are pulled together. In that vein, we turn to H16, tenure (rent or own) by household size by race and ethnicity. Since the relations between household size and race and ethnicity are the same for each individual, we can build the city from the fact that both share an edge of a triangle, which is commutative at the type level of the household in this case. 

Let's download H16 and look at it for some clues. If the relation between household size and race or ethnicity matches on both data tables, then we'll get the relationship from tenure to household type for free - without having introduced any uncertainty into the construction, at this point, although we'll have to ensure that there aren't illegitimate (or unaccounted for) steps in further building down the line.

```{r download tenure hhsize race eth bg}
dec_bgHH_tenure_size_re_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "H16",county_num = "201",
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
```

Then do our basic checks:
```{r household tenure size re test problems}
dec_bgHH_tenure_data_10 <- as.data.table(dec_bgHH_tenure_size_re_data_from_census_10)
    percent_na <- dec_bgHH_tenure_data_10[,sum(is.na(.SD))] / 
      (dec_bgHH_tenure_data_10[,sum(!is.na(.SD))]+dec_bgHH_tenure_data_10[,sum(is.na(.SD))])
    paste("Percentage of NAs in file:",as.integer(100*percent_na))
#make the numbers into numeric type - some operations won't work with either integer or character, although we could play with optimizing it
    dec_bgHH_tenure_data_10[,4:ncol(dec_bgHH_tenure_data_10)] <- dec_bgHH_tenure_data_10[,lapply(.SD[,4:ncol(dec_bgHH_tenure_data_10)], as.numeric)]
#multiplying the side labeled only "Total" by 2:
test <- colSums(dec_bgHH_tenure_data_10[label=="Total",4:ncol(dec_bgHH_tenure_data_10)])*2 ==
      colSums(dec_bgHH_tenure_data_10[label!="Total",4:ncol(dec_bgHH_tenure_data_10)])
#see if any of the tests don't match; if false, need to go back and check on what happened
length(test[test==F])==0
hh_totals_bg <- dec_bgHH_tenure_data_10[label=="Total"&concept=="TENURE BY HOUSEHOLD SIZE",4:ncol(dec_bgHH_tenure_data_10)]
paste0("Number of Households total for Harris County by block_group: ", sum(hh_totals_bg[,]))
nrow(dec_bgHHr_10)==sum(hh_totals_bg[,])
```
And expand in a way that is very similar to what we did with household type and size by race and ethnicity. 

```{r expand H16 into dec_bgHH_tenure_10}
    race_codes <- c("A","B","C","D","E","F","G")
    dec_bgHHr_tenure_10 <- dec_bgHH_tenure_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_tenure_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      mutate(
        race = substr(name,5,5),
        label = str_remove_all(label,"Total!!")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      separate(label, c("tenure","hh_size"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(hh_size_n = as.numeric(substr(hh_size,1,1))) %>%
      filter(!is.na(hh_size) & race%in%race_codes) %>% #to get rid of aggregations by family
      uncount(number_sams,.id = "hhr_tenure_id",.remove = TRUE) 
    dec_bgHHr_tenure_10 <- as.data.table(dec_bgHHr_tenure_10) #dyplyr had stripped it of dt
    paste0("Number of households in file: ", nrow(dec_bgHHr_tenure_10))
    #break into race and ethnicity files
    nrow(dec_bgHHr_tenure_10)==sum(hh_totals_bg[,])
    #and same for ethnicity
    dec_bgHHe_tenure_10 <- dec_bgHH_tenure_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_tenure_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      mutate(
        ethnicity = substr(name,5,5),
        label = str_remove_all(label,"Total!!")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      separate(label, c("tenure","hh_size"), sep = "!!", remove = F, convert = FALSE) %>%
      mutate(hh_size_n = as.numeric(substr(hh_size,1,1))) %>%
      filter(!is.na(hh_size) & ethnicity%in%c("H","I")) %>% 
      uncount(number_sams,.id = "hhe_tenure_id",.remove = TRUE) 
    dec_bgHHe_tenure_10 <- as.data.table(dec_bgHHe_tenure_10) #dyplyr had stripped it of dt
    paste0("Number of households with ethnicity attributes in file: ", nrow(dec_bgHHe_tenure_10))
    nrow(dec_bgHHe_10)==nrow(dec_bgHHe_tenure_10)
    #should clean up
    rm(dec_bgHH_tenure_data_10)
    rm(dec_bgHH_tenure_size_re_data_from_census_10)
```

Before doing the join, let's simply see if the relations between household size and race and ethnicity are the same for both. Assuming it passes that test, then we can add tenure in, and test it by just seeing if the equalities hold around the triangle. We don't need to know anything else about the connection between tenure and household size to be confident in the next steps.
```{r test tables for P28 and H16}
test <- table(dec_bgHHr_10[,geoid],
              dec_bgHHr_10[,hh_size_n],
              dec_bgHHr_10[,race]
)==table(
  dec_bgHHr_tenure_10[,geoid],
  dec_bgHHr_tenure_10[,hh_size_n],
  dec_bgHHr_tenure_10[,race]
)
length(test[test==F])==0
test <- table(dec_bgHHe_10[,geoid],
              dec_bgHHe_10[,hh_size_n],
              dec_bgHHe_10[,ethnicity]
)==table(
  dec_bgHHe_tenure_10[,geoid],
  dec_bgHHe_tenure_10[,hh_size_n],
  dec_bgHHe_tenure_10[,ethnicity]
)
length(test[test==F])==0
```

Let's simply add the tenure information onto the household type and size information. The process is similar to what we did with the sex, age, and race/ethnicity files, where we were able to use age_range as part of the relation and add ethnicity back onto the individuals who had only race for the tract level. 

```{r join hh_type and tenure using bgHH and bgHH_tenure}
dec_bgHHr_10[,("hh_tenure_match_id"):=
                    paste0(geoid,race,as.character(hh_size_n),as.character(100000+sample(1:.N))),
                  by=.(geoid,race,hh_size_n)]
dec_bgHHr_tenure_10[,("hh_tenure_match_id"):=
                    paste0(geoid,race,as.character(hh_size_n),as.character(100000+sample(1:.N))),
                  by=.(geoid,race,hh_size_n)]
dec_bgHHr_10[,("tenure"):=
                    dec_bgHHr_tenure_10[.SD, list(tenure), on = .(hh_tenure_match_id)]]
dec_bgHHe_10[,("hhe_tenure_match_id"):=
                    paste0(geoid,ethnicity,as.character(hh_size_n),as.character(100000+sample(1:.N))),
                  by=.(geoid,ethnicity,hh_size_n)]
dec_bgHHe_tenure_10[,("hhe_tenure_match_id"):=
                    paste0(geoid,ethnicity,as.character(hh_size_n),as.character(100000+sample(1:.N))),
                  by=.(geoid,ethnicity,hh_size_n)]
dec_bgHHe_10[,("tenure"):=
                    dec_bgHHe_tenure_10[.SD, list(tenure), on = .(hhe_tenure_match_id)]]
```

Test whether tenure is now reflected in the right way for the data.
```{r test tables expanded for P28 and H16}
nrow(dec_bgHHr_10[is.na(tenure)])
test <- table(dec_bgHHr_10[,geoid],
              dec_bgHHr_10[,hh_size_n],
              dec_bgHHr_10[,race],
              dec_bgHHr_10[,tenure]
)==table(
  dec_bgHHr_tenure_10[,geoid],
  dec_bgHHr_tenure_10[,hh_size_n],
  dec_bgHHr_tenure_10[,race],
  dec_bgHHr_tenure_10[,tenure]
)
length(test[test==F])==0
nrow(dec_bgHHe_10[is.na(tenure)])
test <- table(dec_bgHHe_10[,geoid],
              dec_bgHHe_10[,hh_size_n],
              dec_bgHHe_10[,ethnicity],
              dec_bgHHe_10[,tenure]
)==table(
  dec_bgHHe_tenure_10[,geoid],
  dec_bgHHe_tenure_10[,hh_size_n],
  dec_bgHHe_tenure_10[,ethnicity],
  dec_bgHHe_tenure_10[,tenure]
)
length(test[test==F])==0
#cleaning up
dec_bgHHr_10[,c("name","label","concept","hhr_size_id","hh_tenure_match_id"):=NULL]
paste0("column names for dec_bgHHr_10: ")
colnames(dec_bgHHr_10)
dec_bgHHe_10[,c("name","label","concept","hhe_size_id","hhe_tenure_match_id"):=NULL]
paste0("column names for dec_bgHHe_10: ")
colnames(dec_bgHHe_10)
```

Then adding ones that didn't originally have race/eth will also come easier, but let's see what we can do to add them together.

H17 Tenure by age of householder by ten year groups, with race/eth, needs to have H18 hh_type by age added to it, or 

Section title - Conserving Mathematical Structures in Representations
H12 is AVERAGE HOUSEHOLD SIZE OF OCCUPIED HOUSING UNITS BY TENURE and also has race/eth - there should be a triangle to do somewhere.
Now let's get P17, average household size by age (with people under 18 in household or not) and race and ethnicity by block group. It adds the question of what happens with other sorts of representations besides summation or subset, with average being easy to understand and important to the underlying idea of maintaining structure. The additional information is a hint about how households with people under 18 differ from those without, by race and ethnicity. Since there is also a total, which we can generate separately from the dec_bgHHr_10 file we can check our process as well as give further shape to the representation for the two age groups.  

```{r download avg hhsize race eth bg}
dec_bgHH_avg_size_re_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P17",county_num = "201",
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
```
Then do our basic checks:
```{r hh_avg_size_re test problems}
dec_bgHH_avg_size_data_10 <- as.data.table(dec_bgHH_avg_size_re_data_from_census_10)
    percent_na <- dec_bgHH_avg_size_data_10[,sum(is.na(.SD))] / 
      (dec_bgHH_avg_size_data_10[,sum(!is.na(.SD))]+dec_bgHH_avg_size_data_10[,sum(is.na(.SD))])
    paste("Percentage of NAs in file:",as.integer(100*percent_na))
#make the numbers into numeric type - some operations won't work with either integer or character, although we could play with optimizing it
    dec_bgHH_avg_size_data_10[,4:ncol(dec_bgHH_avg_size_data_10)] <- dec_bgHH_avg_size_data_10[,lapply(.SD[,4:ncol(dec_bgHH_avg_size_data_10)], as.numeric)]
#multiplying the side labeled only "Total" by 2:
pop_avgs_bg <- dec_bgHH_avg_size_data_10[label=="Average household size!!Total"&concept=="AVERAGE HOUSEHOLD SIZE BY AGE",4:ncol(dec_bgHH_avg_size_data_10)]
paste0("Average household size for Harris County: ", round(mean(colMeans(pop_avgs_bg[,])),2)) # sum(pop_avgs_bg[,])/ncol(pop_avgs_bg) #to remember colMeans
#There is considerable variation at the block_group level
paste0("Minimum average household size for Harris County block groups: ",min(pop_avgs_bg[,]))
paste0("Maximum average household size for Harris County block groups: ",max(pop_avgs_bg[,])) 
paste0("Standard deviation for average household size for Harris County block groups: ",round(sd(pop_avgs_bg[,]),2))
```

We pull in the separate reporting table P17 (Average Household Size) in order to ask how one retains the structure of the average without violating the other commutative structures. Our strategy will be to order each household in a subtype (block_group x race) as either above or below the average. For each subtype, we already know the distribution, and our first task will be to show that we can maintain the structure of the average in the new representation while still respecting the other distributions. Since they are distributed into a set number of places afterward, we don't need to approximate a normal distribution in advance. We'll do it first for the total average household size by race, which is a known amount and can be generated directly from the table in P17 and as a calculation on the data in P28. Then we'll generate the subtypes from the averages for households with members below 18 and those with only members above 18 years of age. In order to do that, we need to know the number of households in each category, which we'll be able to add from P19 (Household Size by Household Type by Presence of Own Children) and P20 (Households by Presence of People Under 18 Years by Household Type by Age of People Under 18 Years), although not by race. The challenge is whether a technique that depends on a deliberately constructed space can provide a meaningful distribution of individuals within that space, which respects the structures that were important in the representation of the problem.  Our purpose is to generalize beyond the current example by showing concretely the sense in which a structured space is a more effective way of representing the process of analysis through a guided abstraction; showing how multiple structures can be respected in the construction of a solution is an important step in that process.  

Now we'll reshape P17 (Average Household Size) to use more efficiently later the average number of households by both race and ethnicity, for households with and without people under 18 in them, for each block_group. 

```{r expand P17 into dec_bgHH_avg_size_data_10}
    dec_bgHH_avg_size_10 <- dec_bgHH_avg_size_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_avg_size_data_10),names_to = "geoid", 
                   values_to = "avg_hh_size") %>% 
      mutate(race = substr(name,5,5))   
    dec_bgHH_avg_size_10 <- as.data.table(dec_bgHH_avg_size_10) #dyplyr had stripped it of dt

    #should clean up all along in the other making_sam, too
    
    rm(dec_bgHH_avg_size_re_data_from_census_10)
```

As a quick check, the averages directly reported as "Total" in P17 should match calculations on the actual description in P28. We will check those, first, as proof that we can make the simplest case work, and then construct the pieces that allow for a sub-typing to be represented in the distribution of the data. We'll test on total, "Black or African American Alone," and "White Alone, not Hispanic or Latino." The two will not match perfectly, because P17 can have numbers greater than 7 for a household and may have some other differences in collection. P17 has averages for households with and without people under 18, by all the same categories, which is the only thing that is added beyond P28.

```{r compare P17 dec_bgHH_avg_size avg to P28 dec_bgHH avg}
  dec_bgHHr_10 <- as.data.table(dec_bgHHr_10)
  #dec_bgHHr_10[,("hh_size_n"):=as.numeric(substr(hh_size,1,1))]
  dec_bgHHr_10[,("avg_hh_size_bg"):=round(mean(hh_size_n),2),by=.(geoid)]
  
  dec_bgHHr_10[,("avg_hh_size_bg_r"):=round(mean(hh_size_n),2),by=.(geoid,race)]
  
  #dec_bgHHe_10[,("hh_size_n"):=as.numeric(substr(hh_size,1,1))]
  dec_bgHHe_10[,("avg_hh_size_bg_e"):=round(mean(hh_size_n),2),by=.(geoid,ethnicity)]
  
  paste0("All")
  summary(dec_bgHH_avg_size_10[name=="P017001",avg_hh_size])
  summary(dec_bgHHr_10[,(avg_hh_size_bg)])
  paste0("Black")
  summary(dec_bgHH_avg_size_10[name=="P017B001",avg_hh_size])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r)])
  paste0("Hispanic")
  summary(dec_bgHH_avg_size_10[name=="P017H001",avg_hh_size])
  summary(dec_bgHHe_10[ethnicity=="H",(avg_hh_size_bg_e)])
  
  #rm(dec_bgHH_avg_size_data_10)
```

We remark immediately on the maximum value for Black households being 10.5, when the highest reported value in P28 is 7. The block group in question (48_201_233703_2) turns out to only have two Black households listed in P28, both in the "7-or-more-person-household"; the same block group is responsible for the max value in both. There are 293 households total in that block group and we did a deep dive into the individual household level data. We are assuming on that basis that it's not a reporting mistake, but just an unusual situation. Remember that the summary statistics are for all the averages across block groups and that only the average for that block group is reported in P17. For the total households in the block group that has the outliers (48_201_233703_2), the summary across all the households in P28 has a median a little higher than the mean, but nothing out of the ordinary and relatively close to the reported mean in P17. The fact that the actual values are whole numbers actually gives us a clue for how to think about the distribution we aim to do in the end.

```{r 48_201_233703_2 summary for households}
summary(dec_bgHHr_10[geoid=="48_201_233703_2",(hh_size_n)])
paste0("Reported average in P17: ",dec_bgHH_avg_size_10[name=="P017001"&geoid=="48_201_233703_2",avg_hh_size])
```

We will implement a matching process on the households in two steps. First, we create a number for every household that is either above or below the average for that race category, by geoid and race, using sample() to create a random distribution centered on the average. Since there is no other information on the dataset, we don't have to worry about matching individual households with attributes that would influence how many people are in the household. After assigning them a number, we will order the households across the subtypes in the category that is one higher in the embedding (in this case, all households, but could also be a larger geographic area) by that new number and assign the available sizes as a vector. This preserves the structure of the differing averages - even given that we do not know anything else about the distribution. We are testing it this time, since we have a ground truth for households by race. If it works, we will then use the same technique to create sub-types by households with and without persons under 18 by race. The last by=.(geoid) keeps the statistical structure constant at the geoid (block group) level, but a by=.(geoid,race) would keep each race having the same outcomes within each. 

```{r add match logic to household size test}
    #create number for each hh on both sides of the average by race
    dec_bgHHr_10[,("match_avg_r_num"):=avg_hh_size_bg_r+
          sample((-.N/2):.N/2,.N,replace=FALSE)/.N,by=.(geoid,race)]
    #order all races together by the new number and match with tract ordered by hh_size_n; 
    dec_bgHHr_10[order(match_avg_r_num),("hh_size_generated"):=.SD[order(hh_size_n),"hh_size_n"],by=.(geoid)]
    
    #let's also create something that shows how much diff it can make; 
    #any distribution could be imposed, although it will always be constrained 
    #by the choice of level where the available categories are kept constant
    dec_bgHHr_10[race!="D",("match_example_r_num"):=avg_hh_size_bg_r+
          sample((-.N/2):.N/2,.N,replace=FALSE)/.N,by=.(geoid,race)]
    #and then with Asians, we'll push it toward the high end
    dec_bgHHr_10[race!="D",("match_example_r_num"):=avg_hh_size_bg_r+
          sample(.N:.N*2,.N,replace=FALSE)/.N,by=.(geoid,race)]
    #order all races together by the new number and match with tract ordered by hh_size_n; 
    dec_bgHHr_10[order(match_example_r_num),("hh_size_example"):=.SD[order(hh_size_n),"hh_size_n"],by=.(geoid)]

```

Let's look at some statistics to see how the newly generated household size looks compared to the originals. First, let's confirm that we didn't accidentally assign the household size to the same households. We're looking for a relatively random distribution, just showing that the old hh_size_n is independent from the new hh_size_generated. [not sure this is helpful]

```{r table confirming hh_sizes are indep}
table(dec_bgHHr_10$hh_size_n,dec_bgHHr_10$hh_size_generated)
table(dec_bgHHr_10[geoid=="48_201_233702_1",hh_size_n],dec_bgHHr_10[geoid=="48_201_233702_1",hh_size_generated])
```

Then let's look at the same summary statistics we had produced before, with the newly generated household sizes included. This should give us a sense for how it worked globally. We can do this for all the distributions, and there are statistical tests for representing the difference in the distributions, but right now we're just getting a general sense. 

```{r summary for hh_size_generated}
  dec_bgHHr_10[,("avg_hh_size_bg_g"):=round(mean(hh_size_generated),2),by=.(geoid)]
  dec_bgHHr_10[,("avg_hh_size_bg_r_g"):=round(mean(hh_size_generated),2),by=.(geoid,race)]
  dec_bgHHr_10[,("avg_hh_size_bg_ex"):=round(mean(hh_size_example),2),by=.(geoid)]
  dec_bgHHr_10[,("avg_hh_size_bg_r_ex"):=round(mean(hh_size_example),2),by=.(geoid,race)]
  
  paste0("All")
  #order is published average, 
  #calculated average from P28, 
  #calculated average from newly generated household size
  #example for Asian population pushed toward higher household size
  
  summary(dec_bgHH_avg_size_10[name=="P017001",avg_hh_size])
  summary(dec_bgHHr_10[,(avg_hh_size_bg)])
  summary(dec_bgHHr_10[,(avg_hh_size_bg_g)])
  summary(dec_bgHHr_10[,(avg_hh_size_bg_ex)])
  paste0("White")
  summary(dec_bgHH_avg_size_10[name=="P017A001",avg_hh_size])
  summary(dec_bgHHr_10[race=="A",(avg_hh_size_bg_r)])
  summary(dec_bgHHr_10[race=="A",(avg_hh_size_bg_r_g)])
  summary(dec_bgHHr_10[race=="A",(avg_hh_size_bg_r_ex)])
  paste0("Black or African American")
  summary(dec_bgHH_avg_size_10[name=="P017B001",avg_hh_size])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r)])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r_g)])
  summary(dec_bgHHr_10[race=="B",(avg_hh_size_bg_r_ex)])
  paste0("Asian")
  summary(dec_bgHH_avg_size_10[name=="P017D001",avg_hh_size])
  summary(dec_bgHHr_10[race=="D",(avg_hh_size_bg_r)])
  summary(dec_bgHHr_10[race=="D",(avg_hh_size_bg_r_g)])
  summary(dec_bgHHr_10[race=="D",(avg_hh_size_bg_r_ex)])
  
```

Let's also look at the variation within a single block group for the household size, and not for the averages across all the block groups. Notice that everything but the means are whole numbers, since it's picking out actual values for households at those indexed spots.

```{r variation generated hh_size bg}
summary(dec_bgHHr_10[geoid=="48_201_233702_3",(hh_size_n)])
summary(dec_bgHHr_10[geoid=="48_201_233702_3",(hh_size_generated)])
paste("And for all of Harris County: ")
summary(dec_bgHHr_10[str_detect(geoid,"48_201"),(hh_size_generated)])
```

```{r cleanup after avg example on dec_bgHHr_10}
  dec_bgHHr_10[,c(hh_sizes_available,hh_size_generated,hh_size_example):=NULL] #get full list next time through
```

[have to think about what other statistical tests to show; when do we want to show how to add ethnicity?]

Let's recap why that was an interesting digression. Maybe a drawing of the commutative relationships and then what it means to retain certain mathematical structures while still transforming other parts of the relation. Can average household size, for example, also help to add in the Hispanic population? Can we use that trick backwards to then construct more subtypes?


Our plan is to add the information about the average household size for households with and without people under 18, using this technique to add a dependent structure within the conceptual space of the representation. If we can take advantage of the existing relational structure, however, we can ensure that the structure is respected and the final representation is more reflective of the underlying relation. Since our starting point for the households included household size and household type, as well as race and ethnicity information, let's see if any of that can be added to the averages before using them in the overall representation. 
We find that P18, for example, gives you the number of householders living alone and the ones that are living with others. P22 gives you the age of the householder by type, which would help us know if there are under 18 year olds living alone (there are a few). Then P20 has type by whether the household has people under 18 connected to household type. P21 tells us how many have their own children in the household by household type. None of these has race and ethnicity, but each will allow us to get closer to specifying the complete construction of all the possible relations in a way that respects the underlying commutativity of relations, as originally conceived by the census bureau to be a complete and coherent representation of the population. 

Section title - dimensions [either talk about dependent types or about triangles and commutativity or both]

Now let's get P18, household type by race and ethnicity, which has more detail in the type of household.
```{r download hhtype P18 race eth bg}
dec_bgHH_type_re_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P18",county_num = "201",
                         block="block_group",api_type="dec/sf1",path_suff="est.csv")
```

Then do our basic checks:
```{r hh_type_re test problems}
dec_bgHH_type_data_10 <- as.data.table(dec_bgHH_type_re_data_from_census_10)
    percent_na <- dec_bgHH_type_data_10[,sum(is.na(.SD))] / 
      (dec_bgHH_type_data_10[,sum(!is.na(.SD))]+dec_bgHH_type_data_10[,sum(is.na(.SD))])
    paste("Percentage of NAs in file:",as.integer(100*percent_na))
#make the numbers into numeric type - some operations won't work with either integer or character, although we could play with optimizing it
    dec_bgHH_type_data_10[,4:ncol(dec_bgHH_type_data_10)] <- dec_bgHH_type_data_10[,lapply(.SD[,4:ncol(dec_bgHH_type_data_10)], as.numeric)]
pop_totals_bg <- dec_bgHH_type_data_10[label=="Total"&concept=="HOUSEHOLD TYPE",4:ncol(dec_bgHH_type_data_10)]
paste0("Number of Households total for Harris County by block_group: ", sum(pop_totals_bg[,]))
sum(pop_totals_bg[,])==nrow(dec_bgHHr_10)
```

Then expand into the representation of individual households by both ethnicity and race.
```{r expand P18 into dec_bgHH_type_10}
#break into race and ethnicity files
    race_codes <- c("A","B","C","D","E","F","G")
    dec_bgHHr_type_10 <- dec_bgHH_type_data_10 %>%
      pivot_longer(4:ncol(dec_bgHH_type_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      mutate(
        race = substr(name,5,5),
        label = str_remove_all(label,"Total!!"),
        label = str_remove_all(label,"!!Other family")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      separate(label, c("family","family_details"), sep = "!!", remove = F, convert = FALSE) %>%
      filter(!is.na(family_details) & race%in%race_codes) %>% #to get rid of aggregations by family
      uncount(number_sams,.id = "hhr_type_id",.remove = TRUE) 
    dec_bgHHr_type_10 <- as.data.table(dec_bgHHr_type_10) #dyplyr had stripped it of dt
    paste0("Number of households in file: ", nrow(dec_bgHHr_type_10))
    nrow(dec_bgHHr_10)==nrow(dec_bgHHr_type_10)
    
    #multiplying the side labeled only "Total" by 2:
    dec_bgHHr_type_10[,4:ncol(dec_bgHHr_type_10)] <- dec_bgHHr_type_10[,lapply(.SD[,4:ncol(dec_bgHHr_type_10)], as.numeric)]
    #and same for ethnicity
    dec_bgHHe_type_10 <- dec_bgHH_type_data_10 %>%
      mutate(
        ethnicity = substr(name,5,5),
        label = str_remove_all(label,"Total!!"),
        label = str_remove_all(label,"!!Other family")) %>%
      filter(label != "Total") %>% #keep only the ones that aren't aggregated
      pivot_longer(4:ncol(dec_bgHH_type_data_10),names_to = "geoid", values_to = "number_sams") %>% 
      separate(label, c("family","family_details"), sep = "!!", remove = F, convert = FALSE) %>%
      filter(!is.na(family_details) & ethnicity%in%c("H","I")) %>% 
      uncount(number_sams,.id = "hhe_type_id",.remove = TRUE) 
    dec_bgHHe_type_10 <- as.data.table(dec_bgHHe_type_10) #dyplyr had stripped it of dt
    paste0("Number of households with ethnicity attributes in file: ", nrow(dec_bgHHe_10))
    nrow(dec_bgHHe_10)==nrow(dec_bgHHe_type_10)
    #should clean up
    rm(dec_bgHH_type_data_10)
    rm(dec_bgHH_type_re_data_from_census_10)
```

We keep looking for relations that are more fundamental - not in the sense that they cause others, but that they represent the possibility of connection while still giving structure to the overall representation. Things like sex, race, ethnicity, and age are fundamental because they show up in lots of the data tables put out by the census, and they have ensured that they are complete and commutative between representations. We're trying to show that we can use those relations to create better representations than just the tables by themselves would allow, and thus to make a larger point about how the construction of data problems and answers move hand in hand. 
In summary, if you try to build the representation piece by piece, while insisting on the structure of how certain you are of the particular step, you are both leaving behind the strength of the relationships in building a coherent representation and forgetting that the relations imply certain sorts of structures - have to work on that phrasing.

For our point, what we're saying is that the relations as structures are represented by triangle commutativity. In practice, that just means that we build the data set by adding where we have some way of maintaining the structure needed to make it meaningful. The basic unit of interest is the relation, and then the way that the whole sets of relations are set out is what's of interest in our mapping. If we think of it as just adding attributes to the individual, including their relations, then the chance of it all building into one are limited - it's why there's more margin of error as you get smaller for the census.

The only way to be sure that the Hispanic stuff adds up correctly is to have something about it built into the bgSAR.
Should we do stuff where overlapping ages, etc., create new "work_columns" and get narrowed down? Is that the same thing that happens with things like Tenure with Household Size? 

We have a building that goes on by understanding how the counts work, and that the relations are commutative at some level, even if we don't know which one yet.

We had wanted to join P22, household type by age of householder, to household type but the relation between age and race/ethnicity could be lost. We find that H18 has tenure (whether one owns, rents, and sometimes whether one has a mortgage) by household type and by age of householder, and 


Then do P22, HOUSEHOLD TYPE BY AGE OF HOUSEHOLDER, but which doesn't have race/eth
(how does that relate to P17? look at totals)

H19 - TENURE BY PRESENCE OF PEOPLE UNDER 18 YEARS (EXCLUDING HOUSEHOLDERS, SPOUSES, AND UNMARRIED PARTNERS)

```{r basic household data download}
#block - sf1 - P15 and P16 (eth and race of hh) P18 (hhtype by race/eth) P22 (hhtype by age of hh) p28 (hhtype by hhsize by race/eth)
#plan - looks like P17 and P18 have much more detail than P28, so combine P17 with P28, then P18 with P28xP17
#then combine P16 with P28xP17xP18 and P15 with P28xP17xP18xP16   
#block - sf1 - H16 tenure by hh_size, with race/eth and H17 - Tenure by age with race/eth

#tract - sf2 - (Tenure by hhtype by age of hh - HCT9) - more age_groups and has tenure
```

```{r hh_relation download block}

dec_hh_relation_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                groupname = "P29",county_num = "201",
                                                                block="block_group",api_type="dec/sf1",path_suff="est.csv")
```



The redistricting data collected in 2010 reports population estimates in yet a different format. It has one table that has races and ethnicity data for everyone over 18 and one that has it for everyone, with no ages attached. The race data gives more details on the composition of "Two or More Races" but nothing else new. The ethnicity file gives block level data for Hispanic or Latino who don't identify as White. Gives slightly more contour, but not much, and risks big differences on age and ethnicity or race per block group. (Although some contour is better than none!)

```{r pl data downloaded}
    dec_eth_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                      groupname = "P2",county_num = "201",
                                                                      block="block_group",api_type="dec/pl",path_suff="est.csv")
    dec_race_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                groupname = "P1",county_num = "201",
                                                                block="block_group",api_type="dec/pl",path_suff="est.csv")
    dec_eth_over18_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                      groupname = "P4",county_num = "201",
                                                                      block="block_group",api_type="dec/pl",path_suff="est.csv")
    dec_race_over18_block_data_from_census_10 <- censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                                                                groupname = "P3",county_num = "201",
                                                                block="block_group",api_type="dec/pl",path_suff="est.csv")
 
    
```




Then have the tract level details join into the bg representation? Should discuss whether to start with the household stuff - check if it has the PCT level of detail on ethnicity anywhere... 
If not, can just save the demographic_bg to the OneDrive and start from there.




https://www2.census.gov/programs-surveys/decennial/2010/technical-documentation/complete-tech-docs/summary-file/sf1.pdf
put in households
add P15 or P16, then other hh stuff that can be added without problems, then add people in
also look at H9
P21 should also be good; p29, p30...

have to think about how family info is different from hh

group quarters are needed b/c hh don't include them
```{r group quarters to add}
dec_group_quarters_block_data_from_census_10 <- 
  censusData_byGroupName(censusdir, vintage="2010", state, censuskey, 
                         groupname = "P5",county_num = "201",
                         block="block_group",api_type="dec/pl",path_suff="est.csv")
```



expand from census

rearrange so that they all add to same whole and then figure out how to distribute them among that space

tests of each step

discussion of power sets vs. categories? 

redoing HCAD first to help with generation of 2011 - a space to move into...


